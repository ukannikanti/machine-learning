 3/1: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 3/2: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 3/3: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 3/4: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 6/1: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 6/2: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 6/3: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 6/4: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 6/5: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 6/6: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 6/7: runfile('/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression/simple_linear_regression.py', wdir='/Users/ukannika/work/choice/machine-learning/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 4 - Simple Linear Regression')
 6/8:
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Training set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
 6/9:
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Test set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
6/10:
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Test set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
6/11:
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Test set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()
6/12:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
6/13: dataset = pd.read_csv('50_Startups.csv')
 7/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
 7/2: dataset = pd.read_csv('50_Startups.csv')
 7/3: dataset = pd.read_csv('50_Startups.csv')
 7/4: dataset = pd.read_csv('50_Startups.csv')
 7/5:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
 7/6: dataset = pd.read_csv('50_Startups.csv')
 7/7: dataset = pd.read_csv('50_Startups.csv')
 7/8: dataset = pd.read_csv('50_Startups.csv')
 7/9:
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 4].values
7/10: X
7/11:
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder = LabelEncoder()
X[:, 3] = labelencoder.fit_transform(X[:, 3])
onehotencoder = OneHotEncoder(categorical_features = [3])
X = onehotencoder.fit_transform(X).toarray()
7/12: X = X[:, 1:]
7/13:
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
7/14:
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)
7/15: y_pred = regressor.predict(X_test)
11/1: vjgjhgj
11/2: version
12/1: jupyter --version
13/1: jupyter --version
14/1: sc
15/1: sc
16/1: spark --version
16/2: java --version
16/3: ckear
16/4: clear
16/5: python -version
16/6: python
16/7: clear
17/1: help
19/1: pyspark
22/1:
import numpy as np
import pandas as pd
from IPython.display import display
22/2: full_data = pd.read_csv('titanic_data.csv')
22/3: display(full_data.head())
22/4:
outcomes = full_data['Survived']
data = full_data.drop('Survived', axis = 1)
22/5:
def accuracy_score(truth, pred):
    """ Returns accuracy score for input truth and predictions. """
    
    # Ensure that the number of predictions matches number of outcomes
    if len(truth) == len(pred): 
        
        # Calculate and return the accuracy as a percent
        return "Predictions have an accuracy of {:.2f}%.".format((truth == pred).mean()*100)
    
    else:
        return "Number of predictions does not match number of outcomes!"


# Test the 'accuracy_score' function
predictions = pd.Series(np.ones(5, dtype = int))
print accuracy_score(outcomes[:5], predictions)
22/6:
def accuracy_score(truth, pred):
    """ Returns accuracy score for input truth and predictions. """
    
    # Ensure that the number of predictions matches number of outcomes
    if len(truth) == len(pred): 
        
        # Calculate and return the accuracy as a percent
        return "Predictions have an accuracy of {:.2f}%.".format((truth == pred).mean()*100)
    
    else:
        return "Number of predictions does not match number of outcomes!"
22/7: predictions = pd.Series(np.ones(5, dtype = int))
22/8: print accuracy_score(outcomes[:5], predictions)
22/9: score = accuracy_score(outcomes[:5], predictions)
22/10:
def predictions_0(data):
    """ Model with no features. Always predicts a passenger did not survive. """
    
    predictions = []
    for _, passenger in data.iterrows():
        
        # Predict the survival of 'passenger'
        predictions.append(0)
    
    # Return our predictions
    return pd.Series(predictions)


# Make the predictions
predictions = predictions_0(data)
22/11: print accuracy_score(outcomes, predictions)
22/12: accuracy_score(outcomes, predictions)
22/13: import visuals as vs
22/14: import visuals as vs
22/15: import visuals as vs
22/16: print(accuracy_score(outcomes, predictions))
22/17: vs.survival_stats(data, outcomes, 'Age', ["Sex == 'male'"])
22/18:
def predictions_2(data):
    """ Model with two features: 
            - Predict a passenger survived if they are female.
            - Predict a passenger survived if they are male and younger than 10. """
    
    predictions = []
    for _, passenger in data.iterrows():
        
        # Remove the 'pass' statement below 
        # and write your prediction conditions here
        pass
    
    # Return our predictions
    return pd.Series(predictions)


# Make the predictions
predictions = predictions_2(data)
22/19: print(accuracy_score(outcomes, predictions))
22/20: vs.survival_stats(data, outcomes, 'Age', ["Sex == 'male'", "Age < 18"])
22/21:
def predictions_3(data):
    """ Model with multiple features. Makes a prediction with an accuracy of at least 80%. """
    
    predictions = []
    for _, passenger in data.iterrows():
        
        # Remove the 'pass' statement below 
        # and write your prediction conditions here
        pass
    
    # Return our predictions
    return pd.Series(predictions)


# Make the predictions
predictions = predictions_3(data)
22/22: print(accuracy_score(outcomes, predictions))
23/1: sc
24/1: sc
24/2: pyspark
25/1: sc
26/1: sparkSession = SparkSession.builder.appName("Mastery2017-Demo").getOrCreate()
26/2: sparkSession = SparkSession.builder.appName("Mastery2017Demo").getOrCreate()
26/3: sparkSession = sc.builder.appName("Mastery2017Demo").getOrCreate()
26/4: sc
27/1: sparkSession = SparkSession .builder.appName("Mastery2017Demo").getOrCreate()
27/2:
# Create a schema to read data from kafka topic
schema = StructType().add("name", "string").add("age", "integer")
27/3: schema = StructType().add("name", "string").add("age", "integer")
27/4: userSchema = StructType().add("name", "string").add("age", "integer")
27/5:
from pyspark.sql.types import StructType
userSchema = StructType().add("name", "string").add("age", "integer")
27/6:
# Creating a spark session.
sparkSession = SparkSession .builder.appName("Mastery2017Demo").getOrCreate()
27/7:
# Creating a schema to read data from Kafka
from pyspark.sql.types import StructType
userSchema = StructType().add("name", "string").add("age", "integer")
27/8:
# Read Stream
dataset = sparkSession.readStream
                      .option("sep", "|") 
                      .schema(userSchema) 
                      .csv("/Users/ukannika")
27/9:
# Read Stream
dataset = sparkSession.readStream \
                      .option("sep", "|") \
                      .schema(userSchema) \
                      .csv("/Users/ukannika") \
27/10:
# Creating a spark session.
sparkSession = SparkSession .builder.appName("Mastery2017Demo").getOrCreate()
27/11:
# Creating a spark session.
sparkSession = SparkSession .builder.appName("Mastery2017Demo").getOrCreate()
27/12:
# Creating a schema to read data from Kafka
from pyspark.sql.types import StructType
userSchema = StructType().add("name", "string").add("age", "integer")
27/13:
# Read Stream
dataset = sparkSession.readStream \
                      .option("sep", "|") \
                      .schema(userSchema) \
                      .csv("/Users/ukannika") \
27/14:
# Creating a spark session.
sparkSession = SparkSession .builder.appName("Mastery2017Demo").getOrCreate()
27/15:
# Creating a schema to read data from Kafka
from pyspark.sql.types import StructType
userSchema = StructType().add("name", "string").add("age", "integer")
28/1: sc
27/16:
# Creating a spark session.
sparkSession = SparkSession .builder.appName("Mastery2017Demo").getOrCreate()
28/2: sc
29/1: sc
30/1: sc
31/1: sc
35/1:
#Spark Session
sc
35/2:
 # Read Stream Data from Kafka
  val dataset = sc \
            .read \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "itmastery2017") \
            .load()
35/3:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017") \
                .getOrCreate()
35/4:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017") \
                .getOrCreate()
35/5:
 # Read Stream Data from Kafka
  val dataset = sparkSession \
            .read \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "itmastery2017") \
            .load()
35/6:
 # Read Stream Data from Kafka
  val dataset = sparkSession \
            .read \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "itmastery2017") \
            .load()
35/7: dataset.show
35/8:
 # Read Stream Data from Kafka
  val dataset = sparkSession \
            .read \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "itmastery2017") \
            .load()
35/9:
 # Read Stream Data from Kafka
  val dataset = sparkSession \
            .read \
            .format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("subscribe", "itmastery2017") \
            .load()
35/10: dataset.show
35/11: sparkSession
35/12:
sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
35/13:
sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
36/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017") \
                .getOrCreate()
36/2:
sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
37/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017") \
                .getOrCreate()
37/2:
sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
37/3:
val dataset = sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
37/4:
dataset = sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
37/5: dataset.show()
37/6:
batchDataset = sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
37/7: batchDataset.show()
37/8: batchDataset.show()
37/9: batchDataset.show()
38/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
38/2:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017Streaming") \
                .getOrCreate()
38/3:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017") \
                .getOrCreate()
37/10:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Batch") \
                .getOrCreate()
37/11:
#Read Stream Data From Kafka
batchDataset = sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
37/12:
# Show Dataset
batchDataset.show()
37/13:
#Read Stream Data From Kafka
batchDataset1 = sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
37/14:
# Show Dataset
batchDataset1.show()
40/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
41/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017") \
                .getOrCreate()
41/2:

#Read Stream Data From Kafka
batchDataset = sparkSession.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "itmastery2017") \
        .load()
41/3: batchDataset.show()
41/4:
#Read Stream Data From Kafka
dataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/5:
#Transform And Flatten
dataset = dataset.selectExpr("CAST(value AS STRING) as json")
                 .select(from_json(col("json"), schema).as("data"))
                 .select("data.*");
41/6:
#Transform And Flatten
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(from_json(col("json"), schema).as("data")) \
                 .select("data.*");
41/7:
#Transform And Flatten
dataset = dataset.selectExpr("CAST(value AS STRING) as json")
41/8: dataset.show()
41/9:
#Transform And Flatten
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(pyspark.sql.functions.from_json(col("json"), schema).as("data")) \
41/10:
#Transform And Flatten
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(pyspark.sql.functions.from_json(col("json"), schema)) \
41/11:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(from_json(col("json"), schema)) \
41/12:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(from_json(col("json"), schema)) \
41/13:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
41/14:
#Transform And Flatten
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
41/15:
#Transform And Flatten
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
41/16:
#Read Stream Data From Kafka
dataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/17:
#Transform And Flatten
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
41/18: dataset.show()
41/19:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(from_json(col("json"), schema)) \
41/20:
#Read Stream Data From Kafka
dataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/21:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(from_json(col("json"), schema)) \
41/22:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json(col("json"), schema)) \
41/23:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json(functions.col("json"), schema)) \
41/24:
# Create a Schema
schema = StructType().add("property", "string").add("state", "string")
41/25:
# Create a Schema
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/26:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json(functions.col("json"), schema)) \
41/27: dataset.show()
41/28:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json(functions.col("json"), schema) as data) \
41/29:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json(functions.col("json"), schema)).as("data") \
41/30:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json(functions.col("json"), schema)).alias("data") \
41/31:
#Read Stream Data From Kafka
dataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/32:
# Create a Schema
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/33:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json(functions.col("json"), schema)).alias("data") \
41/34: dataset.show()
41/35:
#Read Stream Data From Kafka
dataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/36:
# Create a Schema
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/37:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema)).alias("data") \
                 .select("data.*")
41/38: dataset.show()
41/39: dataset.show()
41/40: dataset.show()
41/41:
#Transform And Flatten
from pyspark.sql import functions
dataset1 = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema)).alias("data") \
                 .select("data.*")
41/42:
#Read Stream Data From Kafka
dataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/43:
# Create a Schema
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/44:
#Transform And Flatten
from pyspark.sql import functions
dataset1 = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema)).alias("data") \
                 .select("data.*")
41/45: dataset1.show()
41/46:
#Read Stream Data From Kafka
dataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/47:
# Create a Schema
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/48:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema)).alias("data") \
                 .select("datahdjsgdjg.*")
41/49:
#Transform And Flatten
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema)).alias("data") \
                 .select("data.*")
41/50: dataset.show()
41/51: dataset.schema.print
41/52:
#Read Stream Data From Kafka
rawDataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/53:
# Create a Schema
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/54:
#Transform And Flatten
from pyspark.sql import functions
flattenDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema)).alias("data") \
41/55: flattenDataset.show()
41/56:
#Read Stream Data From Kafka
rawDataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/57:
# Create a Schema
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/58:
#Transform And Flatten
from pyspark.sql import functions
flattenDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema)).alias("data") \
                 .select("jsontostructs(json).*")
41/59: flattenDataset.show()
41/60:
#Read Stream Data From Kafka
rawDataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/61:
# Create a Schema
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/62:
#Transform And Flatten
from pyspark.sql import functions
flattenDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*")
41/63: flattenDataset.show()
41/64:
#Read Stream Data From Kafka

rawDataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/65: rawDataset.show()
41/66:
# Create a Schema

from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/67:
#Transform And Flatten

from pyspark.sql import functions
flattenDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*")
41/68: flattenDataset.show()
42/1:
#Spark Session

sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017") \
                .getOrCreate()
41/69:
#Spark Session

sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017") \
                .getOrCreate()
41/70:
#Read Stream Data From Kafka

rawDataset = sparkSession.read \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("subscribe", "itmastery2017") \
                      .load()
41/71: rawDataset.show()
41/72:
# Create a Schema

from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
41/73:
#Transform And Flatten

from pyspark.sql import functions
flattenDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*")
41/74: flattenDataset.show()
42/2:
#Spark Session

sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017") \
                .getOrCreate()
44/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Batch") \
                .getOrCreate()
44/2:
#Read Stream Data From Kafka

rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
44/3: rawDataset.show()
44/4:
#Create a Schema 
from pyspark.sql.types from StructType
schema = new StructType().add("property", "string").add("state", "string")
44/5:
#Create a Schema 
from pyspark.sql.type from StructType
schema = new StructType().add("property", "string").add("state", "string")
44/6:
#Create a Schema 
from pyspark.sql.types import StructType
schema = new StructType().add("property", "string").add("state", "string")
44/7:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json")
                 .select(functions.from_json("json", schema).alias("data"))
                 .select("data.*");
44/8:
#Create a Schema 
from pyspark.sql.types import StructType
schema = new StructType.add("property", "string").add("state", "string")
44/9:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
44/10:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json")
                 .select(functions.from_json("json", schema).alias("data"))
                 .select("data.*");
44/11:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*");
44/12:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*");
44/13: transformDataset.show()
45/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
45/2:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
45/3:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
45/4:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
45/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*");
45/6:
# Start running the query that prints the running counts to the console
queryStream = transformDataset \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

queryStream.awaitTermination()
45/7:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
45/8:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
45/9:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
45/10:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
45/11:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .start()

# Waits for the termination of this query
queryStream.awaitTermination()
45/12:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("console") \
                              .start()

# Waits for the termination of this query
queryStream.awaitTermination()
46/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
46/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
46/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
46/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
47/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
47/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
47/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
47/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
47/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("console") \
                              .option("checkpointLocation","file:///Users/ukannika/work/choice/check-point")
                              .start("file:///Users/ukannika/work/choice/save")

# Waits for the termination of this query
queryStream.awaitTermination()
47/6:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("console") \
                              .option("checkpointLocation","file:///Users/ukannika/work/choice/check-point")
                              .start()

# Waits for the termination of this query
queryStream.awaitTermination()
47/7:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","file:///Users/ukannika/work/choice/check-point") \
                              .start("file:///Users/ukannika/work/choice/save")

# Waits for the termination of this query
queryStream.awaitTermination()
48/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
48/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
48/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
48/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
48/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3n://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
48/6:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3a://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3a://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
49/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
49/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
49/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
49/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
49/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3n://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3n://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
50/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
50/2:
# Add S3 Credentials

sparkSession.sparkContext.hadoopConfiguration.set("fs.s3n.awsAccessKeyId", "");
sparkSession.sparkContext().hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "");
50/3:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
50/4:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
50/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
50/6:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3a://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3a://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
51/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
51/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
51/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
51/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
52/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
52/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
52/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
52/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
53/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
53/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
53/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
53/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
54/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
54/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
54/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
54/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
55/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
55/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
55/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
55/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
56/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
56/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
56/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
56/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
56/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3a://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3a://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
57/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
57/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
57/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
57/4:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3a://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3a://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
57/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
57/6:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
57/7:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3a://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3a://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
58/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
58/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
58/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
58/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
59/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
59/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
59/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
59/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
59/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3a://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3a://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
60/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
60/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
60/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
60/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
60/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3a://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3a://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
61/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
61/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
61/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
61/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
61/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3a://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3a://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
62/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
62/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
62/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
62/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
63/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
63/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
63/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
63/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
63/5:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
63/6:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
63/7:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
63/8:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
63/9:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3n://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3n://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
63/10:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3n://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3n://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
66/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
66/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
66/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
66/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
66/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3n://choice-dap-streaming-preprod/itmastery-2017/checkpoint") \
                              .start("s3n://choice-dap-streaming-preprod/itmastery-2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
68/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
68/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
68/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
68/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
68/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3n://choice-dap-delivery-preprod/itmastery2017/checkpoint") \
                              .start("s3n://choice-dap-delivery-preprod/itmastery2017/save")

# Waits for the termination of this query
queryStream.awaitTermination()
70/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
70/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
70/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
70/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
70/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3n://choice-data-file-ingestor/dap/checkpoint") \
                              .start("s3n://choice-data-file-ingestor/dap/save")

# Waits for the termination of this query
queryStream.awaitTermination()
71/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
71/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
71/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
71/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
71/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3a://choice-data-file-ingestor/dap/checkpoint") \
                              .start("s3a://choice-data-file-ingestor/dap/save")

# Waits for the termination of this query
queryStream.awaitTermination()
72/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
72/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
72/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
72/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
72/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3n://choice-data-file-ingestor/dap/checkpoint") \
                              .start("s3n://choice-data-file-ingestor/dap/save")

# Waits for the termination of this query
queryStream.awaitTermination()
73/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
73/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
73/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
73/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
73/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation","s3n://choice-data-file-ingestor/dap/checkpoint") \
                              .start("s3n://choice-data-file-ingestor/dap/save")

# Waits for the termination of this query
queryStream.awaitTermination()
74/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
74/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
74/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
74/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
75/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
75/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
75/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
75/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
76/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
76/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
76/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
76/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
76/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("console") \
                              .start()

# Waits for the termination of this query
queryStream.awaitTermination()
77/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
77/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
77/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
77/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
77/5:
# Start running the query that prints the running counts to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("console") \
                              .start()

# Waits for the termination of this query
queryStream.awaitTermination()
79/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
79/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
79/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
79/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*");
79/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "file:///Users/ukannika/work/choice/checkpoint/") \  
                              .start("file:///Users/ukannika/work/choice/save/")
                
# Waits for the termination of this query
queryStream.awaitTermination()
79/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "file:///Users/ukannika/work/choice/checkpoint") \  
                              .start("file:///Users/ukannika/work/choice/save")
                
# Waits for the termination of this query
queryStream.awaitTermination()
79/7:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "file:///Users/ukannika/work/choice/checkpoint") \
                              .start("file:///Users/ukannika/work/choice/save")
                
# Waits for the termination of this query
queryStream.awaitTermination()
80/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
80/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
80/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
80/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
                             .coalesce(1);
80/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*") \
                             .coalesce(1);
80/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "file:///Users/ukannika/work/choice/checkpoint") \
                              .start("file:///Users/ukannika/work/choice/save")
                
# Waits for the termination of this query
queryStream.awaitTermination()
81/1:
from sparkdl import readImages
from pyspark.sql.functions import lit

img_dir = "/Users/ukannika/work/choice/machine-learning/itmastery-2017/image-classification-demo/images"
81/2:
from sparkdl import readImages
from pyspark.sql.functions import lit

img_dir = "/Users/ukannika/work/choice/machine-learning/itmastery-2017/image-classification-demo/images"
81/3:
from sparkdl import readImages
from pyspark.sql.functions import lit

img_dir = "/Users/ukannika/work/choice/machine-learning/itmastery-2017/images"
81/4:
from sparkdl import readImages
from pyspark.sql.functions import lit
82/1:
from sparkdl import readImages
from pyspark.sql.functions import lit
82/2:
#Read images and Create training & test DataFrames for transfer learning
catsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/cats").withColumn("label", lit(1))
dogsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/dogs").withColumn("label", lit(0))

# Create training dataset and test dataset
cats_train, cats_test = catsImagesDataset.randomSplit([0.8, 0.2])
dogs_train, dogs_test = dogsImagesDataset.randomSplit([0.8, 0.2])

#dataframe for training a classification model
train_df = cats_train.unionAll(dogs_train)

#dataframe for testing the classification model
test_df = cats_test.unionAll(dogs_test)
82/3:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import pipeline
from sparkdl import DeepImageFeaturizer

featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol="label")
p = Pipeline(stages=[featurizer, lr])
p_model = p.fit(train_df)
82/4:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from sparkdl import DeepImageFeaturizer

featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol="label")
p = Pipeline(stages=[featurizer, lr])
p_model = p.fit(train_df)
83/1:
from sparkdl import readImages
from pyspark.sql.functions import lit
83/2:
#Read images and Create training & test DataFrames for transfer learning
catsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/cats").withColumn("label", lit(1))
dogsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/dogs").withColumn("label", lit(0))

# Create training dataset and test dataset
cats_train, cats_test = catsImagesDataset.randomSplit([0.8, 0.2])
dogs_train, dogs_test = dogsImagesDataset.randomSplit([0.8, 0.2])

#dataframe for training a classification model
train_df = cats_train.unionAll(dogs_train)

#dataframe for testing the classification model
test_df = cats_test.unionAll(dogs_test)
83/3:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from sparkdl import DeepImageFeaturizer

featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol="label")
p = Pipeline(stages=[featurizer, lr])
p_model = p.fit(train_df)
84/1:
from sparkdl import readImages
from pyspark.sql.functions import lit

#Read images and Create training & test DataFrames for transfer learning
catsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/cats").withColumn("label", lit(1))
dogsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/dogs").withColumn("label", lit(0))

# Create training dataset and test dataset
cats_train, cats_test = catsImagesDataset.randomSplit([0.8, 0.2])
dogs_train, dogs_test = dogsImagesDataset.randomSplit([0.8, 0.2])

#dataframe for training a classification model
train_df = cats_train.unionAll(dogs_train)

#dataframe for testing the classification model
test_df = cats_test.unionAll(dogs_test)
84/2:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from sparkdl import DeepImageFeaturizer

featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol="label")
p = Pipeline(stages=[featurizer, lr])
p_model = p.fit(train_df)
84/3:
predictions = p_model.transform(test_df)
predictions.select("filePath", "prediction").show(truncate=False)
84/4:
predictions = p_model.transform(test_df)
predictions.select("filePath", "prediction").show(100)
85/1:
from sparkdl import readImages
from pyspark.sql.functions import lit

#Read images and Create training & test DataFrames for transfer learning
catsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/cats").withColumn("label", lit(1))
dogsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/dogs").withColumn("label", lit(0))

# Create training dataset and test dataset
cats_train, cats_test = catsImagesDataset.randomSplit([0.8, 0.2])
dogs_train, dogs_test = dogsImagesDataset.randomSplit([0.8, 0.2])

#dataframe for training a classification model
train_df = cats_train.unionAll(dogs_train)

#dataframe for testing the classification model
test_df = cats_test.unionAll(dogs_test)
85/2:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from sparkdl import DeepImageFeaturizer

featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
lr = LogisticRegression(maxIter=10, regParam=0.05, elasticNetParam=0.3, labelCol="label")
p = Pipeline(stages=[featurizer, lr])
p_model = p.fit(train_df)
85/3:
predictions = p_model.transform(test_df)
predictions.show(truncate=False)
85/4:
predictions = p_model.transform(test_df)
predictions.show(truncate=False, 2)
85/5:
predictions = p_model.transform(test_df)
predictions.select("image").show(truncate=False)
85/6:
predictions = p_model.transform(test_df)
finalDF = predictions.select("image", "uri");
display(finalDF, 2)
85/7:
predictions = p_model.transform(test_df)
finalDF = predictions.select("image", "filepath", "probability");
display(finalDF, 2)
85/8:
from IPython.display import display
predictions = p_model.transform(test_df)
finalDF = predictions.select("image", "filepath", "probability");
display(finalDF, 2)
85/9:
from sparkdl import readImages
from pyspark.sql.functions import lit

#Read images and Create training & test DataFrames for transfer learning
catsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/cats").withColumn("label", lit(1))
dogsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/dogs").withColumn("label", lit(0))

# Create training dataset and test dataset
cats_train, cats_test = catsImagesDataset.randomSplit([0.8, 0.2])
dogs_train, dogs_test = dogsImagesDataset.randomSplit([0.8, 0.2])

#dataframe for training a classification model
train_df = cats_train.unionAll(dogs_train)

#dataframe for testing the classification model
test_df = cats_test.unionAll(dogs_test)
85/10:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from sparkdl import DeepImageFeaturizer

featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
lr = LogisticRegression(maxIter=10, regParam=0.05, elasticNetParam=0.3, labelCol="label")
p = Pipeline(stages=[featurizer, lr])
p_model = p.fit(train_df)
85/11:
from IPython.display import display
predictions = p_model.transform(test_df)
finalDF = predictions.select("image", "filepath", "probability");
display(finalDF, 10)
85/12:
from IPython.display import display
predictions = p_model.transform(test_df)
finalDF = predictions.select("image", "filepath", "probability");
final.DF.show(truncate=False).limit(2)
85/13:
from IPython.display import display
predictions = p_model.transform(test_df)
finalDF = predictions.select("image", "filepath", "probability");
finalDF.show(truncate=False).limit(2)
85/14:
from IPython.display import display
predictionsDataset = pipeline_model.transform(test_df)
predictionsDataset.select("filepath", "prediction").show(truncate=False).limit(10)
85/15:
from sparkdl import readImages
from pyspark.sql.functions import lit

#Read images and Create training & test DataFrames for transfer learning
catsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/cats").withColumn("label", lit(1))
dogsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/dogs").withColumn("label", lit(0))

# Create training dataset and test dataset
cats_train, cats_test = catsImagesDataset.randomSplit([0.8, 0.2])
dogs_train, dogs_test = dogsImagesDataset.randomSplit([0.8, 0.2])

#dataframe for training a classification model
train_df = cats_train.unionAll(dogs_train)

#dataframe for testing the classification model
test_df = cats_test.unionAll(dogs_test)
85/16:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from sparkdl import DeepImageFeaturizer

#Create MLlib Pipeline
featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
logisticRegression = LogisticRegression(maxIter=10, labelCol="label")
pipeline = Pipeline(stages=[featurizer, logisticRegression])
pipeline_model = pipeline.fit(train_df)
85/17:
from IPython.display import display
predictionsDataset = pipeline_model.transform(test_df)
predictionsDataset.select("filepath", "prediction").show(truncate=False).limit(10)
85/18:
from IPython.display import display
df = pipeline_model.transform(test_df)
df.select("filepath","prediction", proba_neg(df["probabulity"])).show(truncate=False)
85/19:
from IPython.display import display
from IPython.
df = pipeline_model.transform(test_df)
df.select("filepath","prediction", "probabulity").show(truncate=False)
85/20:
from IPython.display import display
from IPython.
df = pipeline_model.transform(test_df)
df.show(truncate=False)
85/21:
from IPython.display import display
df = pipeline_model.transform(test_df)
df.select("filepath", "prediction", "probability").show(truncate=False)
87/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Batch") \
                .getOrCreate()
87/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
87/3:
#Print
rawDataset.show()
87/4:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
87/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*");
87/6: transformDataset.show()
88/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
88/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
88/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
88/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
88/5:
# Start running the query that prints the streaming data to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("console") \
                              .start()

# Waits for the termination of this query
queryStream.awaitTermination()
89/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Batch") \
                .getOrCreate()
89/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
89/3:
#Print
rawDataset.show()
89/4:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
89/5:
#Read Stream Data From Kafka
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
89/6:
#Print
rawDataset.show()
89/7:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
89/8:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*");
89/9:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*");
89/10:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                 .select(functions.from_json("json", schema).alias("data")) \
                 .select("data.*");
90/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
90/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
90/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
90/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
90/5:
# Start running the query that prints the streaming data to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("console") \
                              .start()

# Waits for the termination of this query
queryStream.awaitTermination()
91/1:
from sparkdl import readImages
from pyspark.sql.functions import lit

#Read images and Create training & test DataFrames for transfer learning
catsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/cats").withColumn("label", lit(1))
dogsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/dogs").withColumn("label", lit(0))

# Create training dataset and test dataset
cats_train, cats_test = catsImagesDataset.randomSplit([0.8, 0.2])
dogs_train, dogs_test = dogsImagesDataset.randomSplit([0.8, 0.2])

#dataframe for training a classification model
train_df = cats_train.unionAll(dogs_train)

#dataframe for testing the classification model
test_df = cats_test.unionAll(dogs_test)
92/1:
from sparkdl import readImages
from pyspark.sql.functions import lit

#Read images and Create training & test DataFrames for transfer learning
catsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/cats").withColumn("label", lit(1))
dogsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/dogs").withColumn("label", lit(0))

# Create training dataset and test dataset
cats_train, cats_test = catsImagesDataset.randomSplit([0.8, 0.2])
dogs_train, dogs_test = dogsImagesDataset.randomSplit([0.8, 0.2])

#dataframe for training a classification model
train_df = cats_train.unionAll(dogs_train)

#dataframe for testing the classification model
test_df = cats_test.unionAll(dogs_test)
92/2:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from sparkdl import DeepImageFeaturizer

#Create MLlib Pipeline
featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
logisticRegression = LogisticRegression(maxIter=10, labelCol="label")
pipeline = Pipeline(stages=[featurizer, logisticRegression])
pipeline_model = pipeline.fit(train_df)
92/3:
from IPython.display import display
df = pipeline_model.transform(test_df)
df.select("filepath", "prediction", "probability").show(truncate=False)
93/1:
from sparkdl import readImages
from pyspark.sql.functions import lit

#Read images and Create training & test DataFrames for transfer learning
catsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/cats").withColumn("label", lit(1))
dogsImagesDataset = readImages("/Users/ukannika/work/choice/machine-learning/images/dogs").withColumn("label", lit(0))

# Create training dataset and test dataset
cats_train, cats_test = catsImagesDataset.randomSplit([0.8, 0.2])
dogs_train, dogs_test = dogsImagesDataset.randomSplit([0.8, 0.2])

#dataframe for training a classification model
train_df = cats_train.unionAll(dogs_train)

#dataframe for testing the classification model
test_df = cats_test.unionAll(dogs_test)
93/2:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from sparkdl import DeepImageFeaturizer

#Create MLlib Pipeline
featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
logisticRegression = LogisticRegression(maxIter=10, labelCol="label")
pipeline = Pipeline(stages=[featurizer, logisticRegression])
pipeline_model = pipeline.fit(train_df)
93/3:
from IPython.display import display
df = pipeline_model.transform(test_df)
df.select("filepath", "prediction", "probability").show(truncate=False)
96/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
96/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
97/1:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Batch") \
                .getOrCreate()
97/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
97/3:
#Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Batch") \
                .getOrCreate()
97/4:
#Read Stream Data From Kafka
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
97/5:
#Print
rawDataset.show()
97/6:
#Read Stream Data From Kafka
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
97/7:
#Print
rawDataset.show()
97/8:
#Print
rawDataset.select(value).show()
97/9:
#Print
rawDataset.show()
96/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
96/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
96/5:
# Start running the query that prints the streaming data to the console
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("console") \
                              .start()

# Waits for the termination of this query
queryStream.awaitTermination()
98/1:
# Create Spark Session
sparkSession = SparkSession \
                .builder \
                .appName("ITMastery2017-Streaming") \
                .getOrCreate()
98/2:
#Read Stream Data From Kafka
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "itmastery2017") \
                            .load()
98/3:
#Create a Schema 
from pyspark.sql.types import StructType
schema = StructType().add("property", "string").add("state", "string")
98/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
98/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "file:///Users/ukannika/work/choice/checkpoint") \
                              .start("file:///Users/ukannika/work/choice/save")
                
# Waits for the termination of this query
queryStream.awaitTermination()
101/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
101/2:
from pyspark.sql.types import StructType
from pyspark.sql.types import DataTypes
schema = StructType().add("adults", DataTypes.IntegerType)
                     .add("bookDateTime", DataTypes.TimestampType).add("checkInDate", DataTypes.TimestampType)
                     .add("checkOutDate", DataTypes.TimestampType).add("confNumber", DataTypes.StringType)
                     .add("currency", DataTypes.StringType).add("customerCountry", DataTypes.StringType)
                     .add("customerId", DataTypes.StringType).add("loyaltyAccountNumber", DataTypes.StringType)
                     .add("minors", DataTypes.StringType).add("numberOfRooms", DataTypes.StringType)
                     .add("propertyId", DataTypes.StringType).add("ratePlan", DataTypes.StringType)
                     .add("roomtypesPos", DataTypes.StringType).add("roomtypesItem", DataTypes.StringType)
                     .add("stayDuration", DataTypes.StringType).add("totalAmount", DataTypes.StringType)
101/3:
from pyspark.sql.types import StructType
from pyspark.sql.types import DataTypes
schema = StructType().add("adults", DataTypes.IntegerType) \
                     .add("bookDateTime", DataTypes.TimestampType).add("checkInDate", DataTypes.TimestampType) \
                     .add("checkOutDate", DataTypes.TimestampType).add("confNumber", DataTypes.StringType) \
                     .add("currency", DataTypes.StringType).add("customerCountry", DataTypes.StringType) \
                     .add("customerId", DataTypes.StringType).add("loyaltyAccountNumber", DataTypes.StringType) \
                     .add("minors", DataTypes.StringType).add("numberOfRooms", DataTypes.StringType) \
                     .add("propertyId", DataTypes.StringType).add("ratePlan", DataTypes.StringType) \
                     .add("roomtypesPos", DataTypes.StringType).add("roomtypesItem", DataTypes.StringType) \
                     .add("stayDuration", DataTypes.StringType).add("totalAmount", DataTypes.StringType) \
101/4:
from pyspark.sql.types import StructType
from pyspark.sql.types import DataTypes
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/5:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/6:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/7:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") 
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") 
                     .add("checkOutDate", "timestamp").add("confNumber", "string") 
                     .add("currency", "string").add("customerCountry", "string") 
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") 
                     .add("minors", "string").add("numberOfRooms", "integer") 
                     .add("propertyId", "string").add("ratePlan", "string") 
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") 
                     .add("stayDuration", "string").add("totalAmount", "double")
101/8:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") 
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/9:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/10:
rawDataset = sparkSession.read \
                         .option("sep", "\t") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/11: rawDataset.show()
101/12:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
101/13:
rawDataset = sparkSession.read \
                         .option("sep", "\t") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/14: rawDataset.show()
101/15: rawDataset.show()
101/16:
rawDataset = sparkSession.read \
                         .option("sep", "\t") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset123.csv")
101/17:
rawDataset = sparkSession.read \
                         .option("sep", "\t") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/18:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "string") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/19:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "string") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/20:
rawDataset = sparkSession.read \
                         .option("sep", "\t") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/21: rawDataset.show()
101/22:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "string") \
101/23:
rawDataset = sparkSession.read \
                         .option("sep", "\t") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/24: rawDataset.show()
101/25:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/26:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/27: rawDataset.show()
101/28:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/29:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/30: rawDataset.show()
101/31:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "string") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/32:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/33: rawDataset.show()
101/34: rawDataset.show()
101/35:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
101/36:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "string") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/37:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/38: rawDataset.show()
101/39:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/40: rawDataset.show()
101/41:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "string") \
101/42:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/43: rawDataset.show()
101/44:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "string") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
101/45:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm:ss a") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/46: rawDataset.show()
101/47:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "string") \
                     .add("bookDateTime", "timestamp").add("checkInDate", "timestamp") \
                     .add("checkOutDate", "timestamp").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/48:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/49: rawDataset.show()
101/50:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
101/51:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/52: rawDataset.show()
101/53:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "timestamp")
101/54:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/55: rawDataset.show()
101/56:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "timestamp")
101/57:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "MM/dd/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/58: rawDataset.show()
101/59:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string")
101/60:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "MM/dd/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/61: rawDataset.show()
101/62:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
101/63:
rawDataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "MM/dd/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("file:///Users/ukannika/Downloads/booking-dataset.csv")
101/64: rawDataset.show()
101/65:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
103/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
103/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
103/3:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
103/4:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
104/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
104/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
104/3:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
104/4:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
105/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
105/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
105/3:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
106/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
106/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
106/3:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
107/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
107/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
107/3:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
108/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
108/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
108/3:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
108/4: rawDataset.show()
108/5: dataset.show()
108/6:
dataset.write \
.format("json") \
.start("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.json");
108/7:
dataset.write \
       .json("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.json");
108/8:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
                         .coalesce(1)
108/9:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv")
                         .repartition(1)
108/10:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv") \  
                         .coalesce(1)
108/11:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv") \ 
                         .coalesce(1)
108/12:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv") \
108/13:
dataset = sparkSession.read \
                         .option("sep", ",") \
                         .option("header", "true") \
                         .option("dateFormat", "dd/MM/yyyy hh:mm") \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.csv") \
                                             .coalesce(1)
108/14:
dataset.write \
       .json("s3a://choice-desertcodecamp-spark-demo/source/booking-dataset.json")
108/15:
dataset = sparkSession.read \
                         .schema(schema) \
                         .json("s3a://choice-desertcodecamp-spark-demo/source/json/booking-dataset.json") \
108/16: dataset.show(5)
108/17:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
108/18:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
108/19:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .json("s3a://choice-desertcodecamp-spark-demo/source/json/booking-dataset.json") \
108/20:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
108/21:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint")     \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
108/22:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .json("s3a://choice-desertcodecamp-spark-demo/source/json") \
108/23:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint")     \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
108/24:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .json("s3a://choice-desertcodecamp-spark-demo/source/json") \
108/25:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("topic", "desertcodecamp-demo") \
                      .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint")     \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
108/26:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string") \
108/27:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
108/28:
dataset = sparkSession.read \
                         .schema(schema) \
                         .json("s3a://choice-desertcodecamp-spark-demo/source/json") \
108/29: dataset.show(5)
108/30: dataset.show(5)
108/31:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string").add("checkInDate", "string") \
                     .add("checkOutDate", "string").add("confNumber", "string") \
                     .add("currency", "string").add("customerCountry", "string") \
                     .add("customerId", "string").add("loyaltyAccountNumber", "string") \
                     .add("minors", "string").add("numberOfRooms", "integer") \
                     .add("propertyId", "string").add("ratePlan", "string") \
                     .add("roomtypesPos", "string").add("roomtypesItem", "string") \
                     .add("stayDuration", "string").add("totalAmount", "double")
108/32:
dataset = sparkSession.read \
                         .schema(schema) \
                         .json("s3a://choice-desertcodecamp-spark-demo/source/json") \
108/33: dataset.show(5)
108/34:
from pyspark.sql import functions
dataset.select(functions.to_json(schema)).show(5)
108/35:
from pyspark.sql import functions
dataset.select(functions.to_json($"c1", $"c2", $"c3")).show(5)
108/36:
from pyspark.sql import functions
dataset.select(functions.to_json("c1", "c2", "c3")).show(5)
108/37:
from pyspark.sql import functions
dataset.select(functions.to_json(struct($"c1"))).show(5)
108/38:
from pyspark.sql import functions
dataset.select(functions.to_json(struct("c1"))).show(5)
108/39:
from pyspark.sql import functions
dataset.select(functions.to_json(StructType($"adults"))).show(5)
108/40:
from pyspark.sql import functions
dataset.select(functions.to_json(StructType("adults"))).show(5)
108/41:
from pyspark.sql import functions
dataset.select(functions.to_json(StructType(functions.col("adults")))).show(5)
108/42:
from pyspark.sql import functions
dataset.select(functions.to_json(StructType("adults"))).show(5)
108/43:
from pyspark.sql import functions
dataset.select(functions.to_json(dataset.adults)).show(5)
108/44:
from pyspark.sql import functions
dataset.select(functions.to_json(StructType(dataset.adults)).show(5)
108/45:
from pyspark.sql import functions
dataset.select(functions.to_json(StructType(dataset.adults))).show(5)
108/46:
from pyspark.sql import functions
dataset.select(functions.to_json(dataset.adults, dataset.bookDateTime)).show(5)
108/47:
from pyspark.sql import functions
dataset.toJSON()
108/48:
from pyspark.sql import functions
dataset.toJSON().show(5)
108/49:
from pyspark.sql import functions
dataset.toJSON().collect().show(5)
108/50:
from pyspark.sql import functions
dataset.toJSON()
108/51:
from pyspark.sql import functions
dataset = dataset.toJSON()
dataset
108/52:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string") \
108/53:
dataset = sparkSession.read \
                         .schema(schema) \
                         .txt("s3a://choice-desertcodecamp-spark-demo/source/json") \
108/54:
dataset = sparkSession.read \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/json") \
108/55:
from pyspark.sql import functions
dataset = dataset.show(5)
108/56:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string") \
108/57:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/txt") \
108/58:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("topic", "desertcodecamp-demo") \
                      .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint")     \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
108/59:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
108/60:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string") \
108/61:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/txt") \
108/62:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("topic", "desertcodecamp-demo") \
                      .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint")     \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
111/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
111/2:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string") \
111/3:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/txt") \
111/4:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("topic", "desertcodecamp-demo") \
                      .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint")     \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
113/1:
schema = StructType().add("adults", "integer")
                     .add("bookDateTime", "string")
                     .add("checkInDate", "string")
                     .add("checkOutDate", "string")
                     .add("confNumber", "string")
                     .add("currency", "string")
                     .add("customerCountry", "string")
                     .add("customerId", "string")
                     .add("loyaltyAccountNumber", "string")
                     .add("minors", "integer")
                     .add("numberOfRooms", "integer")
                     .add("propertyId", "string")
                     .add("ratePlan", "string")
                     .add("roomtypesPos", "string")
                     .add("roomtypesItem", "string")
                     .add("stayDuration", "string")
                     .add("totalAmount", "double")
113/2:
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "integer") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
113/3:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "integer") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
120/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
120/2:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string") \
120/3:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/txt") \
120/4:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("topic", "desertcodecamp-demo") \
                      .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/S3-To-Kafka")     \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
121/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
121/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "integer") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
121/3:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .load()
                            .coalesce(1)
121/4:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .load() \
                            .coalesce(1)
121/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
121/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .format("console") \
                              .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
122/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
122/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
122/3:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .load() \
                            .coalesce(1)
122/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
122/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .format("console") \
                              .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
123/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
123/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
123/3:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .load() \
                            .coalesce(1)
123/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
123/5:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest")
                            .load() \
                            .coalesce(1)
123/6:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
123/7:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
123/8:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .format("console") \
                              .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
124/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
124/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
124/3:
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest")
                            .load() \
                            .coalesce(1)
124/4:
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
124/5: rawDataset.show()
124/6:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
124/7: transformDataset.show()
124/8:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json")
124/9: transformDataset.show()
124/10:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
124/11: transformDataset.show()
124/12:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema)) \
124/13: transformDataset.show()
124/14:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
124/15:
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
124/16:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
124/17: transformDataset.show()
124/18:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
124/19: transformDataset.show()
124/20:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
124/21: transformDataset.show()
124/22:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
124/23: transformDataset.show()
124/24:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
124/25:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json")
124/26: transformDataset.show()
124/27:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema)
124/28:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema))
124/29: transformDataset.show()
124/30:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select("json)
124/31:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select("json")
124/32: transformDataset.show()
124/33:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
124/34:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
124/35: transformDataset.show()
124/36:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer").add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
124/37:
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
124/38:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
124/39: transformDataset.show()
124/40:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
124/41: transformDataset.show()
124/42:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
124/43: transformDataset.show()
126/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
126/2:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string") \
126/3:
dataset = sparkSession.read \
                         .schema(schema) \
                         .csv("s3a://choice-desertcodecamp-spark-demo/source/txt") \
126/4: read.show(1)
126/5: dataset.show(1)
126/6: dataset.show(false)
126/7:
dataset = sparkSession.read \
                         .schema(schema) \
                         .text("s3a://choice-desertcodecamp-spark-demo/source/txt") \
126/8: dataset.show()
126/9:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string") \
126/10:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .text("s3a://choice-desertcodecamp-spark-demo/source/txt") \
126/11:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("topic", "desertcodecamp-demo") \
                      .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/S3-To-Kafka")     \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
127/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
127/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
127/3:
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
127/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
127/5: transformDataset.show()
127/6:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
127/7:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
127/8:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .format("console") \
                              .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
128/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
128/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
128/3:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load()
128/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
128/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .format("console") \
                              .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
129/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
129/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
129/3:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load()
129/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
129/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .format("console") \
                              .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
130/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-S3-To-Kafka") \
                .getOrCreate()
130/2:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string") \
130/3:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .text("s3a://choice-desertcodecamp-spark-demo/source/txt") \
130/4:
queryStream =  dataset.writeStream \
                      .format("kafka") \
                      .option("kafka.bootstrap.servers", "localhost:9092") \
                      .option("topic", "desertcodecamp-demo") \
                      .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/S3-To-Kafka")     \
                      .start()

# Waits for the termination of this query
queryStream.awaitTermination()
131/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
131/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "integer") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
131/3:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .load()
                            .coalesce(1)
131/4:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .load() \
                            .coalesce(1)
131/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
131/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/Kafka-To-S3")     \
                              .start("s3a://choice-desertcodecamp-spark-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
132/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
132/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "integer") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
132/3:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
132/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
132/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/Kafka-To-S3")     \
                              .start("s3a://choice-desertcodecamp-spark-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
133/1:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/Kafka-To-S3")     \
                              .start("s3a://choice-desertcodecamp-spark-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
133/2:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
133/3:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "integer") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
133/4:
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
133/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
133/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/Kafka-To-S3")     \
                              .start("s3a://choice-desertcodecamp-spark-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
133/7:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
133/8:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
133/9:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/Kafka-To-S3")     \
                              .start("s3a://choice-desertcodecamp-spark-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
133/10:
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
134/1:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
134/2:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
134/3:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "integer") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
134/4:
rawDataset = sparkSession.read \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load() \
                            .coalesce(1)
134/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
134/6: transformDataset.show()
134/7:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
134/8: transformDataset.show()
134/9:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/Kafka-To-S3")     \
                              .start("s3a://choice-desertcodecamp-spark-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
134/10: transformDataset.show()
134/11:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema))
134/12:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema).alias("data")) \
                             .select("data.*")
134/13: transformDataset.show()
134/14:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as jsonString") \
                             .select(functions.from_json("jsonString", schema))
134/15: transformDataset.show()
135/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
135/2:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-Console") \
                .getOrCreate()
136/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
137/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-Console") \
                .getOrCreate()
138/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-Console") \
                .getOrCreate()
138/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
138/3:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load()
138/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
138/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .format("console") \
                              .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
139/1:
sparkSession = SparkSession \
                .builder \
                .appName("DesertCodeCamp2017-Kafka-To-S3") \
                .getOrCreate()
139/2:
from pyspark.sql.types import StructType
schema = StructType().add("adults", "integer") \
                     .add("bookDateTime", "string") \
                     .add("checkInDate", "string") \
                     .add("checkOutDate", "string") \
                     .add("confNumber", "string") \
                     .add("currency", "string") \
                     .add("customerCountry", "string") \
                     .add("customerId", "string") \
                     .add("loyaltyAccountNumber", "string") \
                     .add("minors", "string") \
                     .add("numberOfRooms", "integer") \
                     .add("propertyId", "string") \
                     .add("ratePlan", "string") \
                     .add("roomtypesPos", "string") \
                     .add("roomtypesItem", "string") \
                     .add("stayDuration", "string") \
                     .add("totalAmount", "double")
139/3:
rawDataset = sparkSession.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load()
139/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
139/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-desertcodecamp-spark-demo/checkpoint/Kafka-To-S3")     \
                              .start("s3a://choice-desertcodecamp-spark-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
143/1:
sparkSession = SparkSession \
                .builder \
                .appName("Parquet-to-CSV") \
                .getOrCreate()
144/1:
sparkSession = SparkSession \
                .builder \
                .appName("Parquet-to-CSV") \
                .getOrCreate()
144/2:
dataset = sparkSession.read\
                      .parquet("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/1:
sparkSession = SparkSession \
                .builder \
                .appName("Parquet-to-CSV") \
                .getOrCreate()
146/2:
dataset = sparkSession.read\
                      .parquet("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/3:
dataset = sparkSession.read\
                      .txt("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/4:
dataset = sparkSession.read\
                      .text("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/5: dataset.show
146/6: dataset.show()
146/7:
sparkSession = SparkSession \
                .builder \
                .appName("Parquet-to-CSV") \
                .getOrCreate()
146/8:
dataset = sparkSession.read\
                      .text("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/9:
count = dataset.count()
count
146/10:
sparkSession = SparkSession \
                .builder \
                .appName("Parquet-to-CSV") \
                .getOrCreate()
146/11:
dataset = sparkSession.read\
                      .text("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/12: dataset.show()
146/13:
sparkSession = SparkSession \
                .builder \
                .appName("Parquet-to-CSV") \
                .getOrCreate()
146/14:
dataset = sparkSession.read\
                      .parquet("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/15: dataset.show()
146/16: dataset.write.option("header", true).csv("s3a://choice-reviews-ratings/T360/BOOKINGS/")
146/17: dataset.write.option("header", "true").csv("s3a://choice-reviews-ratings/T360/BOOKINGS/")
146/18: dataset.write.option("header", "true")..csv("s3a://choice-reviews-ratings/T360/customer_activity_bookings_v2_prod/")
146/19: dataset.write.option("header", "true").csv("s3a://choice-reviews-ratings/T360/customer_activity_bookings_v2_prod/")
146/20:
dataset = sparkSession.read\
                      .parquet("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/21: dataset.write.option("header", "true").csv("s3a://choice-reviews-ratings/T360/customer_activity_bookings_v2_prod/")
146/22:
dataset = sparkSession.read\
                      .text("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/23: dataset.show()
146/24:
dataset = sparkSession.read\
                      .option("header", "true")
                      .text("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/25:
dataset = sparkSession.read\
                      .option("header", "true") \
                      .text("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/26: dataset.show()
146/27:
dataset = sparkSession.read\
                      .option("header", "true") \
                      .parquet("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
146/28: dataset.show()
146/29: dataset.write.json("s3a://choice-reviews-ratings/T360/customer_activity_bookings_v2_prod")
146/30: dataset.write.json("file:///Users/ukannika/work/choice/data_for_choice_review")
146/31: dataset.write.json("file:///Users/ukannika/work/choice/data_for_choice_review/bookings")
147/1:
sparkSession = SparkSession \
                .builder \
                .appName("Parquet-to-JSON") \
                .getOrCreate()
147/2:
dataset = sparkSession.read \
                      .parquet("s3a://choice-dap-delivery-preprod/UMA/customer_activity_booking_links_v2_prod") \
                      .coalesce(1)
147/3: dataset.write.json("file:///choice-reviews-ratings/T360/customer_activity_booking_links_v2_prod")
147/4: dataset.write.json("s3a://choice-reviews-ratings/T360/customer_activity_booking_links_v2_prod")
147/5: dataset.write.json("s3a://choice-reviews-ratings/T360/customer_activity_booking_links_v2_prod")
147/6:
sparkSession = SparkSession \
                .builder \
                .appName("Parquet-to-JSON") \
                .getOrCreate()
147/7:
dataset = sparkSession.read \
                      .parquet("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
                      .coalesce(1)
147/8: dataset.write.json("s3a://choice-reviews-ratings/T360/customer_activity_bookings_v2_prod")
148/1:
sparkSession = SparkSession \
                .builder \
                .appName("Parquet-to-JSON") \
                .getOrCreate()
148/2:
dataset = sparkSession.read \
                      .parquet("s3a://choice-dap-delivery-preprod/UMA/customer_activity_bookings_v2_prod") \
                      .coalesce(1)
148/3: dataset.write.json("s3a://choice-reviews-ratings/T360/customer_activity_bookings_v2_prod")
150/1:
spark = SparkSession \
                .builder \
                .appName("Test") \
                .getOrCreate()
152/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
152/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
152/3:
dataset = sparkSession.readStream \
                         .schema(schema) \
                         .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
152/4:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
153/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
153/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
153/3:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
153/4:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
153/5:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
153/6:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
153/7:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
153/8:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
154/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
154/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
154/3:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
155/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
155/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
155/3:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
155/4:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
155/5:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
156/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
156/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
156/3:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
157/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
157/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
157/3:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
158/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
158/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
158/3:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
159/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
159/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
159/3:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
160/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
160/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
160/3:
dataset = spark.readStream \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
160/4: dataset.show()
160/5: dataset.show
160/6: dataset.show()
160/7:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
160/8: dataset.show()
160/9:
dataset = spark.read \
               .schema(schema) \
               .csv("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
160/10: dataset.show()
160/11:
dataset = spark.read \
               .schema(schema) \
               .option("header", "true")
               .option("sep", )
               .csv("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
160/12:
dataset = spark.read \
               .schema(schema) \
               .option("header", "true")
               .option("sep", "|")
               .csv("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
160/13:
dataset = spark.read \
               .schema(schema) \
               .option("header", "true") \
               .option("sep", "|") \
               .csv("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
160/14: dataset.show()
160/15:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings") \
       .start()
160/16:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings")
160/17:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings") \
       .save()
163/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
163/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string") \
163/3:
dataset = spark.read \
               .schema(schema) \
               .option("header", "true") \
               .option("sep", "|") \
               .csv("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
163/4:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings") \
       .save()
163/5:
dataset = spark.read \
               .schema(schema) \
               .option("header", "true") \
               .option("sep", "|") \
               .json("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
163/6: dataset.show()
163/7:
dataset = spark.read \
               .schema(schema) \
               .json("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
163/8:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
163/9:
dataset = spark.read \
               .schema(schema) \
               .json("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
163/10: dataset.show()
163/11:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
163/12:
dataset = spark.read \
               .schema(schema) \
               .json("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
163/13: dataset.show()
163/14:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
163/15:
dataset = spark.read \
               .schema(schema) \
               .txt("s3a://choice-spark-structured-streaming-demo/source/bookings1") \
163/16:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
163/17:
dataset = spark.read \
               .schema(schema) \
               .txt("s3a://choice-spark-structured-streaming-demo/source/bookings1")
163/18: dataset.show()
163/19:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1")
163/20: dataset.show()
163/21:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings") \
       .save()
165/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Kafka-S3") \
                .getOrCreate()
165/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
165/3:
rawDataset = spark.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "desertcodecamp-demo") \
                            .option("startingOffsets", "earliest") \
                            .load()
165/4:
rawDataset = spark.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "bookings") \
                            .option("startingOffsets", "earliest") \
                            .load()
165/5:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
165/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-spark-structured-streaming-demo/checkpoint/Kafka-To-S3")\
                              .trigger("1 minute")
                              .start("s3a://choice-spark-structured-streaming-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
165/7:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-spark-structured-streaming-demo/checkpoint/Kafka-To-S3")\
                              .trigger("1 minute") \
                              .start("s3a://choice-spark-structured-streaming-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
165/8:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-spark-structured-streaming-demo/checkpoint/Kafka-To-S3")\
                              .trigger(processingTime="10 seconds") \
                              .start("s3a://choice-spark-structured-streaming-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
167/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Kafka-S3") \
                .getOrCreate()
168/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Kafka-S3") \
                .getOrCreate()
168/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
168/3:
rawDataset = spark.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "bookings") \
                            .option("startingOffsets", "earliest") \
                            .load()
168/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
168/5:
rawDataset = spark.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "bookings") \
                            .option("startingOffsets", "earliest") \
                            .load()
168/6:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
168/7:
from pyspark.sql import functions
df = df.groupBy(functions.window("bookdatetime", "1 minutes), "propid").count()
168/8:
from pyspark.sql import functions
df = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
168/9:
from pyspark.sql import functions
df = df.groupBy(functions.window("bookdatetime", "1 minutes), "propid").count()
168/10:
from pyspark.sql import functions
df = df.groupBy(functions.window("bookdatetime", "1 minutes), df.propid).count()
168/11:
from pyspark.sql import functions
df = df.groupBy(functions.window("bookdatetime", "1 minutes"), df.propid).count()
168/12:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .format("console") \
                              .trigger(processingTime="10 seconds") \
                              .start()
# Waits for the termination of this query
queryStream.awaitTermination()
168/13:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Kafka-S3") \
                .getOrCreate()
168/14:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
168/15:
rawDataset = spark.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "bookings") \
                            .option("startingOffsets", "earliest") \
                            .load()
168/16:
from pyspark.sql import functions
df = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
168/17:
from pyspark.sql import functions
df = df.groupBy(functions.window("bookdatetime", "1 minutes"), df.propid).count()
168/18:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = df.writeStream \
                              .format("console") \
                              .trigger(processingTime="10 seconds") \
                              .start()
# Waits for the termination of this query
queryStream.awaitTermination()
168/19:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = df.writeStream \
                .format("console") \
                .trigger(processingTime="10 seconds") \
                .start()
              .outputMode("Complete")
# Waits for the termination of this query
queryStream.awaitTermination()
168/20:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = df.writeStream \
                .format("console") \
                .trigger(processingTime="10 seconds") \
              .outputMode("Complete") \
                .start()
# Waits for the termination of this query
queryStream.awaitTermination()
168/21:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = df.writeStream \
                .format("console") \
                .trigger(processingTime="10 seconds") \
              .outputMode("Append") \
                .start()
# Waits for the termination of this query
queryStream.awaitTermination()
168/22:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = df.writeStream \
                .format("console") \
                .trigger(processingTime="10 seconds") \
              .outputMode("Update") \
                .start()
# Waits for the termination of this query
queryStream.awaitTermination()
169/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
169/2:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
169/3:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1")
169/4:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings") \
       .save()
171/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Stateful-Aggregation") \
                .getOrCreate()
171/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
171/3:
dataset = spark.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "bookings") \
                            .option("startingOffsets", "earliest") \
                            .load()
171/4:
dataset = spark.readStream \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
172/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
172/2:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
172/3:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings1")
172/4:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings") \
       .save()
172/5:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
171/5:
dataset = spark.readStream \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
171/6:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
171/7:
from pyspark.sql import functions
dataset = dataset
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid)
            .count()
171/8:
from pyspark.sql import functions
dataset = dataset \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
171/9:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = dataset.writeStream \
                .format("console") \
                .trigger(processingTime="10 seconds") \
                .outputMode("Update") \
                .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
172/6:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
172/7:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
172/8:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings2")
172/9:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
171/10:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = dataset.writeStream \
                .format("console") \
                .option("truncate", false)
                .trigger(processingTime="10 seconds") \
                .outputMode("Update") \
                .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
171/11:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Stateful-Aggregation") \
                .getOrCreate()
171/12:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
171/13:
dataset = spark.readStream \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
171/14:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
171/15:
from pyspark.sql import functions
dataset = dataset \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
171/16:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = dataset.writeStream \
                .format("console") \
                .option("truncate", false) \
                .trigger(processingTime="10 seconds") \
                .outputMode("Update") \
                .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
171/17:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = dataset.writeStream \
                .format("console") \
                .option("truncate", "false") \
                .trigger(processingTime="10 seconds") \
                .outputMode("Update") \
                .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
171/18:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("timestamp", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
171/19:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = dataset.writeStream \
                .format("console") \
                .option("truncate", "false") \
                .trigger(processingTime="10 seconds") \
                .outputMode("Update") \
                .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
172/10:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
172/11:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
172/12:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
172/13:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
172/14:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings1")
172/15:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
173/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Stateful-Aggregation") \
                .getOrCreate()
173/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
173/3:
dataset = spark.readStream \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
173/4:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
173/5:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
173/6:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
173/7:
dataset = spark.readStream \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
173/8:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
173/9:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
173/10:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = dataset.writeStream \
                .format("console") \
                .option("truncate", "false") \
                .trigger(processingTime="10 seconds") \
                .outputMode("Update") \
                .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
172/16:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
172/17:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings2")
172/18:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
172/19:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings1")
172/20:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
172/21:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings2")
172/22:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
174/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Kafka-S3") \
                .getOrCreate()
174/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
174/3:
rawDataset = spark.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "bookings") \
                            .option("startingOffsets", "earliest") \
                            .load()
174/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
174/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-spark-structured-streaming-demo/checkpoint/Kafka-To-S3")\
                              .trigger(processingTime="10 seconds") \
                              .start("s3a://choice-spark-structured-streaming-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
174/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-spark-structured-streaming-demo/checkpoint/Kafka-To-S3")\
                              .trigger(processingTime="10 seconds") \
                              .start("s3a://choice-spark-structured-streaming-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
175/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
175/2:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
175/3:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
175/4:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
175/5:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1")
175/6:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings") \
       .save()
175/7:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings2")
175/8:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings") \
       .save()
176/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Kafka-S3") \
                .getOrCreate()
176/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
176/3:
rawDataset = spark.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "bookings") \
                            .option("startingOffsets", "earliest") \
                            .load()
176/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
176/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-spark-structured-streaming-demo/checkpoint/Kafka-To-S3")\
                              .trigger(processingTime="10 seconds") \
                              .start("s3a://choice-spark-structured-streaming-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
177/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Stateful-Stream-Processing") \
                .getOrCreate()
177/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
177/3:
dataset = spark.readStream \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
177/4:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
177/5:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
177/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = dataset.writeStream \
                .format("console") \
                .option("truncate", "false") \
                .trigger(processingTime="10 seconds") \
                .outputMode("Update") \
                .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
178/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
178/2:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
178/3:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings1")
178/4:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
178/5:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
178/6:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings2")
178/7:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
181/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Kafka-S3") \
                .getOrCreate()
181/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
181/3:
rawDataset = spark.readStream \
                            .format("kafka") \
                            .option("kafka.bootstrap.servers", "localhost:9092") \
                            .option("subscribe", "bookings") \
                            .option("startingOffsets", "earliest") \
                            .load()
181/4:
from pyspark.sql import functions
transformDataset = rawDataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
181/5:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = transformDataset.writeStream \
                              .outputMode("append") \
                              .format("parquet") \
                              .option("checkpointLocation", "s3a://choice-spark-structured-streaming-demo/checkpoint/Kafka-To-S3")\
                              .trigger(processingTime="10 seconds") \
                              .start("s3a://choice-spark-structured-streaming-demo/destination")
                
# Waits for the termination of this query
queryStream.awaitTermination()
182/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-S3-Kafka") \
                .getOrCreate()
182/2:
from pyspark.sql.types import StructType
schema = StructType().add("value", "string")
182/3:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/bookings1")
182/4:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation") \
       .save()
182/5:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "bookings") \
       .save()
183/1:
spark = SparkSession \
                .builder \
                .appName("PhxDataConference-Stateful-Stream-Processing") \
                .getOrCreate()
183/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("brand", "string") \
                     .add("propid", "string") \
                     .add("visitorid", "string") \
                     .add("guestrating", "string")
183/3:
dataset = spark.readStream \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation1") \
               .option("startingOffsets", "earliest") \
               .load()
183/4:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
183/5:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
183/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = dataset.writeStream \
                .format("console") \
                .option("truncate", "false") \
                .trigger(processingTime="10 seconds") \
                .outputMode("Update") \
                .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
182/6:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings1")
182/7:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation1") \
       .save()
182/8:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation1") \
       .save()
182/9:
dataset = spark.read \
               .schema(schema) \
               .text("s3a://choice-spark-structured-streaming-demo/source/stateful-aggregation/bookings2")
182/10:
dataset.write\
       .format("kafka") \
       .option("kafka.bootstrap.servers", "localhost:9092") \
       .option("topic", "stateful-aggregation1") \
       .save()
184/1:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/2:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/3:
dataset = spark.readStream \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/4:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/5:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
184/6:
# Start running the query that writes the streaming data to the local directory as a parquet format
queryStream = dataset.writeStream \
                .format("console") \
                .option("truncate", "false") \
                .trigger(processingTime="10 seconds") \
                .outputMode("Update") \
                .start()
                
# Waits for the termination of this query
queryStream.awaitTermination()
184/7:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/8:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/9:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load().show()
184/10:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/11:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/12:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/13:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
184/14: dataset.show()
184/15:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*") 
        
        dataset.show()
184/16:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*") \
        
        dataset.show()
184/17:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/18:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/19:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/20: dataset.show()
184/21:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/22:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/23:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/24:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/25: dataset.show()
184/26: dataset.show()
184/27:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/28: dataset.show()
184/29:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/30:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/31:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
184/32: dataset.show()
184/33:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/34:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/35:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/36:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/37:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
184/38: dataset.show()
184/39:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/40:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
184/41:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
184/42: dataset.show()
184/43: dataset.show(truncate=false)
184/44: dataset.show(truncate)
184/45: dataset.show()
184/46:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/47:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/48:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/49:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/50:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/51:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/52:
from pyspark.sql import functions
dataset = dataset \
            .withWatermark("bookdatetime", "30 seconds") \
            .groupBy(functions.window("bookdatetime", "30 seconds"), dataset.propid) \
            .count()
184/53: dataset.show()
184/54:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/55:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/56:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/57:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/58: dataset.show()
184/59:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
184/60:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/61:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/62:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/63:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
184/64: dataset.show()
184/65:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/66:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "timestamp") \
                     .add("checkindate", "timestamp") \
                     .add("checkoutdate", "timestamp") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/67:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/68:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema)) \
184/69: dataset.show()
184/70:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/71:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/72:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/73:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/74: dataset.show()
184/75:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/76:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "long") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/77:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/78:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json")
184/79: dataset.show()
184/80:
spark = SparkSession \
                .builder \
                .appName("Stateful-Stream-Processing") \
                .getOrCreate()
184/81:
from pyspark.sql.types import StructType
schema = StructType().add("rownumber", "string") \
                     .add("bookdatetime", "string") \
                     .add("checkindate", "string") \
                     .add("checkoutdate", "string") \
                     .add("confnumber", "string") \
                     .add("propid", "string") \
184/82:
dataset = spark.read \
               .format("kafka") \
               .option("kafka.bootstrap.servers", "localhost:9092") \
               .option("subscribe", "stateful-aggregation") \
               .option("startingOffsets", "earliest") \
               .load()
184/83:
from pyspark.sql import functions
dataset = dataset.selectExpr("CAST(value AS STRING) as json") \
                             .select(functions.from_json("json", schema).alias("data")) \
                             .select("data.*")
184/84: dataset.show()
185/1:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'spark_options':'--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}
185/2:
def configs(pipeline_name, retry_delay = 15, emai_address=Team_LITTLE@choicehotels.com)
if env == "dev" or env == "qa" :
    derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
    derived2_config['retry_delay'] = timedelta(minutes=15)
    derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
    derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
    derived2_config['spark_options'] = '--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 3'
    derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
    derived2_config['retry_delay'] = timedelta(hours=1)
    derived2_config['email_address'] = 'team-dis2@choicehotels.com'
    derived2_config['schedule_interval'] = timedelta(minutes=15)
else:
raise ValueError('Invalid environment {}'.format(env))
185/3:
def configs(pipeline_name, retry_delay = 15)
if env == "dev" or env == "qa" :
    derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
    derived2_config['retry_delay'] = timedelta(minutes=15)
    derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
    derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
    derived2_config['spark_options'] = '--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 3'
    derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
    derived2_config['retry_delay'] = timedelta(hours=1)
    derived2_config['email_address'] = 'team-dis2@choicehotels.com'
    derived2_config['schedule_interval'] = timedelta(minutes=15)
else:
raise ValueError('Invalid environment {}'.format(env))
185/4:
def configs(pipeline_name)
if env == "dev" or env == "qa" :
    derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
    derived2_config['retry_delay'] = timedelta(minutes=15)
    derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
    derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
    derived2_config['spark_options'] = '--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 3'
    derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
    derived2_config['retry_delay'] = timedelta(hours=1)
    derived2_config['email_address'] = 'team-dis2@choicehotels.com'
    derived2_config['schedule_interval'] = timedelta(minutes=15)
else:
raise ValueError('Invalid environment {}'.format(env))
185/5:
def configs(pipeline_name, retry_delay = 15, emai_address=Team_LITTLE@choicehotels.com):
if env == "dev" or env == "qa" :
derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
derived2_config['retry_delay'] = timedelta(minutes=15)
derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
derived2_config['schedule_interval'] = timedelta(minutes=15)
elif env == "prod":
derived2_config['spark_options'] = '--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 3'
derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
derived2_config['retry_delay'] = timedelta(hours=1)
derived2_config['email_address'] = 'team-dis2@choicehotels.com'
derived2_config['schedule_interval'] = timedelta(minutes=15)
else:
raise ValueError('Invalid environment {}'.format(env))
185/6:
def configs(pipeline_name, retry_delay = 15, emai_address=Team_LITTLE@choicehotels.com):
    if env == "dev" or env == "qa" :
        print(dev)
185/7:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if env == "dev" or env == "qa" :
        print(dev)
185/8:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if env == "dev" or env == "qa" :
        print("dev")
185/9:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if env == "dev" or env == "qa" :
    print("dev")
185/10:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if env == "dev" or env == "qa" :
        print("dev")
185/11:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if env == "dev" or env == "qa" :
        print("dev")
185/12: configs(dev)
185/13: configs('dev')
185/14:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if pipeline_name == "dev" or env == "qa" :
        print("dev")
        
        configs(dev)
185/15:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if pipeline_name == "dev" or env == "qa" :
        print("dev")
        
configs(dev)
185/16:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if pipeline_name == "dev" or env == "qa" :
        print("dev")
        
configs("dev")
185/17:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if env == "dev" or env == "qa" :
        print(email_address)

    configs(env="dev")
185/18:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if env == "dev" or env == "qa" :
        print(email_address)

configs(env="dev")
185/19:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if env == "dev" or env == "qa" :
        print(email_address)

env="dev"
configs()
185/20:
def configs(pipeline_name, retry_delay = 15, emai_address='Team_LITTLE@choicehotels.com'):
    if pipeline_name == "dev" or env == "qa" :
        print(email_address)

configs("dev")
185/21:
def configs(pipeline_name, retry_delay = 15, email_address='Team_LITTLE@choicehotels.com'):
    if pipeline_name == "dev" or env == "qa" :
        print(email_address)

configs("dev")
185/22:
def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    if env == "dev" or env == "qa" :
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
        derived2_config['retry_delay'] = timedelta(minutes=15)
        derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
        derived2_config['spark_options'] = '--executor-memory ' + executor_memory + ' --driver-memory ' + driver_memory + ' --executor-cores ' + executor_cores +' --num-executors ' +num_executors +'
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
        derived2_config['retry_delay'] = timedelta(hours=1)
        derived2_config['email_address'] = 'team-dis2@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
        print(derived2_config['spark_options'])
        
configs("pipeline_name")
185/23:
def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    if env == "dev" or env == "qa" :
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
        derived2_config['retry_delay'] = timedelta(minutes=15)
        derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
        derived2_config['spark_options'] = '--executor-memory ' + executor_memory + ' --driver-memory ' + driver_memory + ' --executor-cores ' + executor_cores +' --num-executors ' +num_executors
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
        derived2_config['retry_delay'] = timedelta(hours=1)
        derived2_config['email_address'] = 'team-dis2@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
        print(derived2_config['spark_options'])
        
configs("pipeline_name")
185/24:
def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    if env == "dev" or env == "qa" :
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
        derived2_config['retry_delay'] = timedelta(minutes=15)
        derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
        derived2_config['spark_options'] = '--executor-memory ' + executor_memory + ' --driver-memory ' + driver_memory + ' --executor-cores ' + executor_cores +' --num-executors ' +num_executors
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
        derived2_config['retry_delay'] = timedelta(hours=1)
        derived2_config['email_address'] = 'team-dis2@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
        print(spark_options)
        
configs("pipeline_name")
185/25:
derived2_config = {
}

def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    if env == "dev" or env == "qa" :
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
        derived2_config['retry_delay'] = timedelta(minutes=15)
        derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
        derived2_config['spark_options'] = '--executor-memory ' + executor_memory + ' --driver-memory ' + driver_memory + ' --executor-cores ' + executor_cores +' --num-executors ' +num_executors
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
        derived2_config['retry_delay'] = timedelta(hours=1)
        derived2_config['email_address'] = 'team-dis2@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
        print(derived2_config['spark_options'])
        
configs("pipeline_name")
185/26:
derived2_config = {
}

def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    if env == "dev" or env == "qa" :
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
        derived2_config['retry_delay'] = timedelta(minutes=15)
        derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
        derived2_config['spark_options'] = '--executor-memory ' + executor_memory + ' --driver-memory ' + driver_memory + ' --executor-cores ' + executor_cores +' --num-executors ' +num_executors
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
        derived2_config['retry_delay'] = timedelta(hours=1)
        derived2_config['email_address'] = 'team-dis2@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
        
configs("pipeline_name")
185/27:
derived2_config = {
}

def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    if env == "dev" or env == "qa" :
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
        derived2_config['retry_delay'] = timedelta(minutes=15)
        derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
        derived2_config['retry_delay'] = timedelta(hours=1)
        derived2_config['email_address'] = 'team-dis2@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
        
configs("pipeline_name")
185/28:
derived2_config = {
}

def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    if env == "dev" or env == "qa" :
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
        derived2_config['retry_delay'] = timedelta(minutes=15)
        derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":

        derived2_config['schedule_interval'] = timedelta(minutes=15)
        
configs("pipeline_name")
185/29:
derived2_config = {
}

def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    if env == "dev" or env == "qa" :
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
        derived2_config['retry_delay'] = timedelta(minutes=15)
        derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
        derived2_config['schedule_interval'] = timedelta(minutes=15)
        
configs("pipeline_name")
185/30:
derived2_config = {
}

def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    if env == "dev" or env == "qa" :
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=sit'
        derived2_config['retry_delay'] = timedelta(minutes=15)
        derived2_config['email_address'] = 'Team_LITTLE@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
    elif env == "prod":
        derived2_config['extra_java_options'] = '-Dpipeline.name=' + pipeline_name + ' -Dqualifier.name=ec'
        derived2_config['retry_delay'] = timedelta(hours=1)
        derived2_config['email_address'] = 'team-dis2@choicehotels.com'
        derived2_config['schedule_interval'] = timedelta(minutes=15)
        
configs("pipeline_name")
185/31:
def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
     'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
     'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
     'yarn_queue_name':'DERIVED2',
     'spark_options':'--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 2',
     'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
     'files':'metrics.properties,log4j.properties',
     'rsync_sleep_minutes': 3,
     'application_params':'',
     print(spark_options)
configs("application_jar")
185/32:
def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
     'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster'
     'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar'
     'yarn_queue_name':'DERIVED2'
     'spark_options':'--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 2'
     'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar'
     'files':'metrics.properties,log4j.properties'
     'rsync_sleep_minutes': 3
     'application_params':''
     print(spark_options)
configs("application_jar")
185/33:
def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
     spark_classname_and_base_options = '--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster'
     print(spark_options)
configs("application_jar")
185/34:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'spark_options':'--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
     print(spark_options)
configs("application_jar")
185/35:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'spark_options':'--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, env = 'prod', retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
     print(default_params['spark_options'])
configs("application_jar")
185/36:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
     spark_options=test
     print(spark_options)

configs()
185/37:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
     spark_options=test
     print(spark_options)

configs('dev')
185/38:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
     spark_options='test'
     print(spark_options)

configs('dev')
185/39:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 2'

configs('dev')
185/40:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory 4g --driver-memory 4g --executor-cores 3 --num-executors 2'
    print(spark_options)

configs('dev')
185/41:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory ' +executor_memory ' --driver-memory ' + driver_memory ' --executor-cores ' +executor_cores ' --num-executors ' +num_executors
    print(spark_options)

configs('dev')
185/42:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory ' +executor_memory ' --driver-memory ' + driver_memory +' --executor-cores ' +executor_cores ' --num-executors ' +num_executors
    print(spark_options)

configs('dev')
185/43:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory ' +executor_memory +' --driver-memory ' + driver_memory +' --executor-cores ' +executor_cores ' --num-executors ' +num_executors
    print(spark_options)

configs('dev')
185/44:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory ' +executor_memory+' --driver-memory ' +driver_memory+' --executor-cores ' +executor_cores+ ' --num-executors ' +num_executors
    print(spark_options)

configs('dev')
185/45:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options="--executor-memory " +executor_memory
    print(spark_options)

configs('dev')
185/46:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options="--executor-memory " +executor_memory+ " --driver-memory " +driver_memory+ " --executor-cores " +executor_cores+ " --num-executors " +num_executors
    print(spark_options)

configs('dev')
185/47:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options="--executor-memory " +executor_memory
    print(spark_options)

configs('dev')
185/48:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory ' +executor_memory
    print(spark_options)

configs('dev')
185/49:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory ' +str(executor_memory)
    print(spark_options)

configs('dev')
185/50:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory ' +executor_memory +' --driver-memory ' +driver_memory+' --executor-cores ' +executor_cores+ ' --num-executors ' +num_executors
    print(spark_options)

configs('dev')
185/51:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory ' +executor_memory +' --driver-memory ' +driver_memory+' --executor-cores ' +str(executor_cores)+ ' --num-executors ' +num_executors
    print(spark_options)

configs('dev')
185/52:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options='--executor-memory ' +str(executor_memory) +' --driver-memory ' +str(driver_memory)
    print(spark_options)

configs('dev')
185/53:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory:str='4g', driver_memory:str='4g', executor_cores:str=3, num_executors:str=3):
    spark_options='--executor-memory ' +executor_memory +' --driver-memory ' +driver_memory+' --executor-cores ' +executor_cores+ ' --num-executors ' +num_executors
    print(spark_options)

configs('dev')
185/54:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {}',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options = default_params['spark_options_format']
    print(spark_options)

configs('dev')
185/55:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v=1.0.3.1&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {}',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name, retry_delay=15, email_address='Team_LITTLE@choicehotels.com', executor_memory='4g', driver_memory='4g', executor_cores=3, num_executors=3):
    spark_options = default_params['spark_options_format'].format(executor_memory, driver_memory,executor_cores,  num_executors)
    print(spark_options)

configs('dev')
185/56:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {}',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
}

def configs(pipeline_name,
            application_jar_version,
            retry_delay=15, 
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3):
    spark_options = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors)
    application_jar_version = default_params['application_jar_version_format'].format(application_jar_version)
    print(application_jar_version)
configs('dev', 2)
185/57:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {}',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}'
}

def configs(pipeline_name,
            application_jar_version,
            retry_delay=15, 
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3):
    spark_options = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors)
    application_jar_version = default_params['application_jar_version_format'].format(application_jar_version)
    application_name = default_params['application_name_format'].format(application_name)
    print(application_name)

configs("application_name", 2)
185/58:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'yarn_queue_name':'DERIVED2',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {}',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}'
}

def configs(pipeline_name,
            application_jar_version,
            retry_delay=15, 
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3):
    spark_options = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors)
    application_jar_version = default_params['application_jar_version_format'].format(application_jar_version)
    application_name = default_params['application_name_format'].format(pipeline_name)
    print(application_name)

configs("application_name", 2)
185/59:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def configs(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    print(derived2_configs['spark_options'])
    
configs("dev", 2)
185/60:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def configs(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    print(derived2_configs['spark_options'])
    
configs("dev", 2, other_spark_options="test")
185/61:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def configs(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    print(derived2_configs['spark_options'])
    
configs("dev", 2, other_spark_options="test=value")
185/62:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def configs(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    
    
# Submit DAG
def submit(configs):
    print(derived_configs['spark_options'])

submit()
185/63:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def configs(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    
    
# Submit DAG
def submit(configs):
    print(derived_configs['spark_options'])

submit(configs("pipelineName", 2))
185/64:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def configs(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
   # derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    
    
# Submit DAG
def submit(configs):
    print(derived_configs['spark_options'])

submit(configs("pipelineName", 2))
185/65:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def configs(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    
    
# Submit DAG
def submit():
    configs("pipelineName", 2)
    print(derived_configs['spark_options'])

submit()
185/66:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    print(derived_configs['spark_options'])
185/67:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    print(derived_configs['spark_options'])


submit("pipeline", 2)
185/68:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    print(derived_configs['spark_options'])


submit("pipeline", 2)
185/69:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    print('spark_options')


submit("pipeline", 2)
185/70:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    print(derived_configs['spark_options'])


submit("pipeline", 2)
185/71:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
   # derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    print(derived_configs['spark_options'])


submit("pipeline", 2)
185/72:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    #derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
   # derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    #derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
   # derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    print(derived_configs['spark_options'])


submit("pipeline", 2)
185/73:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['email_address'] = email_address
    print(derived_configs['spark_options'])


submit("pipeline", 2)
185/74:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['email_address'] = email_address
    print(derived2_configs['spark_options'])


submit("pipeline", 2)
185/75:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit(pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)
    print(derived2_configs['spark_options'])


submit("pipeline", 2)
185/76:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Initialize or format configuration parameters
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
    print({**default_params, **derived2_configs})
185/77:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Initialize or format configuration parameters
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
    print({**default_params, **derived2_configs})
    
submit("pipeline", 2)
185/78:
# Default params
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':'',
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Initialize or format configuration parameters
    derived2_configs['spark_options'] = default_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = default_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = default_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = default_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
    print({**default_params, **derived2_configs})
    
submit("pipeline", 2)
185/79:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to initialize or format default params
def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Initialize or format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
    print({**default_params, **derived2_configs})
    
submit("pipeline", 2)
185/80:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
    # Submit the DAG using DAPSpark2SubmitOperator
     print(derived2_configs)

submit(pipeline_name="pipeline_name", application_jar_version=2)
185/81:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
     print(derived2_configs)

submit(pipeline_name="pipeline_name", application_jar_version=2)
185/82:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
     print(**derived2_configs)

submit(pipeline_name="pipeline_name", application_jar_version=2)
185/83:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
     print({derived2_configs})

submit(pipeline_name="pipeline_name", application_jar_version=2)
185/84:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
    print({derived2_configs})

submit(pipeline_name="pipeline_name", application_jar_version=2)
185/85:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
    print({derived2_configs})

submit(pipeline_name="pipeline_name", application_jar_version=2)
185/86:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submit( pipeline_name,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

    # Submit the DAG using DAPSpark2SubmitOperator
    print({**derived2_configs})

submit(pipeline_name="pipeline_name", application_jar_version=2)
185/87:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(pipeline_names,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    for pipeline_name in pipeline_names:
        #Format configuration parameters
        derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
        derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
        derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
        derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
        #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
        derived2_configs['email_address'] = email_address
        #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

        # Submit the DAG using DAPSpark2SubmitOperator
        print({**derived2_configs})

submit(pipeline_name="pipeline_name", application_jar_version=2)
185/88:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(pipeline_names,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    for pipeline_name in pipeline_names:
        #Format configuration parameters
        derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
        derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
        derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
        derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
        #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
        derived2_configs['email_address'] = email_address
        #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

        # Submit the DAG using DAPSpark2SubmitOperator
        print({**derived2_configs})

submit(pipeline_names="pipeline_name", application_jar_version=2)
185/89:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(pipeline_names,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    for pipeline_name in pipeline_names:
        #Format configuration parameters
        derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
        derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
        derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
        derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
        #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
        derived2_configs['email_address'] = email_address
        #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

        # Submit the DAG using DAPSpark2SubmitOperator
        print({**derived2_configs})

submit(pipeline_names="[pipeline_name]", application_jar_version=2)
185/90:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(pipeline_names,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    for pipeline_name in pipeline_names:
        #Format configuration parameters
        derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
        derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
        derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
        derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
        #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
        derived2_configs['email_address'] = email_address
        #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

        # Submit the DAG using DAPSpark2SubmitOperator
        print({**derived2_configs})

submit(pipeline_names=["pipeline_name"], application_jar_version=2)
185/91:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(pipeline_names,
            application_jar_version,
            retry_delay_minutes=15, 
            schedule_interval_minutes=15,
            email_address='Team_LITTLE@choicehotels.com', 
            executor_memory='4g', 
            driver_memory='4g', 
            executor_cores=3, 
            num_executors=3,
            other_spark_options=''):
    for pipeline_name in pipeline_names:
        #Format configuration parameters
        derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
        derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
        derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
        derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
        #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
        derived2_configs['email_address'] = email_address
        #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

        # Submit the DAG using DAPSpark2SubmitOperator
        print({**derived2_configs})

submitTasks(pipeline_names=["pipeline_name"], application_jar_version=2)
185/92:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(pipeline_names,
                application_jar_version,
                retry_delay_minutes=15, 
                schedule_interval_minutes=15,
                email_address='Team_LITTLE@choicehotels.com', 
                executor_memory='4g', 
                driver_memory='4g', 
                executor_cores=3, 
                num_executors=3,
                other_spark_options=''):
    for pipeline_name in pipeline_names:
        #Format configuration parameters
        derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
        derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
        derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
        derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
        derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
        derived2_configs['email_address'] = email_address
        derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

        # Submit the DAG using DAPSpark2SubmitOperator
        print({**derived2_configs})

submitTasks(pipeline_names=["pipeline_name"], application_jar_version=2)
185/93:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(pipeline_names,
                application_jar_version,
                retry_delay_minutes=15, 
                schedule_interval_minutes=15,
                email_address='Team_LITTLE@choicehotels.com', 
                executor_memory='4g', 
                driver_memory='4g', 
                executor_cores=3, 
                num_executors=3,
                other_spark_options=''):
    for pipeline_name in pipeline_names:
        #Format configuration parameters
        derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
        derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
        derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
        derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
        #derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
        derived2_configs['email_address'] = email_address
        #derived2_configs['schedule_interval'] = timedelta(minutes=schedule_interval_minutes)

        # Submit the DAG using DAPSpark2SubmitOperator
        print({**derived2_configs})

submitTasks(pipeline_names=["pipeline_name"], application_jar_version=2)
185/94:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(dag_id,
                pipeline_names,
                application_jar_version,
                retry_delay_minutes=15, 
                schedule_interval_minutes=15,
                email_address='Team_LITTLE@choicehotels.com', 
                executor_memory='4g', 
                driver_memory='4g', 
                executor_cores=3, 
                num_executors=3,
                other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['email_address'] = email_address
    #dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=derived2_configs['schedule_interval'],, concurrency=1, max_active_runs=1, catchup=False)
    for pipeline_name in pipeline_names:
        print(pipeline_name)


submitTasks(pipeline_names=["pipeline_name"], application_jar_version=2)
185/95:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(dag_id,
                pipeline_names,
                application_jar_version,
                retry_delay_minutes=15, 
                schedule_interval_minutes=15,
                email_address='Team_LITTLE@choicehotels.com', 
                executor_memory='4g', 
                driver_memory='4g', 
                executor_cores=3, 
                num_executors=3,
                other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
    derived2_configs['email_address'] = email_address
    #dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=derived2_configs['schedule_interval'],, concurrency=1, max_active_runs=1, catchup=False)
    for pipeline_name in pipeline_names:
        print(pipeline_name)


submitTasks(dag_id="dagId",pipeline_names=["pipeline_name"], application_jar_version=2)
185/96:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

format_params = {
 'application_name_format':'Derived2Job-{}',
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {} --driver-memory {} --executor-cores {} --num-executors {} {}',
 'extra_java_options_format':'-Dpipeline.name={}'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

def submitTasks(dag_id,
                pipeline_names,
                application_jar_version,
                retry_delay_minutes=15, 
                schedule_interval_minutes=15,
                email_address='Team_LITTLE@choicehotels.com', 
                executor_memory='4g', 
                driver_memory='4g', 
                executor_cores=3, 
                num_executors=3,
                other_spark_options=''):
    #Format configuration parameters
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory, driver_memory, executor_cores, num_executors, other_spark_options)
    derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
    derived2_configs['email_address'] = email_address
    #dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=derived2_configs['schedule_interval'],, concurrency=1, max_active_runs=1, catchup=False)
    for pipeline_name in pipeline_names:
        derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
        print(pipeline_name)
        print(derived2_configs['extra_java_options'])


submitTasks(dag_id="dagId",pipeline_names=["pipeline_name"], application_jar_version=2)
186/1:
format_params = {
 'application_jar_version_format':'r={releases}&v={version}&c={classifier}',
}

application_jar_version_format.format(releases='releases1',version='2', classifier='shaded' )
186/2:
format_params = {
 'application_jar_version_format':'r={releases}&v={version}&c={classifier}',
}

format_params[application_jar_version_format].format(releases='releases1',version='2', classifier='shaded' )
186/3:
format_params = {
 'application_jar_version_format':'r={releases}&v={version}&c={classifier}',
}

format_params['application_jar_version_format'].format(releases='releases1',version='2', classifier='shaded' )
186/4:
format_params = {
 'application_jar_version_format':'r={releases}&v={version}&c={classifier}',
 'spark_options_format':'--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}'
    
}

format_params['spark_options_format'].format(executor_memory='4g',driver_memory='2G', executor_cores='2',  num_executors=3)
186/5:
format_params = {
 'application_jar_version_format':'r={releases}&v={version}&c={classifier}',
 'spark_options_format':'--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}'
    
}

format_params['spark_options_format'].format(executor_memory='4g',driver_memory='2G', executor_cores='2',  num_executors=3, other_spark_options='')
186/6:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}
 
format_params = {
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={version}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}',
}
 
# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {
 
}
 
def create_dag(dag_id, schedule_interval_minutes):
    dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=timedelta(minutes=schedule_interval_minutes), concurrency=1, max_active_runs=1, catchup=False)
    return dag;
 
def create_derive_task(dag,
           pipeline_name,
           application_jar_version,
           retry_delay_minutes=15,
           schedule_interval_minutes=15,
           email_address='Team_LITTLE@choicehotels.com',
           executor_memory='4g',
           driver_memory='4g',
           executor_cores=3,
           num_executors=3,
           other_spark_options=''):
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory=executor_memory, driver_memory=driver_memory, executor_cores=executor_cores, num_executors=num_executors, other_spark_options=other_spark_options)
    print(derived2_configs['spark_options'])
    #     derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
#     derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
#     derived2_configs['email_address'] = email_address
#     derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
#     derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
#     task = DAPSpark2SubmitOperator(task_id=pipeline_name + '_derived2_spark_job', params={**default_params, **derived2_configs},dag=dag)
#     return task

create_derive_task('dag', 'pipeline', '2')
186/7:
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}
 
format_params = {
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={version}&c=shaded&p=jar',
 'spark_options_format':'--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}',
}
 
# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {
 
}
 
def create_dag(dag_id, schedule_interval_minutes):
    dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=timedelta(minutes=schedule_interval_minutes), concurrency=1, max_active_runs=1, catchup=False)
    return dag;
 
def create_derive_task(dag,
           pipeline_name,
           application_jar_version,
           retry_delay_minutes=15,
           schedule_interval_minutes=15,
           email_address='Team_LITTLE@choicehotels.com',
           executor_memory='4g',
           driver_memory='4g',
           executor_cores=3,
           num_executors=3,
           other_spark_options=''):
    derived2_configs['spark_options'] = format_params['spark_options_format'].format(executor_memory=executor_memory, driver_memory=driver_memory, executor_cores=executor_cores, num_executors=num_executors, other_spark_options=other_spark_options)
    print(derived2_configs['spark_options'])
    #     derived2_configs['application_jar_version'] = format_params['application_jar_version_format'].format(application_jar_version)
#     derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
#     derived2_configs['email_address'] = email_address
#     derived2_configs['extra_java_options'] = format_params['extra_java_options_format'].format(pipeline_name)
#     derived2_configs['application_name'] = format_params['application_name_format'].format(pipeline_name)
#     task = DAPSpark2SubmitOperator(task_id=pipeline_name + '_derived2_spark_job', params={**default_params, **derived2_configs},dag=dag)
#     return task

create_derive_task('dag', 'pipeline', '2',other_spark_options='key=value' )
186/8:
env = 'dev'
extra_java_options = '-Dpipeline.name=pipeline_name'
spark_options_format = '--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}',
retry_delay_minutes = 15
schedule_interval_minutes = 15
email_address='Team_LITTLE@choicehotels.com'

if env == "dev" or env == "qa" :
    spark_options = spark_options_format.format(executor_memory = 2G, driver_memory = 2G, executor_cores = 2, num_executors = 3)
elif env == "prod":
    spark_options = spark_options_format.format(executor_memory = 4G, driver_memory = 4G, executor_cores = 3, num_executors = 3)
else:
  raise ValueError('Invalid environment {}'.format(env))

print(spark_options)
186/9:
env = 'dev'
extra_java_options = '-Dpipeline.name=pipeline_name'
spark_options_format = '--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}',
retry_delay_minutes = 15
schedule_interval_minutes = 15
email_address='Team_LITTLE@choicehotels.com'

if env == "dev" or env == "qa" :
    spark_options = spark_options_format.format(executor_memory = '2G', driver_memory = '2G', executor_cores = '2', num_executors = '3')
elif env == "prod":
    spark_options = spark_options_format.format(executor_memory = 4G, driver_memory = 4G, executor_cores = 3, num_executors = 3)
else:
  raise ValueError('Invalid environment {}'.format(env))

print(spark_options)
186/10:
env = 'dev'
extra_java_options = '-Dpipeline.name=pipeline_name'
spark_options_format = '--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}',
retry_delay_minutes = 15
schedule_interval_minutes = 15
email_address='Team_LITTLE@choicehotels.com'

if env == "dev" or env == "qa" :
    spark_options = spark_options_format.format(executor_memory = '2G', driver_memory = '2G', executor_cores = '2', num_executors = '3')
elif env == "prod":
    spark_options = spark_options_format.format(executor_memory = '4G', driver_memory = '4G', executor_cores = '3', num_executors = '3')
else:
  raise ValueError('Invalid environment {}'.format(env))

print(spark_options)
186/11:
env = 'dev'
extra_java_options = '-Dpipeline.name=pipeline_name'
'spark_options_format' : '--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}',
retry_delay_minutes = 15
schedule_interval_minutes = 15
email_address='Team_LITTLE@choicehotels.com'

if env == "dev" or env == "qa" :
    spark_options = spark_options_format.format(executor_memory = '2G', driver_memory = '2G', executor_cores = '2', num_executors = '3')
elif env == "prod":
    spark_options = spark_options_format.format(executor_memory = '4G', driver_memory = '4G', executor_cores = '3', num_executors = '3')
else:
  raise ValueError('Invalid environment {}'.format(env))

print(spark_options)
186/12:
env = 'dev'
extra_java_options = '-Dpipeline.name=pipeline_name'
spark_options_format = '--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}',
retry_delay_minutes = 15
schedule_interval_minutes = 15
email_address='Team_LITTLE@choicehotels.com'

if env == "dev" or env == "qa" :
   # spark_options = spark_options_format.format(executor_memory = '2G', driver_memory = '2G', executor_cores = '2', num_executors = '3')
elif env == "prod":
    spark_options = spark_options_format.format(executor_memory = '4G', driver_memory = '4G', executor_cores = '3', num_executors = '3')
else:
  raise ValueError('Invalid environment {}'.format(env))

print(spark_options_format)
186/13:
env = 'dev'
extra_java_options = '-Dpipeline.name=pipeline_name'
spark_options_format = '--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}',
retry_delay_minutes = 15
schedule_interval_minutes = 15
email_address='Team_LITTLE@choicehotels.com'

# if env == "dev" or env == "qa" :
#    # spark_options = spark_options_format.format(executor_memory = '2G', driver_memory = '2G', executor_cores = '2', num_executors = '3')
# elif env == "prod":
#     spark_options = spark_options_format.format(executor_memory = '4G', driver_memory = '4G', executor_cores = '3', num_executors = '3')
# else:
#   raise ValueError('Invalid environment {}'.format(env))

print(spark_options_format)
186/14:
env = 'dev'
extra_java_options = '-Dpipeline.name=pipeline_name'
spark_options_format = "--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors} {other_spark_options}"
retry_delay_minutes = 15
schedule_interval_minutes = 15
email_address='Team_LITTLE@choicehotels.com'

if env == "dev" or env == "qa" :
    spark_options = spark_options_format.format(executor_memory = '2G', driver_memory = '2G', executor_cores = '2', num_executors = '3')
elif env == "prod":
    spark_options = spark_options_format.format(executor_memory = '4G', driver_memory = '4G', executor_cores = '3', num_executors = '3')
else:
  raise ValueError('Invalid environment {}'.format(env))

print(spark_options)
186/15:
env = 'dev'
extra_java_options = '-Dpipeline.name=pipeline_name'
spark_options_format = "--executor-memory {executor_memory} --driver-memory {driver_memory} --executor-cores {executor_cores} --num-executors {num_executors}"
retry_delay_minutes = 15
schedule_interval_minutes = 15
email_address='Team_LITTLE@choicehotels.com'

if env == "dev" or env == "qa" :
    spark_options = spark_options_format.format(executor_memory = '2G', driver_memory = '2G', executor_cores = '2', num_executors = '3')
elif env == "prod":
    spark_options = spark_options_format.format(executor_memory = '4G', driver_memory = '4G', executor_cores = '3', num_executors = '3')
else:
  raise ValueError('Invalid environment {}'.format(env))

print(spark_options)
187/1:
#Copyright 2018 Choice Hotels International
import airflow
import os
import sys
from datetime import timedelta, datetime
# from airflow.models import DAG
# from DAPSpark2SubmitOperator import DAPSpark2SubmitOperator, airflow_args


# Base template for derived2 jobs.
# The default parameters to be used for the Derived2 Jobs.
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

# Format Specifiers.
format_specifiers = {
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={version}&c=shaded&p=jar'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to instantiate a DAG.
def create_dag(dag_id, schedule_interval_minutes):
    dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=timedelta(minutes=schedule_interval_minutes), concurrency=1, max_active_runs=1, catchup=False)
    return dag;

# A function to create derived task for given DAG.
def create_derived_task(dag,
           application_name,
           application_jar_version,
           spark_options,
           extra_java_options = '',
           retry_delay_minutes = 15,
           schedule_interval_minutes = 15,
           email_address = 'Team_LITTLE@choicehotels.com'):
    derived2_configs['spark_options'] = spark_options
    derived2_configs['application_jar_version'] = format_specifiers['application_jar_version_format'].format(version = application_jar_version)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['extra_java_options'] = extra_java_options
    derived2_configs['application_name'] = application_name
    #task = DAPSpark2SubmitOperator(task_id = application_name + '_derived2_spark_job', params={**default_params, **derived2_configs},dag=dag)
    return task


create_derived_task("dag", "name", 2)
187/2:
#Copyright 2018 Choice Hotels International
import airflow
import os
import sys
from datetime import timedelta, datetime
# from airflow.models import DAG
# from DAPSpark2SubmitOperator import DAPSpark2SubmitOperator, airflow_args


# Base template for derived2 jobs.
# The default parameters to be used for the Derived2 Jobs.
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

# Format Specifiers.
format_specifiers = {
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={version}&c=shaded&p=jar'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to instantiate a DAG.
def create_dag(dag_id, schedule_interval_minutes):
    dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=timedelta(minutes=schedule_interval_minutes), concurrency=1, max_active_runs=1, catchup=False)
    return dag;

# A function to create derived task for given DAG.
def create_derived_task(dag,
           application_name,
           application_jar_version,
           spark_options,
           extra_java_options = '',
           retry_delay_minutes = 15,
           schedule_interval_minutes = 15,
           email_address = 'Team_LITTLE@choicehotels.com'):
    derived2_configs['spark_options'] = spark_options
    derived2_configs['application_jar_version'] = format_specifiers['application_jar_version_format'].format(version = application_jar_version)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['extra_java_options'] = extra_java_options
    derived2_configs['application_name'] = application_name
    #task = DAPSpark2SubmitOperator(task_id = application_name + '_derived2_spark_job', params={**default_params, **derived2_configs},dag=dag)
    print(retry_delay_minutes)
    return task


create_derived_task("dag", "name", 2)
187/3:
#Copyright 2018 Choice Hotels International
import airflow
import os
import sys
from datetime import timedelta, datetime
# from airflow.models import DAG
# from DAPSpark2SubmitOperator import DAPSpark2SubmitOperator, airflow_args


# Base template for derived2 jobs.
# The default parameters to be used for the Derived2 Jobs.
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

# Format Specifiers.
format_specifiers = {
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={version}&c=shaded&p=jar'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to instantiate a DAG.
def create_dag(dag_id, schedule_interval_minutes):
    dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=timedelta(minutes=schedule_interval_minutes), concurrency=1, max_active_runs=1, catchup=False)
    return dag;

# A function to create derived task for given DAG.
def create_derived_task(dag,
           application_name,
           application_jar_version,
           spark_options,
           extra_java_options = '',
           retry_delay_minutes = 15,
           schedule_interval_minutes = 15,
           email_address = 'Team_LITTLE@choicehotels.com'):
    derived2_configs['spark_options'] = spark_options
    derived2_configs['application_jar_version'] = format_specifiers['application_jar_version_format'].format(version = application_jar_version)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['extra_java_options'] = extra_java_options
    derived2_configs['application_name'] = application_name
    #task = DAPSpark2SubmitOperator(task_id = application_name + '_derived2_spark_job', params={**default_params, **derived2_configs},dag=dag)
    print(retry_delay_minutes)
    return task


create_derived_task("dag", "name", 2, "")
187/4:
#Copyright 2018 Choice Hotels International
import airflow
import os
import sys
from datetime import timedelta, datetime
# from airflow.models import DAG
# from DAPSpark2SubmitOperator import DAPSpark2SubmitOperator, airflow_args


# Base template for derived2 jobs.
# The default parameters to be used for the Derived2 Jobs.
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

# Format Specifiers.
format_specifiers = {
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={version}&c=shaded&p=jar'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to instantiate a DAG.
def create_dag(dag_id, schedule_interval_minutes):
    dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=timedelta(minutes=schedule_interval_minutes), concurrency=1, max_active_runs=1, catchup=False)
    return dag;

# A function to create derived task for given DAG.
def create_derived_task(dag,
           application_name,
           application_jar_version,
           spark_options,
           extra_java_options = '',
           retry_delay_minutes = 15,
           schedule_interval_minutes = 15,
           email_address = 'Team_LITTLE@choicehotels.com'):
    derived2_configs['spark_options'] = spark_options
    derived2_configs['application_jar_version'] = format_specifiers['application_jar_version_format'].format(version = application_jar_version)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['extra_java_options'] = extra_java_options
    derived2_configs['application_name'] = application_name
    #task = DAPSpark2SubmitOperator(task_id = application_name + '_derived2_spark_job', params={**default_params, **derived2_configs},dag=dag)
    print(retry_delay_minutes)
    task = "helo"
    return task


task = create_derived_task("dag", "name", 2, "")
187/5:
#Copyright 2018 Choice Hotels International
import airflow
import os
import sys
from datetime import timedelta, datetime
# from airflow.models import DAG
# from DAPSpark2SubmitOperator import DAPSpark2SubmitOperator, airflow_args


# Base template for derived2 jobs.
# The default parameters to be used for the Derived2 Jobs.
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

# Format Specifiers.
format_specifiers = {
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={version}&c=shaded&p=jar'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to instantiate a DAG.
def create_dag(dag_id, schedule_interval_minutes):
    dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=timedelta(minutes=schedule_interval_minutes), concurrency=1, max_active_runs=1, catchup=False)
    return dag;

# A function to create derived task for given DAG.
def create_derived_task(dag,
           application_name,
           application_jar_version,
           spark_options,
           extra_java_options = '',
           retry_delay_minutes = 15,
           schedule_interval_minutes = 15,
           email_address = 'Team_LITTLE@choicehotels.com'):
    derived2_configs['spark_options'] = spark_options
    derived2_configs['application_jar_version'] = format_specifiers['application_jar_version_format'].format(version = application_jar_version)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['extra_java_options'] = extra_java_options
    derived2_configs['application_name'] = application_name
    #task = DAPSpark2SubmitOperator(task_id = application_name + '_derived2_spark_job', params={**default_params, **derived2_configs},dag=dag)
    print(derived2_configs['retry_delay'])
    return task


task = create_derived_task("dag", "name", 2, "")
187/6:
#Copyright 2018 Choice Hotels International
import airflow
import os
import sys
from datetime import timedelta, datetime
from airflow.models import DAG
#from DAPSpark2SubmitOperator import DAPSpark2SubmitOperator, airflow_args

# Base template for derived2 jobs.

# The default parameters to be used for the Derived2 Jobs.
default_params = {
 'spark_classname_and_base_options':'--class com.choicehotels.dap2.derived.job.DerivedJob --master yarn --deploy-mode cluster',
 'yarn_queue_name':'DERIVED2',
 'external_jars_query_string': 'r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions&v=1.0.0.8&c=&p=jar,r=releases&g=choicehotels.common&a=dropwizard-metrics-extensions-spark&v=1.0.0.8&c=&p=jar',
 'files':'metrics.properties,log4j.properties',
 'rsync_sleep_minutes': 3,
 'application_params':''
}

# Format Specifiers.
format_specifiers = {
 'application_jar_version_format':'r=releases&g=com.choicehotels.dap&a=dap2-spark2-derived&v={version}&c=shaded&p=jar'
}

# the configuration dict that contains all the params required for the SparkSubmit executed by DAPSpark2SubmitOperator and all the params
# required for scheduling the DAG and the Task
derived2_configs = {

}

# A function to instantiate a DAG.
def create_dag(dag_id, schedule_interval_minutes):
    dag = DAG(dag_id=dag_id, default_args=airflow_args, schedule_interval=timedelta(minutes=schedule_interval_minutes), concurrency=1, max_active_runs=1, catchup=False)
    return dag;

# A function to create derived task for given DAG.
def create_derived_task(dag,
           application_name,
           application_jar_version,
           spark_options,
           extra_java_options = '',
           retry_delay_minutes = 15,
           schedule_interval_minutes = 15,
           email_address = 'Team_LITTLE@choicehotels.com'):
    derived2_configs['spark_options'] = spark_options
    derived2_configs['application_jar_version'] = format_specifiers['application_jar_version_format'].format(version = application_jar_version)
    derived2_configs['retry_delay'] = timedelta(minutes=retry_delay_minutes)
    derived2_configs['email_address'] = email_address
    derived2_configs['extra_java_options'] = extra_java_options
    derived2_configs['application_name'] = application_name
   # task = DAPSpark2SubmitOperator(task_id = application_name + '_derived2_spark_job', params={**default_params, **derived2_configs},dag=dag)
    return task
192/1: %ipython
192/2: sc.parallelize(1 to 100)
193/1: sc.parallelize(1 to 50)
193/2: sc.parallelize(1 to 50)
193/3: val name = "helloworld"
197/1: import botot
197/2: import boto
197/3: import boto
197/4: import boto3
197/5:
import boto3
s3 = boto3.resource('s3')
197/6:
import boto3
s3 = boto3.resource('s3')
for bucket in s3.buckets.all():
    print(bucket.name)
200/1:
import os
import sys
import pandas as pd

def start():
    data = pd.read_csv("*.txt", header=None)
    data.head()
200/2:
import os
import sys
import pandas as pd

def start():
    data = pd.read_csv("*.txt", header=None)
    data.head()
    
start()
200/3:
import os
import sys
import pandas as pd
import glob as glob

def start():
    files = glob.glob('*.txt')
    prin(files)
    
start()
200/4:
import os
import sys
import pandas as pd
import glob as glob

def start():
    files = glob.glob('*.txt')
    print(files)
    
start()
200/5:
import os
import sys
import pandas as pd
import glob as glob

def start():
    files = glob.glob('*.txt')
    data = pd.read_csv(files, header=None)
    data.head()
    print(files)
    
start()
200/6:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in csv_files:
        data = pd.read_csv(filename)
        list_data.append(data)
        
    final_data = pd.concat(list_data,ignore_index=True)
    final_data.head()
start()
200/7:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        data = pd.read_csv(filename)
        list_data.append(data)
        
    final_data = pd.concat(list_data,ignore_index=True)
    final_data.head()
start()
200/8:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        data = pd.read_csv(filename)
        list_data.append(data)
        
    final_data = pd.concat(list_data,ignore_index=True)
    final_data.head()
start()
200/9:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        data = pd.read_csv(filename)
        list_data.append(data)
        
    final_data = pd.concat(list_data,ignore_index=True)
    print(final_head)
    
start()
200/10:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        data = pd.read_csv(filename)
        list_data.append(data)
        
    pd.concat(list_data,ignore_index=True)
    print(list_data)
    
start()
200/11:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    pd.concat(list_data,ignore_index=True)
    print(list_data)
    
start()
200/12:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/13:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/14:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename)
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/15:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename[:0])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/16:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename[:1])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/17:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename[:, :1])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/18:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.iloc[:, :1])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/19:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.substring(1))
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/20:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.split(_))
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/21:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename)
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/22:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.split('_'))
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/23:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.rpartition('_')[2])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/24:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.rpartition('_')[2].rpartition('.')[1])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/25:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.rpartition('_')[2].rpartition('.')[2])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/26:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.rpartition('_')[2].rpartition('.')[3])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/27:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.rpartition('_')[2].rpartition('.')[0])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/28:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.rpartition('_, .')[2].rpartition('.')[0])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/29:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        print(filename.rpartition('_')[2].rpartition('.')[0])
        data = pd.read_csv(filename, header=None)
        list_data.append(data)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/30:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe = dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/31:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        print(index)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/32:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe1 = dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe1)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    
start()
200/33:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe = dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    
start()
200/34:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        final_dataframe = dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(final_dataframe)
        list_data.append(final_dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    
start()
200/35:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        final_dataframe = dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(final_dataframe)
        list_data.append(final_dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    
start()
200/36:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        final_dataframe = dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        list_data.append(final_dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    
start()
200/37:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        final_dataframe = dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(final_dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    
start()
200/38:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    
start()
200/39:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    
start()
200/40:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    pd.to_csv('testing.csv', data)
    
start()
200/41:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('testing.csv')
    
start()
200/42:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=false)
    data.to_csv('testing.csv')
    
start()
200/43:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=None)
    data.to_csv('testing.csv')
    
start()
200/44:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('testing.csv', index=None)
    
start()
200/45:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('testing.csv', index=None)
    
start()
200/46:
import os
import sys
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
    
start()
200/47:
import pandas as pd
import glob as glob

def start():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        index = len(dataframe.columns)
        dataframe.insert(index, 'dap_ingestion_timestamp', timestamp)
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
    
start()
200/48:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        dataframe['dap_ingestion_timestamp']= timestamp
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
200/49:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('_')[2].rpartition('.')[0]
        dataframe = pd.read_csv(filename, header=None)
        dataframe['dap_ingestion_timestamp']= timestamp
        print(dataframe)
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/50:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')
        dataframe = pd.read_csv(filename, header=None)
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/51:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')
        print(timestamp)
#         dataframe = pd.read_csv(filename, header=None)
#         dataframe['dap_ingestion_timestamp'] = timestamp
#         list_data.append(dataframe)
        
#     data = pd.concat(list_data, ignore_index=True)
#     data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/52:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0]
        print(timestamp)
#         dataframe = pd.read_csv(filename, header=None)
#         dataframe['dap_ingestion_timestamp'] = timestamp
#         list_data.append(dataframe)
        
#     data = pd.concat(list_data, ignore_index=True)
#     data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/53:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')
        print(timestamp)
#         dataframe = pd.read_csv(filename, header=None)
#         dataframe['dap_ingestion_timestamp'] = timestamp
#         list_data.append(dataframe)
        
#     data = pd.concat(list_data, ignore_index=True)
#     data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/54:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')
        print(timestamp)
#         dataframe = pd.read_csv(filename, header=None)
#         dataframe['dap_ingestion_timestamp'] = timestamp
#         list_data.append(dataframe)
        
#     data = pd.concat(list_data, ignore_index=True)
#     data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/55:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        print(timestamp)
#         dataframe = pd.read_csv(filename, header=None)
#         dataframe['dap_ingestion_timestamp'] = timestamp
#         list_data.append(dataframe)
        
#     data = pd.concat(list_data, ignore_index=True)
#     data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/56:
import pandas as pd
import glob as glob
from datetime import striptime

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        datetime = 
        dataframe = pd.read_csv(filename, header=None)
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/57:
import pandas as pd
import glob as glob
from datetime import striptime

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        datetime = strptime(timestamp)
        print(datetime)
        dataframe = pd.read_csv(filename, header=None)
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/58:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None)
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/59:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None)
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/60:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None)
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/61:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep='\001')
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
200/62:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep='\001')
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
201/1:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep='\001' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        print(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
201/2:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        print(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None)
 
append_timestamp_and_concatenate_files()
201/3:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep='_')
 
append_timestamp_and_concatenate_files()
201/4:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep='_')
 
append_timestamp_and_concatenate_files()
201/5:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep='?')
 
append_timestamp_and_concatenate_files()
201/6:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion1.csv', index=None, sep='?')
 
append_timestamp_and_concatenate_files()
201/7:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
        print(list_data)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion1.csv', index=None, sep='?')
 
append_timestamp_and_concatenate_files()
201/8:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        print(dataframe)
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion1.csv', index=None, sep='?')
 
append_timestamp_and_concatenate_files()
201/9:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    print(data)
    data.to_csv('ingestion1.csv', index=None, sep='?')
 
append_timestamp_and_concatenate_files()
201/10:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion1.csv', index=None, sep='|')
 
append_timestamp_and_concatenate_files()
201/11:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion1.csv', index=None, sep='|')
 
append_timestamp_and_concatenate_files()
201/12:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep='\001')
 
append_timestamp_and_concatenate_files()
201/13:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep='')
 
append_timestamp_and_concatenate_files()
201/14:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=':')
 
append_timestamp_and_concatenate_files()
201/15:
import pandas as pd
import glob as glob

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.txt')
    #intialize empty list that we will append dataframes to
    list_data = []
    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dap_ingestion_timestamp'] = timestamp
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')
 
append_timestamp_and_concatenate_files()
202/1:
import sys
import mlflow
202/2:
import sys
import mlflow

mlflow.set_tracking_uri("http://localhost:5000/")
202/3:
import sys
import mlflow

mlflow.set_tracking_uri("http://localhost:5000/") 
mlflow.create_experiment('experiment_name')
202/4:
import sys
import mlflow

mlflow.set_tracking_uri("http://localhost:5000/") 
mlflow.create_experiment('experiment_name')
mlflow.log_param('key', 'value')
mlflow.log_metric('key_metrc', 'value')
202/5:
import sys
import mlflow

mlflow.set_tracking_uri("http://localhost:5000/") 
mlflow.create_experiment('experiment_name')
202/6:
import sys
import mlflow

mlflow.set_tracking_uri("http://localhost:5000/") 
mlflow.log_param('key', 'value')
mlflow.log_metric('key_metrc', 'value')
202/7:
import sys
import mlflow

mlflow.set_tracking_uri("http://localhost:5000/") 
mlflow.log_param('key', 'value')
mlflow.log_metric('key_metrc', 1)
202/8:
import sys
import mlflow

with mlflow.start_run():
    mlflow.set_tracking_uri("http://localhost:5000/") 
    mlflow.log_param("x", 1)
    mlflow.log_metric("y", 2)
202/9:
import sys
import mlflow

    mlflow.set_tracking_uri("http://localhost:5000/") 
    mlflow.log_param("x", 1)
    mlflow.log_metric("y", 2)
202/10:
import sys
import mlflow

mlflow.set_tracking_uri("http://localhost:5000/") 
mlflow.log_param("x", 1)
mlflow.log_metric("y", 2)
203/1:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
203/2:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import mlflow
203/3:
dataframe = pd.read_csv('/users/ukannika/Downloads/train.csv')
dataframe.head()
203/4:
dataframe = pd.read_csv('/users/ukannika/Downloads/train.csv')
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
regr = linear_model.LinearRegression()
regr.fit(features, labels)
203/5:
data = pd.read_csv('/users/ukannika/Downloads/train.csv')
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
regr = linear_model.LinearRegression()
regr.fit(features, labels)
203/6:
data = pd.read_csv('/users/ukannika/Downloads/train.csv').fillNa(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
regr = linear_model.LinearRegression()
regr.fit(features, labels)
203/7:
data = pd.read_csv('/users/ukannika/Downloads/train.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
regr = linear_model.LinearRegression()
regr.fit(features, labels)
203/8:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
regr = linear_model.LinearRegression()
regr.fit(features, labels)
203/9:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
regr = linear_model.LinearRegression()
regr.fit(features, labels)

y_pred = regr.predict(test_data)
203/10:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
regr = linear_model.LinearRegression()
regr.fit(features, labels)

y_pred = regr.predict(test_data)
print(y_pred)
203/11:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
regr.fit(features, labels)

y_pred = regr.predict(test_data)
print(y_pred)
203/12:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
regr.fit(features, labels)

y_pred = elastic_model.predict(test_data)
print(y_pred)
203/13:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
elastic_model.fit(features, labels)

y_pred = elastic_model.predict(test_data)
print(y_pred)
203/14:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
elastic_model.fit(features, labels)

y_pred = elastic_model.predict(test_data)
print(y_pred)
203/15:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import mlflow
203/16:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
print(features)
print(labels)
# elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
# elastic_model.fit(features, labels)

# y_pred = elastic_model.predict(test_data)
# print(y_pred)
203/17:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
print(labels)
# elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
# elastic_model.fit(features, labels)

# y_pred = elastic_model.predict(test_data)
# print(y_pred)
203/18:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = data.iloc[:, :-1]
labels  = data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
elastic_model.fit(features, labels)

y_pred = elastic_model.predict(test_data)
print(y_pred)
203/19:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
elastic_model.fit(features, labels)

y_pred = elastic_model.predict(test_data)
print(y_pred)
203/20:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
print(features)
# elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
# elastic_model.fit(features, labels)

# y_pred = elastic_model.predict(test_data)
# print(y_pred)
203/21:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
# elastic_model.fit(features, labels)

# y_pred = elastic_model.predict(test_data)
# print(y_pred)
203/22:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
elastic_model.fit(features, labels)

# y_pred = elastic_model.predict(test_data)
# print(y_pred)
203/23:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
elastic_model.fit(features, labels)

y_pred = elastic_model.predict(test_data)
# print(y_pred)
203/24:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv').fillna(0)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
elastic_model.fit(features, labels)

# y_pred = elastic_model.predict(test_data)
# print(y_pred)
203/25:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv')
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
elastic_model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
elastic_model.fit(features, labels)

# y_pred = elastic_model.predict(test_data)
# print(y_pred)
203/26:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv')
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
model.fit(features, labels)

y_pred = model.predict(test_data)
print(y_pred)
203/27:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv')
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
print(test_data)
# y_pred = model.predict(test_data)
# print(y_pred)
203/28:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv', header=None).fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
print(test_data)
# y_pred = model.predict(test_data)
# print(y_pred)
203/29:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
print(test_data)
# y_pred = model.predict(test_data)
# print(y_pred)
203/30:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/31:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/32:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.6, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/33:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.4, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/34:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.LinearRegression()
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/35:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.LinearRegression()
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/36:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.LinearRegression()
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/37:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.4, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/38:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.4, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/39:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.LinearRegression()
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/40:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.4, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/41:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.LinearRegression()
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/42:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=0.6, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/43:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/44:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param(alpha, 1.6)
mlflow.log_param(l1_ratio, 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/45:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param(alpha, 1.6)
mlflow.log_param(l1_ratio, 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/46:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/47:
client = MlflowClient()
experiments = service.list_experiments() # returns a list of mlflow.entities.Experiment
print(experiments)
203/48:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import mlflow
from  mlflow.tracking import MlflowClient
203/49:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import mlflow
from  mlflow.tracking
203/50:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import mlflow
from  mlflow import tracking
203/51:
client = tracking.MlflowClient()
experiments = service.list_experiments() # returns a list of mlflow.entities.Experiment
print(experiments)
203/52: mlflow.create_run(experiment_id, user_id=None, run_name=None, source_type=None, source_name=None, entry_point_name=None, start_time=None, source_version=None, tags=None)
203/53: mlflow.list_experiments()
203/54:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking.list_experiments
203/55:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
203/56: tracking.list_experiments()
203/57: tracking.list_experiments()[0].experiment_id
203/58: tracking.list_experiments()[1].experiment_id
203/59: tracking.list_experiments()[0].experiment_id
203/60:
tracking.create_experiment('PricePrediction')
tracking.list_experiments()[0].experiment_id
203/61: tracking.list_experiments()[1].experiment_id
203/62: tracking.list_experiments().get('PricePrediction')
203/63: tracking.create_run(1)
203/64: client = tracking.MlflowClient()
203/65: client = MlflowClient()
203/66:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/67:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from tracking import MlflowClient
203/68:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from mlflow import MlflowClient
203/69:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from tracking.MlflowClient
203/70:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from tracking.MlflowClient()
203/71:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from tracking.MlflowClient
203/72:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from  mlflow.tracking import MlflowClient
203/73:
mlflow.start_run()
mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/74:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/75:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/76:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from  mlflow.tracking import MlflowClient
203/77: client = tracking.MlflowClient()
203/78:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import *
203/79: mlflow.tracking.MlflowClient()
203/80:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
203/81: mlflow.tracking.MlflowClient()
203/82:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
203/83:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)


(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)

print("Elasticnet model (alpha=%f, l1_ratio=%f):" % (alpha, l1_ratio))
print("  RMSE: %s" % rmse)
print("  MAE: %s" % mae)
print("  R2: %s" % r2)
    
mlflow.end_run()
203/84:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet

import mlflow
import mlflow.sklearn
203/85:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)


(rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)

print("Elasticnet model (alpha=%f, l1_ratio=%f):" % (alpha, l1_ratio))
print("  RMSE: %s" % rmse)
print("  MAE: %s" % mae)
print("  R2: %s" % r2)
    
mlflow.end_run()
203/86:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score, eval_metrics
from mlflow import tracking
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet

import mlflow
import mlflow.sklearn
203/87:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking, eval_metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet

import mlflow
import mlflow.sklearn
203/88:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
    
mlflow.end_run()
203/89:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
    
mlflow.end_run()
203/90:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.LinearRegression()
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/91:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri()
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
    
mlflow.end_run()
203/92:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
    
mlflow.end_run()
203/93:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
    
mlflow.end_run()
203/94:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/95:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.LinearRegression()
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/96:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/97:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.LinearRegression()
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
203/98:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/99:

mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.tracking.create_run(1)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/100:
from mlflow import tracking
mlflow.log_param("my", "param")
mlflow.log_metric("score", 100)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
tracking.create_run(1)
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/101:
from mlflow import tracking
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.source('PricePrediction')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/102:
from mlflow import tracking
mlflow.start_run()
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.source('PricePrediction')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/103: mlflow.end_run()
203/104:
from mlflow import tracking
mlflow.start_run()
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.source('PricePrediction')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/105: mlflow.end_run()
203/106:
from mlflow import tracking
mlflow.start_run(1)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/107:
from mlflow import tracking
mlflow.start_run(experiment_id=1)
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/108:
from mlflow import tracking
mlflow.start_run(experiment_id=1, source_name='train.py')
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/109:
from mlflow import tracking
mlflow.start_run(experiment_id=1, source_name='train.py')
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/110:
from mlflow import tracking
mlflow.run(experiment_id=1, source_name='train.py')
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/111:
from mlflow import tracking
mlflow.start_run(experiment_id=1, source_name='train.py')
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
203/112:
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.LinearRegression()
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
205/1:
mlflow.start_run(experiment_id=1, source_name='train.py')
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
205/2:
import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from mlflow import tracking
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet

import mlflow
205/3:
mlflow.start_run(experiment_id=1, source_name='train.py')
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.6)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
205/4:
mlflow.start_run(experiment_id=1, source_name='train.py')
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.7)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
205/5:
mlflow.start_run(experiment_id=1, source_name='train.py')
train_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/work/choice/git/aws/amazon-sagemaker-examples/advanced_functionality/scikit_bring_your_own/pricePredictionContainer/data/test.csv', header=None)
features = train_data.iloc[:, :-1]
labels  = train_data.iloc[:, -1]
model = linear_model.ElasticNet(alpha=1.6, l1_ratio=0.5, random_state=42)
mlflow.set_tracking_uri('http://localhost:5000')
mlflow.log_param('alpha', 1.7)
mlflow.log_param('l1_ratio', 0.5)
model.fit(features, labels)
y_pred = model.predict(test_data)
print(y_pred)
mlflow.end_run()
205/6:
import json
json.load('/Users/ukannika/test.json')
205/7:
import json
# Read in any hyperparameters that the user passed with the training job
with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)
205/8:
import json
# Read in any hyperparameters that the user passed with the training job
with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)

print(trainingParams)
205/9:
import json
# Read in any hyperparameters that the user passed with the training job
with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)

for key, value in trainingParams.items():
    print key, value
205/10:
import json
# Read in any hyperparameters that the user passed with the training job
with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)

for key, value in trainingParams.items():
    print(key, value)
205/11:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=0, source_name='train.py')

with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)

for key, value in trainingParams.items():
    mlflow.log_param(key, value)
    
mlflow.end_run()
205/12:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=0, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)

for key, value in trainingParams.items():
    mlflow.log_param(key, value)
    
mlflow.end_run()
205/13:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=0, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)

for key, value in trainingParams.items():
    mlflow.log_param(key, value)
    
mlflow.end_run()
205/14:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.end_run()
mlflow.start_run(experiment_id=0, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)

for key, value in trainingParams.items():
    mlflow.log_param(key, value)
    
mlflow.end_run()
205/15:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=0, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)

for key, value in trainingParams.items():
    mlflow.log_param(key, value)
    
mlflow.end_run()
205/16:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)

for key, value in trainingParams.items():
    mlflow.log_param(key, value)
    
mlflow.end_run()
205/17:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams is None:
    print('executing')
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/18:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams is not None:
    print('executing')
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/19:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams.items() is not None:
    print('executing')
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/20:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams.itemsw() is not None:
    print('executing')
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/21:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if not trainingParams:
    print('executing')
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/22:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.end_run()
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if not trainingParams:
    print('executing')
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/23:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.end_run()
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test1.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams:
    print('executing')
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/24:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.end_run()
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams:
    print('executing')
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
204/1:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.start_run(experiment_id=1, source_name='train.py')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        # log training params    
        if trainingParams:
        for key, value in trainingParams.items():
            mlflow.log_param(key, value)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        mlflow.end_run()
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
205/25:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.end_run()
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams:
    for key, value in trainingParams:
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/26:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.end_run()
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams:
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/27:
import json
# Read in any hyperparameters that the user passed with the training job
mlflow.end_run()
mlflow.start_run(experiment_id=1, source_name='train.py')

with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams:
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
204/2:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.start_run(experiment_id=1, source_name='train.py')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        # log training params    
        if trainingParams:
        for key, value in trainingParams.items():
            mlflow.log_param(key, value)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        mlflow.end_run()
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/3:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.start_run(experiment_id=1, source_name='train.py')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        # log training params    
        if trainingParams:
        for key, value in trainingParams.items():
            mlflow.log_param(key, value)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        mlflow.end_run()
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/4:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.start_run(experiment_id=1, source_name='train.py')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        mlflow.end_run()
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/5:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.start_run(experiment_id=1, source_name='train.py')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        mlflow.end_run()
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/6:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri('http://localhost:5000')
    mlflow.start_run(experiment_id=1, source_name='train.py')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        mlflow.end_run()
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/7:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'
input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri()
    mlflow.start_run(experiment_id=1, source_name='train.py')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        mlflow.end_run()
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/8:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)
    mlflow.start_run(experiment_id=1, source_name='train.py')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        mlflow.end_run()
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/9:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active runs
    mlflow.end_run() 
    mlflow.start_run(experiment_id=1, source_name='train.py')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)
            
        # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
        # End mlflow active run.
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
205/28:
import json
# Read in any hyperparameters that the user passed with the training job
os.environ["EXPERIMENT_ID"] = "1"
mlflow.end_run()
mlflow.start_run(source_name='train.py')

with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams:
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/29:
import json
# Read in any hyperparameters that the user passed with the training job
os.environ["EXPERIMENT_ID"] = "1"
mlflow.end_run()
mlflow.start_run(source_name='train.py')

with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams:
    for key, value in trainingParams.items():
        mlflow.log_param(key, value)
    
mlflow.end_run()
205/30:
import json
# Read in any hyperparameters that the user passed with the training job
os.environ["EXPERIMENT_ID"] = "1"
mlflow.end_run()
mlflow.start_run(source_name='train.py')

with open('/Users/ukannika/test.json', 'r') as tc:
    trainingParams = json.load(tc)
    
if trainingParams:
    for key, value in trainingParams.items():
        mlflow.log_param('test', 12)
    
mlflow.end_run()
204/10:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Training complete.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
206/1:
import pandas as pd
import mlflow

from sklearn import linear_model
from sklearn.linear_model import ElasticNet
204/11:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        training(training_input_files)
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/12:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        training(training_input_files)
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/13:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        training(training_input_files)
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/14:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
        testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
        if len(training_input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        
        training(training_input_files)
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/15:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        training(training_input_files)
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/16:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
from training import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        training(training_input_files)
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/17:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        training(training_input_files)
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/18:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        training('testing')
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/19:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        training('testing')
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/20:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        training.train('testing')
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/21:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
from training import train

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        training.train('testing')
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/22:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
from training import train

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        training.hello('testing')
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/23:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
from training import hello

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        hello('testing')
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/24:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
from choicetraining import hello

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        hello('testing')
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/25:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import choice_training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        choice_training.train('testing')
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
204/26:
#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.
from __future__ import print_function
import os
import json
import pickle
import sys
import traceback

import pandas as pd
import mlflow
import choice_training

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'
mlflow_remote_uri = 'http://localhost:5000'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a channels of input data called 'training', 'validation', 'testing'. Since we run in
# File mode, the input files are copied to the directory specified here.
training_channel_name='training'
validation_channel_name='validation'
testing_channel_name='testing'
training_path = os.path.join(input_path, training_channel_name)
validation_path = os.path.join(input_path, validation_channel_name)
testing_path = os.path.join(input_path, testing_channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    mlflow.set_tracking_uri(mlflow_remote_uri)     
    # End any active run. 
    mlflow.end_run() 
    try:
        #Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         training_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         validation_input_files = [ os.path.join(validation_path, file) for file in os.listdir(validation_path) ]
#         testing_input_files = [ os.path.join(testing_path, file) for file in os.listdir(testing_path) ]
        
#         if len(training_input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
        
        choice_training.train('testing')
        choice_training.test('testing')
        choice_training.validation('testing')
        
        
        # save the model
        with open(os.path.join(model_path, 'linear-model.pkl'), 'w') as out:
            pickle.dump(regr, out)
        print('Writing training job params.')
          # log training params    
        if trainingParams:
            for key, value in trainingParams.items():
                mlflow.log_param(key, value)
        # End mlflow active run.
        mlflow.end_run()
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        # End mlflow active run.
        mlflow.end_run()
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
207/1:
import os

try:
    os.path.join('/Users/ukannika', 'test');
207/2:
import os

try:
    training_path = os.path.join('/Users/ukannika', 'test');
207/3:
import os

try:
    training_path = os.path.join('/Users/ukannika', 'test1234');
207/4:
import os
import sys

try:
    training_path = os.path.join('/Users/ukannika', 'test1234');
207/5:
import os
import sys

try:
    training_path = os.path.join('/Users/ukannika/', 'test1234');
207/6:
import os
import sys

try:
    training_path = os.path.join('/Users/ukannika/', 'test1234');
207/7:
import os
import sys
training_path = os.path.join('/Users/ukannika/', 'test1234');
207/8:
import os
import sys
training_path = os.path.join('/Users/ukannika/', 'test1234');
207/9:
import os
import sys
training_path = os.path.join('/Users/ukannika/', 'test1234');
207/10:
import os
import sys

training_path = os.path.join('/Users/ukannika/', 'test1234');
validation_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
207/11:
import os
import sys

training_path = os.path.join('/Users/ukannika/', 'test1234');

try:
    validation_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
207/12:
import os
import sys

training_path = os.path.join('/Users/ukannika/', 'test1234');

try:
    validation_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
except ValueError:
    print("No files found for validation")
207/13:
import os
import sys

training_path = os.path.join('/Users/ukannika/', 'test1234');

try:
    validation_input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
except FileNotFoundError:
    print("No files found for validation")
209/1: uuid.uuid4()
209/2:
import uuid
uuid.uuid4()
209/3:
import uuid
str(uuid.uuid4())
209/4:
// Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.csv')

    #intialize empty list that we will append dataframes to
    list_data = []

    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    print(data)
209/5:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.csv')

    #intialize empty list that we will append dataframes to
    list_data = []

    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    print(data)
209/6:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #specify a pattern of the file and pass it as a parameter into a glob function
    txt_files = glob.glob('*.csv')

    #intialize empty list that we will append dataframes to
    list_data = []

    #write a for loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, sep=',' )
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)
    data = pd.concat(list_data, ignore_index=True)
    print(data)

append_timestamp_and_concatenate_files()
210/1:
import mlflow
mlflow.set_tracking_uri('http://ec2-34-222-14-202.us-west-2.compute.amazonaws.com:5000')
mlflow.create_experiment('price-prediction-linear-regression')
211/1:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapeChar='\u0000' sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/2:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapeChar='\u0000', sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/3:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapeChar=None, sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/4:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar=None, sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/5:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/6:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        print(timestamp)
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        print(dapingestiontimestamp)
        print(daprecordid)
        print(dapsourcefilename)
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/7:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        print(timestamp)
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        print(dapingestiontimestamp)
        print(daprecordid)
        print(dapsourcefilename)
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/8:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        print(timestamp)
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        dataframe.show()
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/9:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        print(timestamp)
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        dataframe.head()
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/10:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        print(timestamp)
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        dataframe.head()
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/11:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        print(timestamp)
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)
        dataframe.head()

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/12:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.head()
    data.to_csv('ingestion.csv', index=None, sep=',')

append_timestamp_and_concatenate_files()
211/13:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    print(data)
    data.to_csv('ingestion.csv', index=None, sep=',')

append_timestamp_and_concatenate_files()
211/14:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        print(dataframe)
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')

append_timestamp_and_concatenate_files()
211/15:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/16:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None,  sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/17:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar=None, sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion1.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
211/18:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_timestamp_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('/Users/ukannika/work/choice/git/tar/2018-09-09/*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep=',')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion2.csv', index=None, sep=',')


append_timestamp_and_concatenate_files()
212/1:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('ingestion.csv', index=None, sep=',')
212/2:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []

    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    print(data)
212/3: append_system_columns_and_concatenate_files()
212/4:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')

    #Intialize empty list that will append dataframes to
    list_data = []
    print(txt_files)
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/5: append_system_columns_and_concatenate_files()
212/6: append_system_columns_and_concatenate_files()
212/7:
import pandas as pd
import glob as glob
import uuid
import numpy as np

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    print(np.split(txt_files, 2))
    #Intialize empty list that will append dataframes to
    list_data = []
    print(txt_files)
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/8: append_system_columns_and_concatenate_files()
212/9:
import pandas as pd
import glob as glob
import uuid
import numpy as np

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    print(np.split(txt_files, 2))
    #Intialize empty list that will append dataframes to
    list_data = []
    print(txt_files)
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/10: append_system_columns_and_concatenate_files()
212/11:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    #Intialize empty list that will append dataframes to
    list_data = []
    print(len(txt_files))
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/12: append_system_columns_and_concatenate_files()
212/13:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+n]
212/14: list(chunks(text_files), 2)
212/15:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    #Intialize empty list that will append dataframes to
    list_data = []
    
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/16: list(chunks(text_files), 2)
212/17:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    #Intialize empty list that will append dataframes to
    list_data = []
    print(list(chunks(text_files), 2))
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/18:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+n]
212/19: append_system_columns_and_concatenate_files()
212/20:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    #Intialize empty list that will append dataframes to
    list_data = []
    print(list(chunks(txt_files), 2))
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/21:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+n]
212/22: append_system_columns_and_concatenate_files()
212/23:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    #Intialize empty list that will append dataframes to
    list_data = []
    print(list(chunks(txt_files, 2)))
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/24:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+n]
212/25: append_system_columns_and_concatenate_files()
212/26:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+number_of_chunks]
212/27: append_system_columns_and_concatenate_files()
212/28:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    #Intialize empty list that will append dataframes to
    list_data = []
    list_of_files = list(chunks(txt_files, 2)))
    for files in list_of_files:
        print(files)
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/29:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+number_of_chunks]
212/30: append_system_columns_and_concatenate_files()
212/31:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    #Intialize empty list that will append dataframes to
    list_data = []
    list_of_files = list(chunks(txt_files, 2)))
    for files in list_of_files:
        print('hello')
        
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/32:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+number_of_chunks]
212/33: append_system_columns_and_concatenate_files()
212/34:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    #Intialize empty list that will append dataframes to
    list_data = []
    list_of_files = list(chunks(txt_files, 2))
    for files in list_of_files:
        print('hello')
        
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/35:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+number_of_chunks]
212/36: append_system_columns_and_concatenate_files()
212/37:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    #Intialize empty list that will append dataframes to
    list_data = []
    list_of_files = list(chunks(txt_files, 2))
    for files in list_of_files:
        print(files)
        
    #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
    for filename in txt_files:
        timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
        dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
        dataframe['dapingestiontimestamp'] = timestamp
        dataframe['daprecordid'] = str(uuid.uuid4())
        dataframe['dapsourcefilename'] = filename
        list_data.append(dataframe)

    data = pd.concat(list_data, ignore_index=True)
    #data.to_csv('./upload/ingestion.csv', index=None, sep=',')
212/38:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+number_of_chunks]
212/39: append_system_columns_and_concatenate_files()
212/40:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    
    list_of_files = list(chunks(txt_files, 2))
    
    for files in list_of_files:       
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        data.to_csv('ingestion.csv', index=None, sep=',')
212/41:
def chunks(list, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+number_of_chunks]
212/42: append_system_columns_and_concatenate_files()
212/43:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    
    list_of_files = list(chunks(txt_files, 2))
    
    for files in list_of_files:       
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        data.to_csv('ingestion.csv', index=None, sep=',')
212/44:
def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list[i:i+number_of_chunks]
212/45: append_system_columns_and_concatenate_files()
212/46:
def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
212/47: append_system_columns_and_concatenate_files()
212/48:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 2))
    for files in list_of_files(index):       
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        print(index)
        data.to_csv('ingestion.csv', index=None, sep=',')
212/49:
def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
212/50: append_system_columns_and_concatenate_files()
212/51:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 2))
    for index, files in enumerate(list_of_files):       
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        print(index)
        data.to_csv('ingestion.csv', index=None, sep=',')
212/52:
def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
212/53: append_system_columns_and_concatenate_files()
212/54:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 2))
    for index, files in enumerate(list_of_files):       
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        print(index)
        data.to_csv(index, index=None, sep=',')
212/55:
def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
212/56: append_system_columns_and_concatenate_files()
212/57:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 2))
    for index, files in enumerate(list_of_files):       
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        print(index)
        data.to_csv('ingestion'+index+'.csv', index=None, sep=',')
212/58:
def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
212/59: append_system_columns_and_concatenate_files()
212/60:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 2))
    for index, files in enumerate(list_of_files):       
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        print(index)
        data.to_csv('ingestion'+`index`+'.csv', index=None, sep=',')
212/61:
def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
212/62: append_system_columns_and_concatenate_files()
212/63:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 2))
    for index, files in enumerate(list_of_files):       
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        print(index)
        data.to_csv('ingestion'+'index'+'.csv', index=None, sep=',')
212/64:
def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
212/65: append_system_columns_and_concatenate_files()
212/66:
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 2))
    for index, files in enumerate(list_of_files):       
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        print(index)
        data.to_csv('ingestion'+str(index)+'.csv', index=None, sep=',')
212/67:
def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
212/68: append_system_columns_and_concatenate_files()
213/1: import scikitlearn
213/2: from sklearn.linear_model import ElasticNet
213/3:
from sklearn.linear_model import ElasticNet
import pandas as pd
213/4:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5)
model.fit(x_train, y_train)
213/5:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
213/6:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5)
model.fit(x_train, y_train)
213/7:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
213/8:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5)
model.fit(x_train, y_train)

y_predictions = model.predict(x_test);
    
MSE = mean_squared_error(y_test, y_predictions)
R2 = r2_score(y_test, y_predictions)

print(R2)
print(MSE)
213/9:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
213/10:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=1.0, l1_ratio=0.5)
model.fit(x_train, y_train)

y_predictions = model.predict(x_test);
    
MSE = mean_squared_error(y_test, y_predictions)
R2 = r2_score(y_test, y_predictions)

print(R2)
print(MSE)
213/11:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=3.0, l1_ratio=2.5)
model.fit(x_train, y_train)

y_predictions = model.predict(x_test);
    
MSE = mean_squared_error(y_test, y_predictions)
R2 = r2_score(y_test, y_predictions)

print(R2)
print(MSE)
213/12:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=1.0)
model.fit(x_train, y_train)

y_predictions = model.predict(x_test);
    
MSE = mean_squared_error(y_test, y_predictions)
R2 = r2_score(y_test, y_predictions)

print(R2)
print(MSE)
213/13:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.50, l1_ratio=0.0)
model.fit(x_train, y_train)

y_predictions = model.predict(x_test);
    
MSE = mean_squared_error(y_test, y_predictions)
R2 = r2_score(y_test, y_predictions)

print(R2)
print(MSE)
213/14:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.2)
model.fit(x_train, y_train)

y_predictions = model.predict(x_test);
    
MSE = mean_squared_error(y_test, y_predictions)
R2 = r2_score(y_test, y_predictions)

print(R2)
print(MSE)
213/15:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

print(y_test)

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.2)
model.fit(x_train, y_train)

y_predictions = model.predict(x_test);
    
MSE = mean_squared_error(y_test, y_predictions)
R2 = r2_score(y_test, y_predictions)

print(R2)
print(MSE)
213/16:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.2)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test, y_pred)
MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/17:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.2)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test, y_pred)
MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/18:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.2)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/19:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=1, l1_ratio=0.2)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/20:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.2)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/21:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=2, l1_ratio=0.2)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)
MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/22:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=3, l1_ratio=0.2)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)
MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/23:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=4, l1_ratio=0.2)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)
MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/24:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=4, l1_ratio=3.2)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)
MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/25:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=4, l1_ratio=1.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)
MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/26:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0).dropna()
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0).dropna()
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=4, l1_ratio=1.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)
MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/27:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=4, l1_ratio=1.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);
    
print(y_test)
print(y_pred)
MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/28:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/29:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=1.5, l1_ratio=0.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/30:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=2.5, l1_ratio=0.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/31:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=2.5, l1_ratio=1.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/32:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=2.5, l1_ratio=3.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/33:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=2.5, l1_ratio=2.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/34:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=2.5, l1_ratio=1.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/35:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=3.5, l1_ratio=1.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/36:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=3.5, l1_ratio=1.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred, sample_weight=22000)

print(R2)
print(MSE)
213/37:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=3.5, l1_ratio=1.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/38:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=1.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/39:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.0)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/40:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/41:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MSE)
213/42:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
213/43:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

EVS = explained_variance_score(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(EVS)
213/44:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
from sklearn.metrics import explained_variance_score, mean_absolute_error, r2_score
213/45:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

EVS = explained_variance_score(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(EVS)
213/46:
train_data = pd.read_csv('/Users/ukannika/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/47:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/48:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
from sklearn.metrics import explained_variance_score, mean_absolute_error,mean_squared_error, r2_score
213/49:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/50:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/51:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.LinearRegression()
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/52:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.LinearRegression()
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/53:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

model = linear_model.LinearRegression()
model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/54:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/55:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.2, l1_ratio=0.5)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/56:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.5, l1_ratio=0)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/57:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.5)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/58:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.8, l1_ratio=0.5)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/59:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.8, l1_ratio=0.5)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/60:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=1.8, l1_ratio=0.5)
model.fit(x_train, y_train)
y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/61:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.5)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
213/62:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score
213/63:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.5)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/1:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.5)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/2:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score
219/3:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

elaticnet = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.5)

model.fit(x_train, y_train)

y_pred = model.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/4:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.5)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/5:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.5)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/6:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score
219/7:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.5, l1_ratio=0.5)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/8:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.5)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/9:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.2, l1_ratio=0.5)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/10:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.6, l1_ratio=0.5)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/11:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=1, l1_ratio=0.5)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/12:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=3, l1_ratio=0.5)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/13:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.3, l1_ratio=0.2)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/14:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.7, l1_ratio=0.2)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
219/15:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.7, l1_ratio=0.2)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
220/1:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.7, l1_ratio=0.2)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
220/2:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score
220/3:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.7, l1_ratio=0.2)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
220/4:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.7, l1_ratio=0.2)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
220/5:
train_data = pd.read_csv('/Users/ukannika/Desktop/data/train.csv').fillna(0)
test_data = pd.read_csv('/Users/ukannika/Desktop/data/test.csv').fillna(0)
x_train = train_data.iloc[:, :-1]
y_train  = train_data.iloc[:, -1]
x_test = test_data.iloc[:, :-1]
y_test  = test_data.iloc[:, -1]

enet = linear_model.ElasticNet(alpha=0.7, l1_ratio=0.2)
enet.fit(x_train, y_train)
y_pred = enet.predict(x_test);

MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)

print(R2)
print(MAE)
print(MSE)
220/6:
from sklearn import linear_model
from sklearn.linear_model import ElasticNet
import pandas as pd
from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score
221/1:
import mlflow
mlflow.set_tracking_uri('http://ec2-18-237-106-232.us-west-2.compute.amazonaws.com:5000')
mlflow.create_experiment(name='demo-price-prediction-enet')
221/2:
import mlflow
mlflow.set_tracking_uri('http://ec2-18-237-106-232.us-west-2.compute.amazonaws.com:5000')
mlflow.create_experiment('demo-price-prediction-enet')
221/3:
import mlflow
mlflow.set_tracking_uri('http://ec2-18-237-106-232.us-west-2.compute.amazonaws.com:5000')
mlflow.create_experiment('mastery-price-prediction-enet')
221/4:
import mlflow
mlflow.set_tracking_uri('http://ec2-18-237-106-232.us-west-2.compute.amazonaws.com:5000')
mlflow.create_experiment('price-prediction-linear-regression')
222/1:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 10))
    for index, files in enumerate(list_of_files):
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        data.to_csv('ingestion'+str(index)+'.csv', index=None, sep=',')


def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
222/2: append_system_columns_and_concatenate_files()
222/3:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 2))
    for index, files in enumerate(list_of_files):
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        data.to_csv('ingestion'+str(index)+'.csv', index=None, sep=',')


def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
222/4: append_system_columns_and_concatenate_files()
222/5:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 6))
    for index, files in enumerate(list_of_files):
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        data.to_csv('ingestion'+str(index)+'.csv', index=None, sep=',')


def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
222/6: append_system_columns_and_concatenate_files()
222/7:
## Copyright 2018 Choice Hotels International
import pandas as pd
import glob as glob
import uuid

def append_system_columns_and_concatenate_files():
    #A pattern of the files to load.
    txt_files = glob.glob('*.txt')
    list_of_files = list(chunks(txt_files, 1))
    for index, files in enumerate(list_of_files):
        #Intialize empty list that will append dataframes to
        list_data = []
        #A for-loop that will go through each of the file name through globbing and the end result will be the list of dataframes
        for filename in files:
            timestamp = filename.rpartition('.')[0].rpartition('_')[-1]
            dataframe = pd.read_csv(filename, header=None, escapechar='\u0000', sep='\001')
            dataframe['dapingestiontimestamp'] = timestamp
            dataframe['daprecordid'] = str(uuid.uuid4())
            dataframe['dapsourcefilename'] = filename
            list_data.append(dataframe)

        data = pd.concat(list_data, ignore_index=True)
        data.to_csv('ingestion'+str(index)+'.csv', index=None, sep=',')


def chunks(list_to_chunk, number_of_chunks):
    # For item i in a range that is a length of l,
    for i in range(0, len(list_to_chunk), number_of_chunks):
        # Create an index range for l of n items:
        yield list_to_chunk[i:i+number_of_chunks]
222/8: append_system_columns_and_concatenate_files()
223/1:
%load_ext sparkmagic.magics
%manage_spark
224/1:
%load_ext sparkmagic.magics
%manage_spark
224/2:
%%spark
val dataset = spark.read.option("sep", ",").option("header", "true").csv("s3a://choice-mlflow-input/umasrivenkat_kannikanti/data/price-prediction/training")
224/3:
%%spark
val dataset = spark.read.option("sep", ",").option("header", "true").csv("s3a://choice-mlflow-input/umasrivenkat_kannikanti/data/price-prediction/training"
224/4:
%%spark
println("hello")
224/5:
%%spark
println("hello")
224/6:
%load_ext sparkmagic.magics
%manage_spark
226/1:
%load_ext sparkmagic.magics
%manage_spark
226/2:
%load_ext sparkmagic.magics
%manage_spark
226/3:
%load_ext sparkmagic.magics
%manage_spark
226/4:
%load_ext sparkmagic.magics
%manage_spark
226/5:
%load_ext sparkmagic.magics
%manage_spark
227/1:
%load_ext sparkmagic.magics
%manage_spark
228/1:
%load_ext sparkmagic.magics
%manage_spark
229/1:
%%spark
spark
229/2:
%load_ext sparkmagic.magics
%manage_spark
231/1: println("Action1")
231/2:
%python
println("Action1")
231/3:
%%python
println("Action1")
231/4:
%%python
print("Action1")
231/5:
%%python
print("Action1")
231/6:
%%python
print("Action2")
233/1:
timestamp=120
if(str.len(timestamp) == 3)
    print("correcy")
233/2:
timestamp=120
print(len(timestamp))
233/3:
timestamp=120
print(len(str(timestamp))
233/4: val dataset = spark.read.option("sep", ",").option("header", "true").csv("s3a://choice-mlflow-input/umasrivenkat_kannikanti/data/price-prediction/training"
233/5:
timestamp=120
print(len(str(timestamp)))
233/6:
timestamp='120'
print(len(str(timestamp)))
233/7:
timestamp='120'
print(len(timestamp))
233/8:
timestamp='120
if(len(str(timestamp)) == 3)
    print(correct)
233/9:
timestamp=120
if(len(str(timestamp)) == 3)
    print(correct)
233/10:
timestamp=120
if(len(str(timestamp))) == 3)
    print(correct)
233/11:
timestamp=120
if(len(str(timestamp)) == 3)
    print(correct)
233/12:
timestamp=120
if len(str(timestamp)) == 3
    print(correct)
233/13: val dataset = spark.read.option("sep", ",").option("header", "true").csv("s3a://choice-mlflow-input/umasrivenkat_kannikanti/data/price-prediction/training"
233/14:
timestamp=120
if len(str(timestamp)) == 3:
    print(correct)
233/15:
timestamp=120
if len(str(timestamp)) == 3:
    print('correct')
234/1:
%python
import argparse
234/2: import argparse
234/3:
import argparse
parser = argparse.ArgumentParser(description='Arguments')
234/4:
import argparse
parser = argparse.ArgumentParser(description='Arguments')
parser.add_argument(name="basefilename")
234/5:
import argparse
parser = argparse.ArgumentParser(description='Arguments')
parser.add_argument(name="basefilename", dest="base_file_name")
234/6:
import argparse
parser = argparse.ArgumentParser(description='Arguments')
parser.add_argument(dest="base_file_name")
234/7:
import argparse
parser = argparse.ArgumentParser(description='Arguments')
parser.add_argument("basefilename", dest="base_file_name")
234/8:
import argparse
parser = argparse.ArgumentParser(description='Arguments')
parser.add_argument("base_file_name", help="echo the string you use here")
234/9: function()
236/1:
str = "if len(timestamp) == 14"

if len(str) == 14
    print("Yes")
236/2:
str = "if len(timestamp) == 14"

if len(str) == 14:
    print("Yes")
236/3:
str = "20180804002000"

if len(str) == 14:
    print("Yes")
236/4:
str = "20180804002000"

if len(str) == 14:
    print("Yes")
else:
    print("No")
236/5:
str = "2018080400200"

if len(str) == 14:
    print("Yes")
else:
    print("No")
236/6:
import time
print(time.time())
236/7:
import time
print(time.time().strftime('%Y-%m-%d %H:%M:%S')
236/8:
import time
print(time.time().strftime('%Y-%m-%d %H:%M:%S'))
236/9:
import time
print(time.strftime('%Y-%m-%d %H:%M:%S'))
238/1: %pyspark info
238/2: %%spark info
238/3: %%spark
239/1:
%%spark
sc
239/2:
%%spark
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
239/3:
%%spark
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
239/4:
%%spark
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
distData.show
240/1:
%%spark
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
distData.show
240/2:
%%spark
pyspark
240/3: %pyspark
240/4: %%spark
240/5: %sc
240/6:
%sc
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
240/7:
%sc
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
distData.show
240/8:
%sc
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
distData.show()
240/9:
%sc
data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)
distData.show()
240/10:
%sc
df = sc.createDataFrame([("foo", 1), ("bar", 2), ("baz", 3)], ('k', 'v'))
df.show(n=2)
240/11:
%sc
data = [1, 2, 3, 4, 5]
 sc.textFile("file:///Users/ukannika/").show()
240/12:
%sc
sc.textFile("file:///Users/ukannika/").show()
240/13:
%sc
import SparkSession
sc.textFile("file:///Users/ukannika/").show()
240/14:
%sc
sc.read.text("file:///Users/ukannika/").show()
240/15:
%sc
spark.read.text("file:///Users/ukannika/").show()
240/16: %pyspark
240/17: %sc info
240/18: %%spark info
240/19: %pyspark info
241/1: %%spark
241/2:
%%spark
import SparkSession
241/3:
%%spark
import SparkSession
242/1:
%%spark
import SparkSession
243/1:
%%spark
import SparkSession
243/2:
%%spark
spark
244/1: import SparkSession
245/1:
%%spark
import SparkSession
246/1: import SparkSession
248/1: import SparkSession
248/2:
import findspark
findspark.init()

import pyspark # only run after findspark.init()
from pyspark.sql import SparkSession
248/3:
import findspark
findspark.init()

import pyspark # only run after findspark.init()
from pyspark.sql import SparkSession
249/1:
import findspark
findspark.init()

import pyspark # only run after findspark.init()
from pyspark.sql import SparkSession
250/1: import SparkSession
250/2: import SparkSession
251/1:
import findspark
findspark.init()

import pyspark # only run after findspark.init()
from pyspark.sql import SparkSession
251/2:
import pyspark # only run after findspark.init()
from pyspark.sql import SparkSession
251/3:
import pyspark # only run after findspark.init()
from pyspark.sql import SparkSession
251/4:
spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
251/5: spark.read.json("/Users/ukannika/test.json")
251/6: spark.read.json("/Users/ukannika/test.json").show()
251/7: spark.read.txt("/Users/ukannika/test.json").show()
251/8: spark.read.text("/Users/ukannika/test.json").show()
252/1: %pyspark
253/1:
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
253/2: print(spark)
254/1: import LinearRegression
255/1: from sklearn.neighbors.kde import KernelDensity
255/2:
from sklearn.neighbors.kde import KernelDensity
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
255/3:
from sklearn.neighbors.kde import KernelDensity
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
X
255/4:
from sklearn.neighbors.kde import KernelDensity
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)
kde.score_samples(X)
255/5:
from sklearn.neighbors.kde import KernelDensity
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
kde = KernelDensity(kernel='gaussian', bandwidth=1).fit(X)
kde.score_samples(X)
255/6:
from sklearn.neighbors.kde import KernelDensity
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
kde = KernelDensity(kernel='gaussian', bandwidth=1).fit(X)
kde.score_samples(X)
255/7:
from sklearn.neighbors.kde import KernelDensity
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
kde = KernelDensity(kernel='gaussian', bandwidth=1).fit(X)
kde.score_samples(X)
255/8:
from sklearn.neighbors.kde import KernelDensity
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)
kde.score_samples(X)
258/1: import pyspark
258/2: from pyspark.sql import SparkSession
258/3: from pyspark.sql import SparkSession
258/4: from pyspark.sql import SparkSession
258/5:
spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
258/6:
spark = SparkSession \
    .builder \
    .appName("LinearRegression") \
    .getOrCreate()
259/1: spark.read.csv("USV_housing.csv").show
259/2: spark.read.csv("USV_housing.csv").show(false)
259/3: spark.read.csv("USV_housing.csv").show()
259/4: spark.read.csv("USV_housing.csv").show(10, false)
259/5: spark.read.csv("USV_housing.csv").show(n=20, truncate=True)
259/6: spark.read..option("header", True).csv("USV_housing.csv").show(n=20, truncate=false)
259/7: spark.read.csv(path="USV_housing.csv", header=True).show(n=20, truncate=false)
259/8: spark.read.csv(path="USV_housing.csv", header=True).show(n=20, truncate=False)
259/9: spark.read.csv(path="USV_housing.csv", header=True).show(n=20, truncate=False).select("")
259/10: spark.read.csv(path="USV_housing.csv", header=True).show(n=20, truncate=False).select("Avg_Area_Income")
259/11: spark.read.csv(path="USV_housing.csv", header=True).select("Avg_Area_Income").show(n=20, truncate=False)
259/12: spark.read.csv(path="USV_housing.csv", header=True, quote="\"", escape="\"").select("Avg_Area_Income").show(n=20, truncate=False)
259/13: spark.read.csv(path="USV_housing.csv", header=True, quote="\"", escape="\"").select("Avg_Area_Income").show(n=20, truncate=False)
259/14: spark.read.csv(path="USV_housing.csv", header=True, quote="\"", escape="\"").select("Avg_Area_Income").show(n=20, truncate=False)
259/15: spark.read.csv(path="USV_housing.csv", header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"").select("Avg_Area_Income").show(n=20, truncate=False)
259/16: spark.read.csv(path="USV_housing.csv", header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"").select("Avg_Area_Income").count()
259/17:
spark.read.csv(path="USV_housing.csv", header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
.select("Avg_Area_Income", "Avg_Area_House_Age", "Avg_Area_Number_of_Rooms", "Avg_Area_Number_of_Bedrooms", "Area_Population", "Price")
.show(n=10, truncate=False)
259/18:
spark.read.csv(path="USV_housing.csv", header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
.select("Avg_Area_Income", "Avg_Area_House_Age", "Avg_Area_Number_of_Rooms", "Avg_Area_Number_of_Bedrooms", "Price")
.show(n=10, truncate=False)
259/19:
spark.read.csv(path="USV_housing.csv", header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"") \
.select("Avg_Area_Income", "Avg_Area_House_Age", "Avg_Area_Number_of_Rooms", "Avg_Area_Number_of_Bedrooms", "Area_Population", "Price") \
.show(n=10, truncate=False)
259/20:
spark.read.csv(path="USV_housing.csv", header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
.select("Income","Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price")
.show(n=10, truncate=False)
259/21:
spark.read.csv(path="USV_housing.csv", header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income","Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.show(n=10, truncate=False)
259/22:
spark.read.csv(path="USV_housing.csv", header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(round("Income", 0),"Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.show(n=10, truncate=False)
259/23:
spark.read.csv(path="USV_housing.csv", header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(round('Income', 0).alias('Income'),"Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.show(n=10, truncate=False)
259/24:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(round('Income', 0).alias('Income'),"Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.show(n=10, truncate=False)
259/25:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
259/26:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(round('Income', 0).alias('Income'),"Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.show(n=10, truncate=False)
259/27:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(round('Income', 0).alias('Income'),"Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.show(n=10, truncate=False)
259/28:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import StructField
259/29:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(round('Income', 0).alias('Income'),"Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.show(n=10, truncate=False)
259/30:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
259/31:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(round('Income', 0).alias('Income'),"Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.show(n=10, truncate=False)
259/32:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income","Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.show(n=10, truncate=False)
259/33:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income","Age","Number_of_Rooms","Number_of_Bedrooms","Area_Population", "Price") \
.printSchema()
259/34:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(round("Income", 0) \
.show(n=10, truncate=False)
259/35:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(round("Income", 0)) \
.show(n=10, truncate=False)
259/36:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income") \
.where(col("Income").isNull())
.show(n=10, truncate=False)
259/37:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income") \
.where(col("Income").isNull()) \
.show(n=10, truncate=False)
259/38:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import Column
259/39:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income") \
.where(col("Income").isNull()) \
.show(n=10, truncate=False)
259/40:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
259/41:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income") \
.where(col("Income").isNull()) \
.show(n=10, truncate=False)
259/42:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income") \
.where(col("Income").isNull()) \
.count()
259/43:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income") \
.where(col("Income").isNotNull()) \
.show(n=1200, truncate=False)
259/44:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income") \
.where(col("*").isNotNull()) \
.show(n=1200, truncate=False)
259/45:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(cast("Income", "double")) \
.show(n=1200, truncate=False)
259/46:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from pyspark.sql.functions import cast
259/47:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
259/48:
schema = StructType([StructField('Income', DoubleType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(cast("Income", "double")) \
.show(n=1200, truncate=False)
259/49:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select(cast("Income", "double")) \
.show(n=1200, truncate=False)
259/50:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DoubleType(), True),
                     StructField('Number_of_Rooms', DoubleType(), True),
                     StructField('Number_of_Bedrooms', DoubleType(), True),
                     StructField('Area_Population', DoubleType(), True),
                     StructField('Price', DoubleType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income") \
.show(n=1200, truncate=False)
259/51:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income") \
.show(n=1200, truncate=False)
259/52:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") \
.show(n=1200, truncate=False)
259/53:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") 

dataset.show(n=1200, truncate=False)
259/54:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") \

dataset.show(n=1200, truncate=False)
259/55:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read \
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") 

dataset.show(n=1200, truncate=False)
259/56:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read \
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") 

dataset.show(n=10, truncate=False)
259/57:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
259/58:
spark = SparkSession \
    .builder \
    .appName("LinearRegression") \
    .config("spark.sql.execution.arrow.enabled", True)
    .getOrCreate()
259/59:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read \
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") 

dataset.show(n=10, truncate=False)

dataset.toPandas
259/60:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read \
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") 

dataset.show(n=10, truncate=False)

df = dataset.toPandas()
df.show()
259/61:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read \
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") 

df = dataset.toPandas()
df.show()
259/62:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read \
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") 

df = dataset.toPandas()
df.head()
259/63:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
259/64:
spark = SparkSession \
    .builder \
    .appName("LinearRegression") \
    .config("spark.sql.execution.arrow.enabled", True)
    .getOrCreate()
259/65:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read \
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") 

df = dataset.toPandas()
df.head()
259/66:
spark = SparkSession \
    .builder \
    .appName("LinearRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
259/67:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read \
               .csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")\
               .select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population", "Price") 

df = dataset.toPandas()
df.head()
259/68:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
259/69:
spark = SparkSession \
    .builder \
    .appName("LinearRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
259/70:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = dataset.select("Price")

# Create linear regression object
linearRegression = linear_model.LinearRegression()
linearRegression.fit(train_X, train_Y)
259/71:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = dataset.select("Price")

# Create linear regression object
linearRegression = linear_model.LinearRegression()
linearRegression.fit(train_X.toPandas(), train_Y.toPandas())
259/72: print('Coefficients: \n', regr.coef_)
259/73: print('Coefficients: \n', linearRegression.coef_)
259/74:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
259/75:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
259/76:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(train_X.toPandas(), train_Y.toPandas())

# Make predictions using the testing set
pred_Y = linearRegression.predict(test_X)
259/77:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(train_X.toPandas(), train_Y.toPandas())

# Make predictions using the testing set
pred_Y = linearRegression.predict(test_X.toPandas())
259/78: print(pred_Y)
259/79:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")

test_dataset.count()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
259/80:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")

print(test_dataset.count())

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
259/81:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
259/82:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(train_X.toPandas(), train_Y.toPandas())

# Make predictions using the testing set
pred_Y = linearRegression.predict(test_X.toPandas())
259/83:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
259/84:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
259/85:
spark = SparkSession \
    .builder \
    .appName("LinearRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
259/86:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
259/87:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test)
259/88:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))


# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

# Plot outputs
plt.scatter(X_test, Y_test,  color='black')
plt.plot(X_test, Y_pred, color='blue', linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()
259/89:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
259/90:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
259/91:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))


# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))


ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/92:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_absolute_error(Y_test, Y_pred))


# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))


ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/93:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
259/94:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean squared error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))


ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/95:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))


ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/96:
# Create linear regression object 
linearRegression = linear_model.HuberRegressor()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test)
259/97:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/98:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test)
259/99:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/100: print(Y_pred)
259/101:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/102: sns.regplot(x=X_test, y=Y_test, color="g")
259/103: sns.regplot(x=Y_test, y=Y_pred, color="g")
259/104:
Y_test
sns.regplot(x=Y_test, y=Y_pred, color="g")
259/105:
print(Y_test)
sns.regplot(x=Y_test, y=Y_pred, color="g")
259/106:
print(Y_test, Y_pred)
sns.regplot(x=Y_test, y=Y_pred, color="g")
259/107:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).round(2)
259/108:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/109: print(Y_test, Y_pred)
259/110: sns.regplot(x=Y_test, y=Y_pred, color="g")
259/111:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).round(0)
259/112:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/113: sns.regplot(x=Y_test, y=Y_pred, color="g")
259/114:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).round(0)
print(Y_pred)
259/115:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).round(2)
print(Y_pred)
259/116:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).round(2)
259/117:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).round(2)

print(Y_pred.astype("Decimal"))
259/118:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).round(2)

print(Y_pred.astype("integer"))
259/119:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).round(2)

print(Y_pred.astype("int"))
259/120:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).astype("int")

print(Y_pred)
259/121:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).astype("int")
259/122:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/123: sns.regplot(x=Y_test, y=Y_pred, color="g")
259/124: sns.pairplot(train_X)
259/125: sns.pairplot(X_train)
259/126: sns.pairplot(training_dataset.toPandas(), "Price")
259/127: sns.pairplot(data=training_dataset.toPandas(), hue="Price")
259/128: sns.pairplot(data=training_dataset.toPandas(), hue="Price")
259/129: sns.pairplot(data=training_dataset.toPandas())
259/130:
schema = StructType([StructField('Income', IntegerType(), True),
                     StructField('Age', IntegerType(), True),
                     StructField('Number_of_Rooms', IntegerType(), True),
                     StructField('Number_of_Bedrooms', IntegerType(), True),
                     StructField('Area_Population', IntegerType(), True),
                     StructField('Price', IntegerType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
259/131:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).astype("int")
259/132:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/133:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = train_Y.toPandas()
259/134:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).astype("int")
259/135:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/136: sns.pairplot(data=training_dataset.toPandas())
259/137:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
259/138:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).astype("int")
259/139:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/140:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value", ax=ax1)
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
259/141:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
260/1:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
260/2:
spark = SparkSession \
    .builder \
    .appName("RidgeRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
261/1:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
261/2:
spark = SparkSession \
    .builder \
    .appName("RidgeRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
261/3:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
262/1:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
262/2:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
262/3:
spark = SparkSession \
    .builder \
    .appName("RidgeRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
262/4:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
262/5:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
262/6: print(ridgeRegression.coef_)
262/7:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
262/8:
spark = SparkSession \
    .builder \
    .appName("RidgeRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
262/9:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
262/10:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
262/11: print(ridgeRegression.coef_)
262/12:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
262/13: print(ridgeRegression.coef_)
262/14: print(ridgeRegression.coef_, ridgeRegression.intercept_)
262/15: print(ridgeRegression.coef_)
264/1: print(ridgeRegression.coef_)
264/2:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
264/3:
spark = SparkSession \
    .builder \
    .appName("RidgeRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
264/4:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
264/5:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/6: print(ridgeRegression.coef_)
264/7:
print(ridgeRegression.coef_)
print(Y_pred)
264/8: print()
264/9: print(ridgeRegression.coef_)
264/10:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=1.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/11:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=9.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/12:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/13:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.001)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/14:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.001)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/15:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.001)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/16:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.00001)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/17: print(ridgeRegression.coef_)
264/18:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/19:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/20:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
258/7:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
258/8:
spark = SparkSession \
    .builder \
    .appName("LinearRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
258/9:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
258/10:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).astype("int")
258/11:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
258/12:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
265/1:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
265/2:
spark = SparkSession \
    .builder \
    .appName("LinearRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
265/3:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
265/4:
# Create linear regression object 
linearRegression = linear_model.LinearRegression()

# Train the model using the training sets
linearRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = linearRegression.predict(X_test).astype("int")
265/5:
# The coefficients
print('Coefficients: \n', linearRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/21:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=100)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/22:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/23:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
264/24:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=1000)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/25:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/26:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=10000)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/27:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/28:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=100000)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/29:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/30:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.5)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/31:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/32:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.000008)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/33:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/34:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=10000)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/35:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/36:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=100000)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/37:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/38:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.1)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/39:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
264/40:
training_dataset = spark.read.csv(path="datasets/train_regression.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/test_regression.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
training_dataset.show()
test_dataset.show()
264/41:
training_dataset = spark.read.csv(path="datasets/train_regression.csv",  header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/test_regression.csv",  header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
training_dataset.show()
test_dataset.show()
264/42:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
264/43:
# Create Ridge regression object 
ridgeRegression = linear_model.Ridge(alpha=0.1)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
264/44:
# The coefficients
print('Coefficients: \n', ridgeRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
266/1:
# Create Ridge regression object 
ridgeRegression = linear_model.Lasso(alpha=0.1)

# Train the model using the training sets
ridgeRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = ridgeRegression.predict(X_test).astype("int")
266/2:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
266/3:
spark = SparkSession \
    .builder \
    .appName("RidgeRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
266/4:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
266/5:
# Create Ridge regression object 
lassoRegression = linear_model.Lasso(alpha=0.1)

# Train the model using the training sets
lassoRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = lassoRegression.predict(X_test).astype("int")
266/6:
# The coefficients
print('Coefficients: \n', lassoRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
266/7:
# Create Ridge regression object 
lassoRegression = linear_model.Lasso(alpha=10000)

# Train the model using the training sets
lassoRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = lassoRegression.predict(X_test).astype("int")
266/8:
# The coefficients
print('Coefficients: \n', lassoRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
266/9:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import norm
from sklearn.preprocessing import StandardScaler
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline
266/10: df_train = pd.read_csv('datasets/train_regression.csv')
266/11: df_train.columns
266/12: sns.distplot(df_train['SalePrice']);
266/13:
#scatter plot grlivarea/saleprice
var = 'GrLivArea'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));
266/14:
#scatter plot totalbsmtsf/saleprice
var = 'TotalBsmtSF'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));
266/15:
#box plot overallqual/saleprice
var = 'OverallQual'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
f, ax = plt.subplots(figsize=(8, 6))
fig = sns.boxplot(x=var, y="SalePrice", data=data)
fig.axis(ymin=0, ymax=800000);
266/16:
var = 'YearBuilt'
data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)
f, ax = plt.subplots(figsize=(16, 8))
fig = sns.boxplot(x=var, y="SalePrice", data=data)
fig.axis(ymin=0, ymax=800000);
plt.xticks(rotation=90);
270/1:
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
from pyspark.sql.types import *
from pyspark.sql.functions import col
from sklearn import linear_model
from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
270/2:
spark = SparkSession \
    .builder \
    .appName("RidgeRegression") \
    .config("spark.sql.execution.arrow.enabled", True) \
    .getOrCreate()
270/3:
schema = StructType([StructField('Income', DecimalType(), True),
                     StructField('Age', DecimalType(), True),
                     StructField('Number_of_Rooms', DecimalType(), True),
                     StructField('Number_of_Bedrooms', DecimalType(), True),
                     StructField('Area_Population', DecimalType(), True),
                     StructField('Price', DecimalType(), True)])

training_dataset = spark.read.csv(path="datasets/USV_housing.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
test_dataset = spark.read.csv(path="datasets/USV_housing_test.csv", schema=schema, header=True, charToEscapeQuoteEscaping="\"", multiLine=True, quote="\"", escape="\"")
 
train_X = training_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
train_Y = training_dataset.select("Price")
X_train = train_X.toPandas()
Y_train = train_Y.toPandas()

test_X = test_dataset.select("Income", "Age", "Number_of_Rooms", "Number_of_Bedrooms", "Area_Population")
test_Y = test_dataset.select("Price")
X_test = test_X.toPandas()
Y_test = test_Y.toPandas()
270/4:
# Create Ridge regression object 
lassoRegression = linear_model.Lasso(alpha=0.1)

# Train the model using the training sets
lassoRegression.fit(X_train, Y_train)

# Make predictions using the testing set
Y_pred = lassoRegression.predict(X_test).astype("int")
270/5:
# The coefficients
print('Coefficients: \n', lassoRegression.coef_)

# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(Y_test, Y_pred))

# The mean absolute error
print("Mean absolute error: %.2f" % mean_absolute_error(Y_test, Y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(Y_test, Y_pred))

ax1 = sns.distplot(Y_test, hist=False, color="r", label="Actual Value")
sns.distplot(Y_pred, hist=False, color="b", label="Fitted Values" , ax=ax1)
272/1:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
272/2:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
272/3:
df = pd.read_csv('insurance.csv') # import the CSV as a pandas dataframe

df.head() # show the first five rows
272/4:
df = pd.read_csv('./datasets/insurance.csv') # import the CSV as a pandas dataframe

df.head() # show the first five rows
272/5:
from sklearn.preprocessing import LabelEncoder

# convert str values to int using the scikit-learn encoder

st = df.apply(LabelEncoder().fit_transform)

st.head()
272/6:
sns.set(color_codes=True)
plt.figure(figsize=(14, 12))
sns.heatmap(st.astype(float).corr(), 
            linewidths=0.2, 
            square=True, 
            linecolor='white', 
            annot=True,
            cmap="YlGnBu")
plt.show()
272/7:
g = sns.FacetGrid(df, col="smoker",  size= 5, sharey=False, sharex = True)
g.map(sns.distplot, "charges", color = 'r');
g.set_axis_labels("charges", "proportion");
g.despine(left=True)
272/8:
plt.figure(figsize=(13,6))
plt.title("Distribution of age")
ax = sns.distplot(df["age"], color = 'purple')
272/9:
sns.catplot(x="smoker", kind="count", hue = 'sex', data = df , palette='pastel');
plt.show()
272/10:
sns.lmplot(x="age", y="charges", hue="smoker", data=df, palette=dict(yes="r", no="g"), size = 7);
ax.set_title('Smokers and non-smokers');
plt.show()
272/11:
df['age'] = df['age'].astype(float)
df['children'] = df['children'].astype(float)

df = pd.get_dummies(df)

df.head()
272/12:
y = df['charges']
X = df.drop(columns=['charges'])
272/13:
rom sklearn.model_selection import train_test_split


# use 10% of dataset as testing data

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=42)
272/14:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.model_selection import train_test_split
272/15:
# use 10% of dataset as testing data

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=42)
272/16:
br = BayesianRidge().fit(X_train, y_train)

br_train_pred = br.predict(X_train)
br_test_pred = br.predict(X_test)


br_train_mse = mean_squared_error(y_train, br_train_pred)
br_test_mse = mean_squared_error(y_test, br_test_pred)


print('MSE train data: {:.5}, MSE test data: {:.5}'.format(br_train_mse, br_test_mse))

print('RMSE train data: {:.5}, RMSE test data: {:.5}'.format(
    np.sqrt(np.absolute(br_train_mse)), 
    np.sqrt(np.absolute(br_train_mse))))
                                                               
print('R2 train data: {:.5}, R2 test data: {:.5}'.format(
    r2_score(y_train, br_train_pred),
    r2_score(y_test, br_test_pred)))
272/17:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import BayesianRidge
272/18:
br = BayesianRidge().fit(X_train, y_train)

br_train_pred = br.predict(X_train)
br_test_pred = br.predict(X_test)


br_train_mse = mean_squared_error(y_train, br_train_pred)
br_test_mse = mean_squared_error(y_test, br_test_pred)


print('MSE train data: {:.5}, MSE test data: {:.5}'.format(br_train_mse, br_test_mse))

print('RMSE train data: {:.5}, RMSE test data: {:.5}'.format(
    np.sqrt(np.absolute(br_train_mse)), 
    np.sqrt(np.absolute(br_train_mse))))
                                                               
print('R2 train data: {:.5}, R2 test data: {:.5}'.format(
    r2_score(y_train, br_train_pred),
    r2_score(y_test, br_test_pred)))
272/19:
br = BayesianRidge().fit(X_train, y_train)

br_train_pred = br.predict(X_train)
br_test_pred = br.predict(X_test)


br_train_mse = mean_squared_error(y_train, br_train_pred)
br_test_mse = mean_squared_error(y_test, br_test_pred)


print('MSE train data: {:.5}, MSE test data: {:.5}'.format(br_train_mse, br_test_mse))

print('RMSE train data: {:.5}, RMSE test data: {:.5}'.format(
    np.sqrt(np.absolute(br_train_mse)), 
    np.sqrt(np.absolute(br_train_mse))))
                                                               
print('R2 train data: {:.5}, R2 test data: {:.5}'.format(
    r2_score(y_train, br_train_pred),
    r2_score(y_test, br_test_pred)))
272/20:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import BayesianRidge
from sklearn.metrics import r2_score, mean_squared_error
272/21:
br = BayesianRidge().fit(X_train, y_train)

br_train_pred = br.predict(X_train)
br_test_pred = br.predict(X_test)


br_train_mse = mean_squared_error(y_train, br_train_pred)
br_test_mse = mean_squared_error(y_test, br_test_pred)


print('MSE train data: {:.5}, MSE test data: {:.5}'.format(br_train_mse, br_test_mse))

print('RMSE train data: {:.5}, RMSE test data: {:.5}'.format(
    np.sqrt(np.absolute(br_train_mse)), 
    np.sqrt(np.absolute(br_train_mse))))
                                                               
print('R2 train data: {:.5}, R2 test data: {:.5}'.format(
    r2_score(y_train, br_train_pred),
    r2_score(y_test, br_test_pred)))
272/22:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import BayesianRidge
from sklearn.metrics import r2_score, mean_squared_error
272/23:
br = BayesianRidge().fit(X_train, y_train)

br_train_pred = br.predict(X_train)
br_test_pred = br.predict(X_test)


br_train_mse = mean_squared_error(y_train, br_train_pred)
br_test_mse = mean_squared_error(y_test, br_test_pred)


print('MSE train data: {:.5}, MSE test data: {:.5}'.format(br_train_mse, br_test_mse))

print('RMSE train data: {:.5}, RMSE test data: {:.5}'.format(
    np.sqrt(np.absolute(br_train_mse)), 
    np.sqrt(np.absolute(br_train_mse))))
                                                               
print('R2 train data: {:.5}, R2 test data: {:.5}'.format(
    r2_score(y_train, br_train_pred),
    r2_score(y_test, br_test_pred)))
272/24:
br = BayesianRidge().fit(X_train, y_train)

br_train_pred = br.predict(X_train)
br_test_pred = br.predict(X_test)

print(br_train_pred)

br_train_mse = mean_squared_error(y_train, br_train_pred)
br_test_mse = mean_squared_error(y_test, br_test_pred)


print('MSE train data: {:.5}, MSE test data: {:.5}'.format(br_train_mse, br_test_mse))

print('RMSE train data: {:.5}, RMSE test data: {:.5}'.format(
    np.sqrt(np.absolute(br_train_mse)), 
    np.sqrt(np.absolute(br_train_mse))))
                                                               
print('R2 train data: {:.5}, R2 test data: {:.5}'.format(
    r2_score(y_train, br_train_pred),
    r2_score(y_test, br_test_pred)))
272/25:
br = BayesianRidge().fit(X_train, y_train)

br_train_pred = br.predict(X_train)
br_test_pred = br.predict(X_test)

br_train_mse = mean_squared_error(y_train, br_train_pred)
br_test_mse = mean_squared_error(y_test, br_test_pred)


print('MSE train data: {:.5}, MSE test data: {:.5}'.format(br_train_mse, br_test_mse))

print('RMSE train data: {:.5}, RMSE test data: {:.5}'.format(
    np.sqrt(np.absolute(br_train_mse)), 
    np.sqrt(np.absolute(br_train_mse))))
                                                               
print('R2 train data: {:.5}, R2 test data: {:.5}'.format(
    r2_score(y_train, br_train_pred),
    r2_score(y_test, br_test_pred)))
272/26:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import BayesianRidge
from sklearn.metrics import r2_score, mean_squared_error
272/27:
df = pd.read_csv('./datasets/insurance.csv') # import the CSV as a pandas dataframe

df.head() # show the first five rows
272/28:
from sklearn.preprocessing import LabelEncoder

# convert str values to int using the scikit-learn encoder

st = df.apply(LabelEncoder().fit_transform)

st.head()
272/29:
sns.set(color_codes=True)
plt.figure(figsize=(14, 12))
sns.heatmap(st.astype(float).corr(), 
            linewidths=0.2, 
            square=True, 
            linecolor='white', 
            annot=True,
            cmap="YlGnBu")
plt.show()
272/30:
g = sns.FacetGrid(df, col="smoker",  size= 5, sharey=False, sharex = True)
g.map(sns.distplot, "charges", color = 'r');
g.set_axis_labels("charges", "proportion");
g.despine(left=True)
272/31:
plt.figure(figsize=(13,6))
plt.title("Distribution of age")
ax = sns.distplot(df["age"], color = 'purple')
272/32:
sns.catplot(x="smoker", kind="count", hue = 'sex', data = df , palette='pastel');
plt.show()
272/33:
sns.lmplot(x="age", y="charges", hue="smoker", data=df, palette=dict(yes="r", no="g"), size = 7);
ax.set_title('Smokers and non-smokers');
plt.show()
272/34:
df['age'] = df['age'].astype(float)
df['children'] = df['children'].astype(float)

df = pd.get_dummies(df)

df.head()
272/35:
y = df['charges']
X = df.drop(columns=['charges'])
272/36:
# use 10% of dataset as testing data

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.1, random_state=42)
272/37:
br = BayesianRidge().fit(X_train, y_train)

br_train_pred = br.predict(X_train)
br_test_pred = br.predict(X_test)

br_train_mse = mean_squared_error(y_train, br_train_pred)
br_test_mse = mean_squared_error(y_test, br_test_pred)


print('MSE train data: {:.5}, MSE test data: {:.5}'.format(br_train_mse, br_test_mse))

print('RMSE train data: {:.5}, RMSE test data: {:.5}'.format(
    np.sqrt(np.absolute(br_train_mse)), 
    np.sqrt(np.absolute(br_train_mse))))
                                                               
print('R2 train data: {:.5}, R2 test data: {:.5}'.format(
    r2_score(y_train, br_train_pred),
    r2_score(y_test, br_test_pred)))
273/1: import pandas as pd
273/2:
# Read data from json file
df = pd.read_json(path_or_buf='file://Users/ukannika/Desktop/data_sample.txt')
df.show()
# Location of file, can be url or local folder structure
273/3:
# Read data from json file
df = pd.read_json('file://Users/ukannika/Desktop/data_sample.txt')
df.show()
# Location of file, can be url or local folder structure
273/4:
# Read data from json file
df = pd.read_json(path_or_buf='file:///Users/ukannika/Desktop/data_sample.txt')
df.show()
# Location of file, can be url or local folder structure
273/5:
# Read data from json file
df = pd.read_json(path_or_buf='file:///Users/ukannika/Desktop/data_sample.txt', orient='columns')
df.show()
# Location of file, can be url or local folder structure
273/6:
# Read data from json file
df = pd.read_json('file:///Users/ukannika/Desktop/data_sample.txt', orient='columns')
df.show()
# Location of file, can be url or local folder structure
273/7:
# Read data from json file
df = pd.read_json(path_or_buf='file:///Users/ukannika/Desktop/test.txt', orient='columns')
df.show()
# Location of file, can be url or local folder structure
273/8:
# Read data from json file
df = pd.read_json(path_or_buf='file:///Users/ukannika/Desktop/data_sample.txt', orient='columns')
df.show()
# Location of file, can be url or local folder structure
273/9:
# Read data from json file
df = pd.read_json(path_or_buf='/Users/ukannika/Desktop/data_sample.txt', orient='columns')
df.show()
# Location of file, can be url or local folder structure
273/10:
# Read data from json file
df = pd.read_json(path_or_buf='/Users/ukannika/Desktop/data_sample.txt', orient='columns')
# Location of file, can be url or local folder structure
273/11:
# Read data from json file
df = pd.read_json(path_or_buf='/Users/ukannika/Desktop/data_sample.txt')
# Location of file, can be url or local folder structure
273/12:
import pandas as pd
import json
273/13:
# Read data from json file
df = json.load('/Users/ukannika/Desktop/data_sample.txt')
# Location of file, can be url or local folder structure
273/14:
# Read data from json file
df = json.load('/Users/ukannika/Desktop/data_sample.txt')
# Location of file, can be url or local folder structure
273/15:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', orient='columns')
# Location of file, can be url or local folder structure
273/16:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='columns')
# Location of file, can be url or local folder structure
273/17:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='index')
# Location of file, can be url or local folder structure
273/18:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', typ='series')
# Location of file, can be url or local folder structure
273/19:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', typ='series')
df.head
# Location of file, can be url or local folder structure
273/20:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', typ='series')
df.head()
# Location of file, can be url or local folder structure
273/21:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', typ='series')
df.head()
# Location of file, can be url or local folder structure
273/22:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', typ='frame')
df.head()
# Location of file, can be url or local folder structure
273/23:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', orient='records')
df.head()
# Location of file, can be url or local folder structure
273/24:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='records')
df.head()
# Location of file, can be url or local folder structure
273/25:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='columns')
df.head()
# Location of file, can be url or local folder structure
273/26:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='index')
df.head()
# Location of file, can be url or local folder structure
273/27:
# Read data from json file
df = pd.read_json(r'/Users/ukannika/Desktop/test.txt')
df.head()
# Location of file, can be url or local folder structure
273/28:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='values')
df.head()
# Location of file, can be url or local folder structure
273/29:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='index')
df.head()
# Location of file, can be url or local folder structure
273/30:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='columns')
df.head()
# Location of file, can be url or local folder structure
273/31:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='index')
df.head()
# Location of file, can be url or local folder structure
273/32:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='table')
df.head()
# Location of file, can be url or local folder structure
273/33:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='columns')
df.head()
# Location of file, can be url or local folder structure
273/34:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='records')
df.head()
# Location of file, can be url or local folder structure
273/35:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='records')
df.head()
# Location of file, can be url or local folder structure
273/36:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True)
df.head()
# Location of file, can be url or local folder structure
273/37:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', lines=True)
df.head()
# Location of file, can be url or local folder structure
273/38:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', lines=True, dtype=True)
df.head()
# Location of file, can be url or local folder structure
273/39:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt',orient='records', lines=True, dtype=True)
df.head()
# Location of file, can be url or local folder structure
273/40:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', orient='records', lines=True, dtype=True, keep_default_na=False)
df.head()
# Location of file, can be url or local folder structure
273/41:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', orient='records', lines=True, dtype=True, keep_default_dates==False)
df.head()
# Location of file, can be url or local folder structure
273/42:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', orient='records', lines=True, dtype=True, keep_default_dates=False)
df.head()
# Location of file, can be url or local folder structure
273/43:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', lines=True, dtype=True, precise_float=False)
df.head()
273/44:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/data_sample.txt', lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/45:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/46:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/47:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/48:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/49:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='records' lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/50:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='records', lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/51:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient=str, lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/52:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', orient='columns', lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/53:
import pandas as pd
import json
273/54:
with open("/Users/ukannika/Desktop/test.txt", "r") as read_file:
    data = json.load(read_file)
    
dataframe = pd.DataFrame(data)
273/55:
with open("/Users/ukannika/Desktop/test.txt", "r") as read_file:
    data = json.load(read_file)
273/56:
with open("/Users/ukannika/Desktop/test.txt", "r") as read_file:
    data = json.load(read_file)

dataframe = pd.DataFrame(data)
273/57:
with open("/Users/ukannika/Desktop/test.txt", "r") as read_file:
    data = json.load(read_file)
    
dataframe = pd.DataFrame(data)
273/58:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/59:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype=True, precise_float=False, convert_dates=False)
df.head()
273/60:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype=True, precise_float=False, convert_dates=False)
273/61:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype=True)
df.head()
273/62:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype={"EventName":str, "EventDate":str})
df.head()
273/63:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype={"EventName":str, "EventDate":str})
df.head()
273/64:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt',orient = 'records', lines=True, dtype={"EventName":str, "EventDate":str})
df.head()
273/65:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt',orient = 'records', lines=True, dtype={"EventName":str, "EventDate":str})
df.head()
273/66:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt',orient = 'records', lines=True, dtype={"EventName":str, "EventDate":float})
df.head()
273/67:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype={"EventName":str, "EventDate":float})
df.head()
273/68:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype={"EventName":str, "EventDate":float})
df.head()
273/69:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype={"EventName":str, "EventDate":str})
df.head()
273/70:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype={"EventName":str, "EventDate":str})
df.head()
273/71:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype={"EventName":str, "EventDate":float})
df.head()
273/72:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype={"EventName":str, "EventDate":float})
df.head()
273/73:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.load(f)

pprint(data)
273/74:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.load(f)

print(data)
273/75:
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    data = json.load(f)

print(data)
273/76:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.load(f)

print(data)
273/77:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.load(f)

print(data)
273/78:
with open('/Users/ukannika/Desktop/test.txt', 'r') as f:
    data = json.load(f)

print(data)
273/79:
with open('/Users/ukannika/Desktop/test.txt', 'r') as f:
    data = json.load(f)

print(data)
273/80:
# Read data from json file
df = pd.read_json('/Users/ukannika/Desktop/test.txt', lines=True, dtype={"EventName":str, "EventDate":float})
df.head()
273/81:
with open('/Users/ukannika/Desktop/test.txt', 'r') as f:
    data = json.load(f)

print(data)
273/82:
with open(/Users/ukannika/Desktop/test.txt', 'r') as f:
    data = [json.loads(line) for line in f]

print(data)
273/83:
with open('/Users/ukannika/Desktop/test.txt', 'r') as f:
    data = [json.loads(line) for line in f]

print(data)
273/84:
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    data = json.load(f)

print(data)
273/85:
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    data = json.loads(f)

print(data)
275/1: import pandas as pd
275/2:
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    data = json.load(f)

print(data)
275/3:
import pandas as pd
import json
275/4:
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    data = json.load(f)

print(data)
275/5:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.load(f)

print(data)
275/6:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.dump(f)

print(data)
275/7: print(json.load(open("/Users/ukannika/Desktop/test.txt","r")))
275/8:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.dump(f)
275/9:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.load(f)
275/10:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.load(f.read())
275/11:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = json.load(f)
275/12:
with open('/Users/ukannika/Desktop/test.txt') as f:
        data = [json.loads(line) for line in f]
275/13:
with open('/Users/ukannika/Desktop/test.txt') as f:
    data = [json.loads(line) for line in f]
275/14:
with open('/Users/ukannika/Desktop/test.txt', 'r') as f:
    data = [json.loads(line) for line in f]
275/15:
data = []
with open('/Users/ukannika/Desktop/test.txt') as f:
    for line in f:
        data.append(json.loads(line))
275/16:
with open('/Users/ukannika/Desktop/test.txt', 'r') as f:
    data = [json.loads(line) for line in f]
275/17: pd.read_json(path_or_buf='/Users/ukannika/Desktop/data_sample.txt', lines=True)
275/18: pd.read_json(path_or_buf='/Users/ukannika/Desktop/data_sample.txt',orient='columns', typ='frame' lines=True)
275/19: pd.read_json(path_or_buf='/Users/ukannika/Desktop/data_sample.txt',orient='columns', typ='frame', lines=True)
275/20: pd.DataFrame.from_dict('/Users/ukannika/Desktop/data_sample.txt', orient='columns')
275/21: df = pd.DataFrame.from_dict('/Users/ukannika/Desktop/data_sample.txt', orient='columns')
275/22: df = pd.DataFrame().from_dict('/Users/ukannika/Desktop/data_sample.txt', orient='columns')
275/23:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
data
275/24:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = DataFrame(data=data)

df.head()
275/25:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)

df.head()
275/26: select * from df
275/27:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)
275/28: select * from df
275/29: select * from data
275/30:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)
275/31:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)
df.describe()
275/32:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)
df.describe(include=all)
275/33:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)
df.describe(include=include)
275/34:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)
df.describe(include='all')
275/35: df.select(UserID)
275/36: df.select(['UserID'])
275/37: df[['UserID']]
275/38: df[['UserID'], ['FirstName'], ['LastName'], ['Email'], ['DOB'], ['CreditScore'], ['AnnualIncome']]
275/39: df[['UserID','FirstName','LastName','Email','DOB','CreditScore','AnnualIncome']]
275/40:
customerDF = df[['UserID','FirstName','LastName','Email','DOB','CreditScore','AnnualIncome']]
customerDF.head()
275/41:
customerDF = df[['UserID','FirstName','LastName','Email','DOB','CreditScore','AnnualIncome']]
salesDF = df[['UserID','VehicleID','Make','Model','Year','Automatic_Transmission','EventName']]

customerDF.head()
salesDF.head()
275/42:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)
df.describe(include='all')
275/43:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)
df.info()
275/44: customerDF.head()
275/45: salesDF.head()
275/46:
data = []
with open('/Users/ukannika/Desktop/data_sample.txt') as f:
    for line in f:
        data.append(json.loads(line))
df = pd.DataFrame(data=data)
df.info()
275/47:
customerDF = df[['UserID','FirstName','LastName','Email','DOB','CreditScore','AnnualIncome']]
salesDF = df[['UserID','VehicleID','Make','Model','Year','Automatic_Transmission','EventName', 'EventDate']]
275/48: customerDF.head()
275/49: salesDF.head()
279/1:
ENERGY_LEVEL = [100, 113, 110, 85, 105, 102, 86, 63, 81, 101, 94, 106, 101, 79, 94, 90, 97]
ENERGY_DIFF = []

def find_significant_energy_increase_recursive(A):
    print(A)


for i in range(len(ENERGY_LEVEL) - 1):
    diff = ENERGY_LEVEL[i + 1] - ENERGY_LEVEL[i]
    ENERGY_DIFF.append(diff)
print(ENERGY_DIFF)
279/2:
array = [1, 2, -3, 4, 6, -2]
arrLength = array.len
279/3:
array = [1, 2, -3, 4, 6, -2]
arrLength = array.length
print(arrLength)
280/1:
print('PyDev console: using IPython 6.5.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/ukannika/work/choice/git/cis/untitled'])
280/2: runfile('/Users/ukannika/work/choice/git/cis/untitled/test.py', wdir='/Users/ukannika/work/choice/git/cis/untitled')
281/1:
print('PyDev console: using IPython 6.5.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/ukannika/work/choice/git/cis/untitled'])
281/2: runfile('/Users/ukannika/work/choice/git/cis/untitled/test.py', wdir='/Users/ukannika/work/choice/git/cis/untitled')
282/1:
print('PyDev console: using IPython 6.5.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/ukannika/work/choice/git/cis/untitled'])
282/2: runfile('/Users/ukannika/work/choice/git/cis/untitled/test.py', wdir='/Users/ukannika/work/choice/git/cis/untitled')
283/1:
print('PyDev console: using IPython 6.5.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/ukannika/work/choice/git/cis/untitled'])
283/2: runfile('/Users/ukannika/work/choice/git/cis/untitled/test.py', wdir='/Users/ukannika/work/choice/git/cis/untitled')
284/1:
print('PyDev console: using IPython 6.5.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/ukannika/work/choice/git/cis/untitled'])
284/2: runfile('/Users/ukannika/work/choice/git/cis/untitled/test.py', wdir='/Users/ukannika/work/choice/git/cis/untitled')
285/1:
print('PyDev console: using IPython 6.5.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/ukannika/work/choice/git/cis/untitled'])
285/2: runfile('/Users/ukannika/work/choice/git/cis/untitled/test.py', wdir='/Users/ukannika/work/choice/git/cis/untitled')
286/1:
print('PyDev console: using IPython 6.5.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/ukannika/work/choice/git/cis/untitled'])
286/2: runfile('/Users/ukannika/work/choice/git/cis/untitled/test.py', wdir='/Users/ukannika/work/choice/git/cis/untitled')
287/1:
print('PyDev console: using IPython 6.5.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/ukannika/work/choice/git/cis/untitled'])
287/2: runfile('/Users/ukannika/work/choice/git/cis/untitled/test.py', wdir='/Users/ukannika/work/choice/git/cis/untitled')
288/1: import pandas as pd
288/2:
housingDataset = pd.read_csv("/Users/ukannika/work/personal/machine-learning/datasets/housing.csv", header=0)
housingDataset.head()
288/3:
housingDataset = pd.read_csv("/Users/ukannika/work/personal/machine-learning/datasets/housing.csv")
housingDataset.head()
288/4: housing.info()
288/5:
housing = pd.read_csv("/Users/ukannika/work/personal/machine-learning/datasets/housing.csv")
housing.head()
288/6: housing.info()
288/7: housing["ocean_proximity"].value_counts()
288/8: housing.describe()
288/9:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
288/10: housing.hist(bins=50, figsize=(20, 15))
288/11:
housing.hist(bins=50, figsize=(20, 15))
plt.show()
288/12:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
288/13: np.random.permutation(len(data))
288/14: np.random.permutation(len(housing))
288/15:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 20%)
print(test_set_size)
288/16:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 20)
print(test_set_size)
288/17:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 0.2)
print(test_set_size)
288/18:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 0.2)
test_indeces = shuffled_indices[:test_set_size]
print(test_indeces)
288/19:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 0.2)
test_indeces = shuffled_indices[:test_set_size]
train_indeces = shuffled_indices[test_set_size:]
print(train_indeces)
288/20:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 0.2)
test_indeces = shuffled_indices[:test_set_size]
train_indeces = shuffled_indices[test_set_size:]
288/21:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 0.2)
test_indeces = shuffled_indices[:test_set_size]
train_indeces = shuffled_indices[test_set_size:]
train_indeces
288/22:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 0.2)
test_indeces = shuffled_indices[:test_set_size]
train_indeces = shuffled_indices[test_set_size:]

test = housing.iloc[test_indeces]
train = housing.iloc[train_indeces]
288/23:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 0.2)
test_indeces = shuffled_indices[:test_set_size]
train_indeces = shuffled_indices[test_set_size:]

test = housing.iloc[test_indeces]
train = housing.iloc[train_indeces]

test.head()
288/24:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 0.2)
test_indeces = shuffled_indices[:test_set_size]
train_indeces = shuffled_indices[test_set_size:]

test_set = housing.iloc[test_indeces]
train_set = housing.iloc[train_indeces]

len(train_set)
len(test_set)
288/25:
shuffled_indices = np.random.permutation(len(housing))
test_set_size = int(len(housing) * 0.2)
test_indeces = shuffled_indices[:test_set_size]
train_indeces = shuffled_indices[test_set_size:]

test_set = housing.iloc[test_indeces]
train_set = housing.iloc[train_indeces]

len(test_set)
len(train_set)
288/26: housing.plot(kind="scatter", x="longitude", y="latitude")
288/27: housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
288/28: housing.corr()
288/29:
attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))
288/30:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import pandas.plotting import scatter_matrix
288/31:
attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))
288/32:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
288/33:
attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))
288/34: housing.plot(kind="scatter", x="median_income", y="median_house_value", alpha = 0.1)
289/1:
median = housing["total_bedrooms"].median()
housing["total_bedrooms"].fillna(median, inplace=True)
289/2:
housing = pd.read_csv("/Users/ukannika/work/personal/machine-learning/datasets/housing.csv")
housing.head()
289/3:
median = housing["total_bedrooms"].median()
housing["total_bedrooms"].fillna(median, inplace=True)
289/4:
median = housing["total_bedrooms"].median()
housing["total_bedrooms"].fillna(median, inplace=True)
289/5:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
289/6:
housing = pd.read_csv("/Users/ukannika/work/personal/machine-learning/datasets/housing.csv")
housing.head()
289/7:
median = housing["total_bedrooms"].median()
housing["total_bedrooms"].fillna(median, inplace=True)
289/8:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
289/9:
encoder = CategoricalEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat.head()
289/10:
encoder = CategoricalEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat.head()
289/11:
encoder = CategoricalEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat.head()
289/12:
housing_cat = housing["ocean_proximity"]
housing_cat.head()
289/13:
housing_cat = housing["ocean_proximity"]
housing_cat
289/14:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import CategoricalEncoder
289/15:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import CategoricalEncoder
289/16:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
encoder.fit_transform(housing_cat)
289/17:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_reshaped = housing_cat.values.reshape(-1, 1)
housing_cat_1hot = encoder.fit_transform(housing_cat_reshaped)
housing_cat_1hot
289/18:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_reshaped = housing_cat.values.reshape(-1, 1)
housing_cat_1hot = encoder.fit_transform(housing_cat_reshaped)
housing_cat_1hot
289/19:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_reshaped = housing_cat.values.reshape(-1, 1)
housing_cat_reshaped
289/20:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_reshaped = housing_cat.values.reshape(-1, 1)
len(housing_cat_reshaped)
289/21:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_reshaped = housing_cat.values.reshape(-1, 1)
len(housing_cat)
289/22:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat.factorize()
289/23:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
289/24:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_encoded
289/25:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_encoded.reshape(-1, 1)
289/26:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_encoded.reshape(-1, 2)
289/27:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_encoded.reshape(1, 1)
289/28:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_encoded.reshape(-1, 1)
289/29:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
289/30:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
housing_cat_1hot
289/31:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
len(housing_cat_1hot)
289/32:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
housing_cat_1hot.describe()
289/33:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
housing_cat_1hot.info()
289/34:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
housing_cat_1hot.toArray()
289/35:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
housing_cat_1hot.toarray()
289/36:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
encoder.categorical_features
289/37:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
encoder.categorical_features.capitalize
289/38:
encoder = OneHotEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
encoder.categorical_features_
289/39:
housing_num = housing.drop("ocean_proximity", axis=1)
housing_num
289/40:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]
numerical_attributes
289/41:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
289/42:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
289/43:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
291/1:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
291/2:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

pipeline1 = Pipeline(['imputer', SimpleImputer])
291/3:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
291/4:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn_features.transformers import DataFrameSelector
291/5:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.transformers import DataFrameSelector
291/6:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import DataFrameSelector
291/7:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

pipeline = Pipeline(['imputer', SimpleImputer(strategy="median"), 
                      'categorical_encoder', OneHotEncoder()
                      'std_scalar', StandardScaler()
                     ])

housing_prepared = pipeline.fit_transform(housing)
housing_prepared
291/8:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

pipeline = Pipeline(['imputer', SimpleImputer(strategy="median"), 
                      'categorical_encoder', OneHotEncoder(),
                      'std_scalar', StandardScaler()
                     ])

housing_prepared = pipeline.fit_transform(housing)
housing_prepared
291/9:
housing = pd.read_csv("/Users/ukannika/work/personal/machine-learning/datasets/housing.csv")
housing.head()
291/10:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

pipeline = Pipeline(['imputer', SimpleImputer(strategy="median"), 
                      'categorical_encoder', OneHotEncoder(),
                      'std_scalar', StandardScaler()
                     ])

housing_prepared = pipeline.fit_transform(housing)
housing_prepared
291/11:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
291/12:
class DataFrameSelector(BaseEstimator, TransformerMixin):
    def __init__(self, attribute_names):
        self.attribute_names = attribute_names
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[self.attribute_names].values
291/13:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion
291/14:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline(['selector', DataFrameSelector(numerical_attributes),
                      'imputer', SimpleImputer(strategy="median"), 
                      'std_scalar', StandardScaler()
                     ])

cat_pipeline = Pipeline(['selector', DataFrameSelector(cat_attributes),
                         'cat_encoder', OneHotEncoder()
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared
291/15:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline(['selector', DataFrameSelector(numerical_attributes),
                      'imputer', SimpleImputer(strategy="median"), 
                      'std_scalar', StandardScaler()
                     ])

cat_pipeline = Pipeline(['selector', DataFrameSelector(cat_attributes),
                         'cat_encoder', OneHotEncoder()
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared
291/16:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline(['selector', DataFrameSelector(numerical_attributes),
                      'imputer', SimpleImputer(strategy="median"), 
                      'std_scalar', StandardScaler()
                     ])

cat_pipeline = Pipeline(['selector', DataFrameSelector(cat_attributes),
                         'cat_encoder', OneHotEncoder()
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])
291/17:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]
291/18:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline(['selector', DataFrameSelector(cat_attributes),
                         'cat_encoder', OneHotEncoder()
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared
291/19:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])
291/20:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared
291/21:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.head()
291/22:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape()
291/23:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape
291/24:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape

print(housing_prepared)
291/25:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape

print(housing_prepared, precision=3)
291/26:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape

np.set_printoptions(threshold=np.nan)
print(housing_prepared, precision=3)
291/27:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape

np.set_printoptions(threshold=np.nan)
print(housing_prepared)
291/28:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape
291/29:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', Cate())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape
291/30:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', Cate())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape
291/31:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared.shape
291/32:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
pd.DataFrame(data=housing_prepared[1:,1:])
291/33:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
pd.DataFrame(data=housing_prepared[1:,1:], index=housing_prepared[1:,0], columns=housing_prepared[0,1:])
291/34:
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared
291/35:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion
from sklearn.linear_model import LinearRegression
291/36:
linear_reg = LinearRegression()
linear_reg.fit(linear_reg)
291/37:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
291/38:
train_set, test_set = train_test_split(housing, test_size=0.2)
housing_num = housing.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared
291/39:
train_set, test_set = train_test_split(housing, test_size=0.2)

housing_num = train_set.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(train_set)
housing_prepared
291/40:
train_set, test_set = train_test_split(housing, test_size=0.2, random_state = 42)

housing_num = train_set.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(train_set)
housing_prepared
291/41:
linear_reg = LinearRegression()
numerical_attributes
291/42:
housing = pd.read_csv("/Users/ukannika/work/personal/machine-learning/datasets/housing.csv")
housing.head()
291/43:
linear_reg = LinearRegression()
housing_train = train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = train_set["median_house_value"].copy()
housing_train
291/44:
linear_reg = LinearRegression()
housing_train = train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = train_set["median_house_value"].copy()
housing_labels
291/45:
linear_reg = LinearRegression()
housing_train = train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = train_set["median_house_value"].copy()

housing_train
291/46:
linear_reg = LinearRegression()
housing_train = train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = train_set["median_house_value"].copy()

linear_reg.fit(housing_train, housing_labels)
291/47:
housing_num = train_set.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(train_set)
housing_prepared

train_set, test_set = train_test_split(housing_prepared, test_size=0.2, random_state = 42)
291/48:
linear_reg = LinearRegression()
housing_train = train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = train_set["median_house_value"].copy()

linear_reg.fit(housing_train, housing_labels)
291/49:
linear_reg = LinearRegression()
housing_train = train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = train_set["median_house_value"].copy()

linear_reg.fit(housing_train, housing_labels)
291/50:
linear_reg = LinearRegression()
linear_reg.fit(housing_prepared, housing_labels)
291/51:
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set

housing_num = housing_test.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_test_prepared = full_pipeline.fit_transform(housing_test)
linear_reg.predict(housing_test_prepared)
291/52:
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)

numerical_attributes_test = list(housing_num_test)
cat_attributes_test = ["ocean_proximity"]

num_pipeline_test = Pipeline([('selector', DataFrameSelector(numerical_attributes_test)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline_test = Pipeline([('selector', DataFrameSelector(cat_attributes_test)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline_test = FeatureUnion(transformer_list=[("num_pipeline_test", num_pipeline_test),
                                              ("cat_pipeline_test", cat_pipeline_test)])

housing_test_prepared = full_pipeline.fit_transform(housing_test)
linear_reg.predict(housing_test_prepared)
291/53:
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)

numerical_attributes_test = list(housing_num_test)
cat_attributes_test = ["ocean_proximity"]

num_pipeline_test = Pipeline([('selector', DataFrameSelector(numerical_attributes_test)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline_test = Pipeline([('selector', DataFrameSelector(cat_attributes_test)),
                         ('cat_encoder', OneHotEncoder())
])
291/54:
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)
291/55: housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
291/56: test_set
291/57: test_set.head()
291/58: test_set.head
291/59: train_set.head
291/60: train_set.head()
291/61: train_set.head()
291/62: train_set.drop("median_house_value", axis=1)
291/63:
train_set, test_set = train_test_split(housing, test_size=0.2, random_state = 42)
housing_train = train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = train_set["median_house_value"].copy()

housing_num = housing_train.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing_train)
housing_prepared
291/64:
linear_reg = LinearRegression()
linear_reg.fit(housing_prepared, housing_labels)
291/65: test_set.shape
291/66: train_set.shape
291/67: housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
291/68:
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)

numerical_attributes_test = list(housing_num_test)
cat_attributes_test = ["ocean_proximity"]

num_pipeline_test = Pipeline([('selector', DataFrameSelector(numerical_attributes_test)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline_test = Pipeline([('selector', DataFrameSelector(cat_attributes_test)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline_test = FeatureUnion(transformer_list=[("num_pipeline_test", num_pipeline_test),
                                              ("cat_pipeline_test", cat_pipeline_test)])

housing_test_prepared = full_pipeline.fit_transform(housing_test)
linear_reg.predict(housing_test_prepared)
291/69:
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)

numerical_attributes_test = list(housing_num_test)
cat_attributes_test = ["ocean_proximity"]

num_pipeline_test = Pipeline([('selector', DataFrameSelector(numerical_attributes_test)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline_test = Pipeline([('selector', DataFrameSelector(cat_attributes_test)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline_test = FeatureUnion(transformer_list=[("num_pipeline_test", num_pipeline_test),
                                              ("cat_pipeline_test", cat_pipeline_test)])

housing_test_prepared = full_pipeline.fit_transform(housing_test)
housing_predictions = linear_reg.predict(housing_test_prepared)
housing_predictions
291/70:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
291/71: mean_squared_error(housing_labels, housing_predictions)
291/72: mean_squared_error(housing_labels_test, housing_predictions)
291/73: mean_squared_error(housing_labels_test, housing_predictions)
291/74:
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)
housing_labels_test = housing_test["median_house_value"].copy()

numerical_attributes_test = list(housing_num_test)
cat_attributes_test = ["ocean_proximity"]

num_pipeline_test = Pipeline([('selector', DataFrameSelector(numerical_attributes_test)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline_test = Pipeline([('selector', DataFrameSelector(cat_attributes_test)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline_test = FeatureUnion(transformer_list=[("num_pipeline_test", num_pipeline_test),
                                              ("cat_pipeline_test", cat_pipeline_test)])

housing_test_prepared = full_pipeline.fit_transform(housing_test)
housing_predictions = linear_reg.predict(housing_test_prepared)
housing_predictions
291/75:
train_set, test_set = train_test_split(housing, test_size=0.2, random_state = 42)
housing_train = train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = train_set["median_house_value"].copy()

housing_num = housing_train.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing_train)
housing_prepared
291/76:
linear_reg = LinearRegression()
linear_reg.fit(housing_prepared, housing_labels)
291/77:
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)
housing_labels_test = housing_test["median_house_value"].copy()

numerical_attributes_test = list(housing_num_test)
cat_attributes_test = ["ocean_proximity"]

num_pipeline_test = Pipeline([('selector', DataFrameSelector(numerical_attributes_test)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline_test = Pipeline([('selector', DataFrameSelector(cat_attributes_test)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline_test = FeatureUnion(transformer_list=[("num_pipeline_test", num_pipeline_test),
                                              ("cat_pipeline_test", cat_pipeline_test)])

housing_test_prepared = full_pipeline.fit_transform(housing_test)
housing_predictions = linear_reg.predict(housing_test_prepared)
housing_predictions
291/78:
housing_labels_test = housing_test["median_house_value"].copy()
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)

numerical_attributes_test = list(housing_num_test)
cat_attributes_test = ["ocean_proximity"]

num_pipeline_test = Pipeline([('selector', DataFrameSelector(numerical_attributes_test)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline_test = Pipeline([('selector', DataFrameSelector(cat_attributes_test)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline_test = FeatureUnion(transformer_list=[("num_pipeline_test", num_pipeline_test),
                                              ("cat_pipeline_test", cat_pipeline_test)])

housing_test_prepared = full_pipeline.fit_transform(housing_test)
housing_predictions = linear_reg.predict(housing_test_prepared)
housing_predictions
291/79:
housing_labels_test = test_set["median_house_value"].copy()
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)

numerical_attributes_test = list(housing_num_test)
cat_attributes_test = ["ocean_proximity"]

num_pipeline_test = Pipeline([('selector', DataFrameSelector(numerical_attributes_test)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline_test = Pipeline([('selector', DataFrameSelector(cat_attributes_test)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline_test = FeatureUnion(transformer_list=[("num_pipeline_test", num_pipeline_test),
                                              ("cat_pipeline_test", cat_pipeline_test)])

housing_test_prepared = full_pipeline.fit_transform(housing_test)
housing_predictions = linear_reg.predict(housing_test_prepared)
housing_predictions
291/80: mean_squared_error(housing_labels_test, housing_predictions)
291/81:
mse = mean_squared_error(housing_labels_test, housing_predictions)
np.sqrt(mse)
291/82:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
291/83:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
291/84:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso()
291/85:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(housing_prepared, housing_labels)
291/86:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = lasso_reg.predict(housing_test_prepared)
291/87:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = lasso_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/88:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso(alpha=1.1)
lasso_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = lasso_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/89:
ridge_reg = Ridge(alpha=0.5)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/90:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso(alpha=100)
lasso_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = lasso_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/91:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso(alpha=1000)
lasso_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = lasso_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/92:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso(alpha=10000)
lasso_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = lasso_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/93:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = lasso_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/94:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
291/95:
housing = pd.read_csv("/Users/ukannika/work/personal/machine-learning/datasets/housing.csv")
housing.head()
291/96:
class DataFrameSelector(BaseEstimator, TransformerMixin):
    def __init__(self, attribute_names):
        self.attribute_names = attribute_names
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X[self.attribute_names].values
291/97:
train_set, test_set = train_test_split(housing, test_size=0.2, random_state = 42)
housing_train = train_set.drop("median_house_value", axis=1) # drop labels for training set
housing_labels = train_set["median_house_value"].copy()

housing_num = housing_train.drop("ocean_proximity", axis=1)
numerical_attributes = list(housing_num)
cat_attributes = ["ocean_proximity"]

num_pipeline = Pipeline([('selector', DataFrameSelector(numerical_attributes)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attributes)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline = FeatureUnion(transformer_list=[("num_pipeline", num_pipeline),
                                              ("cat_pipeline", cat_pipeline)])

housing_prepared = full_pipeline.fit_transform(housing_train)
housing_prepared
291/98:
linear_reg = LinearRegression()
linear_reg.fit(housing_prepared, housing_labels)
291/99:
housing_labels_test = test_set["median_house_value"].copy()
housing_test = test_set.drop("median_house_value", axis=1) # drop labels for training set
housing_num_test = housing_test.drop("ocean_proximity", axis=1)

numerical_attributes_test = list(housing_num_test)
cat_attributes_test = ["ocean_proximity"]

num_pipeline_test = Pipeline([('selector', DataFrameSelector(numerical_attributes_test)),
                      ('imputer', SimpleImputer(strategy="median")), 
                      ('std_scalar', StandardScaler())
                     ])

cat_pipeline_test = Pipeline([('selector', DataFrameSelector(cat_attributes_test)),
                         ('cat_encoder', OneHotEncoder())
])

full_pipeline_test = FeatureUnion(transformer_list=[("num_pipeline_test", num_pipeline_test),
                                              ("cat_pipeline_test", cat_pipeline_test)])

housing_test_prepared = full_pipeline.fit_transform(housing_test)
housing_predictions = linear_reg.predict(housing_test_prepared)
housing_predictions
291/100:
mse = mean_squared_error(housing_labels_test, housing_predictions)
np.sqrt(mse)
291/101:
# Try with Lasso Regression(L1 Regularization)
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = lasso_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/102:
ridge_reg = Ridge(alpha=0.5)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/103: ridge_reg.coef_
291/104:
ridge_reg = Ridge(alpha=100)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/105: ridge_reg.coef_
291/106:
ridge_reg = Ridge(alpha=10000)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/107: ridge_reg.coef_
291/108:
ridge_reg = Ridge(alpha=0.0)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/109: linear_reg.coef_
291/110: ridge_reg.coef_
291/111:
ridge_reg = Ridge(alpha=0.2)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/112:
ridge_reg = Ridge(alpha=0.5)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/113:
ridge_reg = Ridge(alpha=100)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/114: ridge_reg.coef_
291/115:
ridge_reg = Ridge(alpha=10000)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/116:
ridge_reg = Ridge(alpha=10000)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/117:
ridge_reg = Ridge(alpha=0.1)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
291/118: ridge_reg.coef_
291/119:
ridge_reg = Ridge(alpha=0.1)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
ridge_reg.coef_
291/120:
ridge_reg = Ridge(alpha=1)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
ridge_reg.coef_
291/121:
ridge_reg = Ridge(alpha=10)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
ridge_reg.coef_
291/122:
ridge_reg = Ridge(alpha=0)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
ridge_reg.coef_
291/123:
ridge_reg = Ridge(alpha=1000)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
ridge_reg.coef_
291/124:
ridge_reg = Ridge(alpha=1000)
ridge_reg.fit(housing_prepared, housing_labels)
housing_predictions_ridge = ridge_reg.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_ridge)
np.sqrt(mse_ridge)
ridge_reg.coef_
291/125:
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import FeatureUnion
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import BayesianRidge
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
291/126:
bayesian = BayesianRidge()
bayesian.fit(housing_prepared, housing_labels)
housing_predictions_bayesian = bayesian.predict(housing_test_prepared)
mse_ridge = mean_squared_error(housing_labels_test, housing_predictions_bayesian)
np.sqrt(mse_ridge)
291/127:
bayesian = BayesianRidge()
bayesian.fit(housing_prepared, housing_labels)
housing_predictions_bayesian = bayesian.predict(housing_test_prepared)
mse_bayesian = mean_squared_error(housing_labels_test, housing_predictions_bayesian)
np.sqrt(mse_bayesian)
291/128:
bayesian = BayesianRidge()
bayesian.fit(housing_prepared, housing_labels)
291/129:
bayesian = BayesianRidge()
bayesian.fit(housing_prepared.toarray, housing_labels)
291/130:
bayesian = BayesianRidge()
bayesian.fit(housing_prepared.toarray(), housing_labels)
291/131:
bayesian = BayesianRidge()
bayesian.fit(housing_prepared.toarray(), housing_labels)
291/132:
bayesian = BayesianRidge()
bayesian.fit(housing_prepared.toarray(), housing_labels)
housing_predictions_bayesian = bayesian.predict(housing_test_prepared)
mse_bayesian = mean_squared_error(housing_labels_test, housing_predictions_bayesian)
np.sqrt(mse_bayesian)
291/133:
bayesian = BayesianRidge()
bayesian.fit(housing_prepared.toarray(), housing_labels)
housing_predictions_bayesian = bayesian.predict(housing_test_prepared)
mse_bayesian = mean_squared_error(housing_labels_test, housing_predictions_bayesian)
bayesian.coef_
292/1:
%matplotlib inline
from sklearn.datasets import fetch_mldata
# Change data_home to wherever to where you want to download your data
mnist = fetch_mldata('MNIST original')
292/2:
%matplotlib inline
from sklearn.datasets import fetch_mldata
292/3: datasets.load_wine(True)
292/4:
%matplotlib inline
import sklearn.datasets as datasets
292/5: datasets.load_wine(True)
292/6: datasets.load_wine
292/7: datasets.load_wine(True)
292/8:
data = datasets.load_wine(True)
print(data)
292/9:
data = datasets.load_wine(True)
print(data.__class__)
292/10:
data = datasets.load_wine(True)
X = data[0]
292/11:
data = datasets.load_wine(True)
X = data[0]
print(X)
292/12:
data = datasets.load_wine(True)
X = data[0]
print(X.size)
292/13:
data = datasets.load_wine(True)
X = data[0]
print(X.__class__)
292/14:
data = datasets.load_wine(True)
X = data[0]
Y = data[1]
292/15: print (Y)
292/16: print (Y.size)
292/17: print (Y.shape)
292/18: print (X.shape)
292/19: print (Y.shape)
292/20: print (Y.shape)
292/21: list(data.target_names)
292/22:
data.target[[10, 80, 140]]
list(data.target_names)
292/23: data = datasets.load_wine(True)
292/24:
data.target[[10, 80, 140]]
list(data.target_names)
292/25:
X.target[[10, 80, 140]]
list(X.target_names)
292/26:
data = datasets.load_wine(True)
X = data[0]
Y = data[1]
292/27:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.linear_model import LogisticRegression
292/28:
data = datasets.load_wine(True)
X = data[0]
Y = data[1]
292/29: print(wine.data)
292/30: wine = datasets.load_wine(True)
292/31: print(wine.data)
292/32: print(wine.data)
292/33: print(wine.data)
292/34: wine = datasets.load_iris()
292/35: print(wine.data)
292/36: wine = datasets.load_wine()
292/37: print(wine.data)
292/38: print(wine.target)
292/39: print(wine.target_names)
292/40: train_test_split(wine)
292/41:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
292/42: train_test_split(wine)
292/43: x_train, y_train = train_test_split(wine)
292/44: wine = datasets.load_wine(True)
292/45: x_train, y_train = train_test_split(wine)
292/46: x_train, x_target, y_train, y_label = train_test_split(wine)
292/47: X, Y = train_test_split(wine)
292/48: print(x_train)
292/49:
X, Y = train_test_split(wine)
x_train = X[0]
x_label = X[1]
292/50: print(x_train)
292/51:
X, Y = train_test_split(wine)
x_train = X[0]
x_label = X[1]
292/52: X, Y = train_test_split(wine)
292/53: print(X)
292/54:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn import datasets

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
Y = iris.target

logreg = LogisticRegression(C=1e5)

# Create an instance of Logistic Regression Classifier and fit the data.
logreg.fit(X, Y)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
h = .02  # step size in the mesh
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()
294/1: import numpy as np
294/2:
# 1D
np.array[(1, 2, 3)]
294/3:
# 1D
np.array([1, 2, 3])
294/4:
# 1D
np.array([1, 2, 3])
print(a)
294/5:
# 1D
a = np.array([1, 2, 3])
print(a)
294/6:
# 1D
array1D = np.array([1, 2, 3])
print(a)
294/7:
# 1D
array1D = np.array([1, 2, 3])
print(array1D)
294/8:
# 2D
array2D = np.array([[1, 2, 3], [2, 3, 5]])
print(array2D)
294/9:
# Get number of dimenstions
array2D.ndim
294/10:
# Get number of dimenstions
array2D.ndim
array1D.shape
294/11:
# Get number of dimenstions
array2D.ndim
array1D.shape
array2D.shape
294/12:
# 1D
array1D = np.array([1, 2, 3], dtype="int16")
print(array1D)
294/13:
# 2D
array2D = np.array([[1, 2, 3], [2, 3, 5]], dtype="int64")
print(array2D)
294/14:
# 2D
array2D = np.array([[1, 2, 3], [2, 3, 5]], dtype="int748384")
print(array2D)
294/15:
# 2D
array2D = np.array([[1, 2, 3], [2, 3, 5]], dtype="int32")
print(array2D)
294/16:
# Get a specific row
array2D[0, :]
294/17:
# Get a specific row
array2D[1, :]
294/18: import matplotlib.pyplot as plt
294/19: plt.plot([1,2,3], [4,5,6])
294/20: plt.plot([1,2,3], [4,1,6])
292/55: wine = datasets.load_wine()
292/56: wine.data
292/57: x_train = wine.data
292/58: wine.feature_names
292/59:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbgfs", C=10)
softmax_reg.fit(wine.data, wine.target)
292/60:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10)
softmax_reg.fit(wine.data, wine.target)
292/61:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 1000)
softmax_reg.fit(wine.data, wine.target)
292/62:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 100000)
softmax_reg.fit(wine.data, wine.target)
292/63:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 10000)
softmax_reg.fit(wine.data, wine.target)
292/64:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 100000)
softmax_reg.fit(wine.data, wine.target)
292/65: softmax_reg.coef_
292/66:
wine = datasets.load_wine(True)
X,Y = train_test_split(wine)
print(X)
292/67:
wine = datasets.load_wine(True)
X,Y = train_test_split(wine)
print(X[0])
292/68:
wine = datasets.load_wine(True)
X,Y = train_test_split(wine)
print(X[1])
292/69:
wine = datasets.load_wine(True)
wine
292/70:
wine = datasets.load_wine()
x
292/71:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

X[0, :]
292/72:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.set_printoptions(True)
X[0, :]
292/73:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.set_printoptions(True)
(X[0, :], Y[0,]
292/74:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.set_printoptions(True)
(X[0, :], Y[0,:]
292/75:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.set_printoptions(True)
(X[0, :], Y[0]
292/76:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.set_printoptions(True)
(X[0, :], Y[0])
292/77:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.set_printoptions(True)
(X[10, :], Y[0])
292/78:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.set_printoptions(True)
(X[10, :], Y[10])
292/79:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.set_printoptions(True)
(X[110, :], Y[110])
292/80:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.append(X, Y)
292/81:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

data = np.append(X, Y)

data[110, :]
292/82:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

data = np.append(X, Y)
print(data)
292/83:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

data = np.append(X, Y, 1)
print(data)
292/84:
wine = datasets.load_wine(True)
print(wine)
292/85:
wine = datasets.load_wine(True)
wine.count
292/86:
wine = datasets.load_wine(True)
wine.index
292/87:
wine = datasets.load_wine(True)
wine.size
292/88:
wine = datasets.load_wine(True)
wine.count
292/89:
wine = datasets.load_wine(True)
wine.count
292/90:
wine = datasets.load_wine(True)
wine.count
292/91:
wine = datasets.load_wine(True)
wine.count
292/92:
wine = datasets.load_wine(True)
wine.count
292/93:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
292/94:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 100000)
softmax_reg.fit(wine.data, wine.target)

softmax_reg.predict_proba
292/95:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 100000)
softmax_reg.fit(wine.data, wine.target)

softmax_reg.predict_proba.__class__
292/96:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 100000)
softmax_reg.fit(wine.data, wine.target)

softmax_reg.predict_proba()
292/97:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

X.__class__
292/98:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

X.shape
292/99:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import numpy as np
292/100:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

X.shape

np.concatenate(X, Y).shape
292/101:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

X.shape

np.concatenate(X, Y)
292/102:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

X[:10, :].shape
292/103:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

X.shape
292/104:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:170, :]
y_train = X[170:, :]

y_train.shape
292/105:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:170, :]
x_label = Y[:170, :]
y_train = X[170:, :]
Y_label = Y[170:, :]

y_label
292/106:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:170, :]
x_label = Y[:170, :]
y_train = X[170:, :]
Y_label = Y[170:, :]
292/107:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:170, :]
y_train = X[170:, :]


Y_label = Y[170:, :]
292/108:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:170, :]
y_train = X[170:, :]


Y.shape
292/109:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:170, :]
x_label = Y[:170,]
y_train = X[170:, :]
Y_label = Y[170:,]

y_label
292/110:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:170, :]
x_label = Y[:170,]
y_train = X[170:, :]
y_label = Y[170:,]

y_label
292/111:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.take(X, [1, 2, 3])
292/112:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.take(X, [1, 2, 3]).shape
292/113:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.take(X, [1, 2, 3], :).shape
292/114:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:165, :]
x_label = Y[:165,]
y_train = X[165:, :]
y_label = Y[165:,]

y_label
292/115:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:160, :]
x_label = Y[:160,]
y_train = X[160:, :]
y_label = Y[160:,]

y_label
292/116:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

x_train = X[:100, :]
x_label = Y[:100,]
y_train = X[100:, :]
y_label = Y[100:,]

y_label
292/117:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.take(X, [[0, 1, 3], [2, 3]])
292/118:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.take(X, 1)
292/119:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.take(X, 1).shape
292/120:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.take(X, 1).shape
292/121:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.take(X, 1:, :).shape
292/122:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.column_stack(X, Y)
292/123:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.column_stack(X)
292/124:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X[:,:-1] = Y
292/125:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X[:,:14] = Y
292/126:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.append(X, Y, axis=1)
292/127:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack(X, Y)
292/128:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack((X, Y))
292/129:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.concatenate((X, Y), axis=0)
292/130:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.concatenate((X, Y), axis=1)
292/131:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.append(X, Y, axis=1)
292/132:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack((X,Y))
292/133:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack((X,Y[:,]))
292/134:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack((X,Y[:,0]))
292/135:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
Y[:,0]
292/136:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
Y[:]
292/137:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack((X,Y[:]))
292/138:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.append((X,Y[:]))
292/139:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.append(X,Y[:])
292/140:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.append(X,Y[:]).shape
292/141:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.append(X,Y[:], axis=1).shape
292/142:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack(X,Y[:]).shape
292/143:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack([X,Y[:]]).shape
292/144:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X.shape
292/145:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X.shape
y[:].shape
292/146:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X.shape
Y[:].shape
292/147:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X.shape
Y[:, :].shape
292/148:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.hstack((X, Y))
292/149:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.hstack([X, Y])
292/150:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

a = np.array([1, 2], [2, 3])
b = np.array([10], [20])

np.hstack([a, b])
292/151:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

a = np.array([1, 2], [2, 3], dtype="int32")
b = np.array([10], [20], dtype="int32")

np.hstack([a, b])
292/152:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

a = np.array([[1, 2], [2, 3]])
b = np.array([[10], [20]])
292/153:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

a = np.array([[1, 2], [2, 3]])
b = np.array([[10], [20]])

np.hstack(a, b)
292/154:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

a = np.array([[1, 2], [2, 3]])
b = np.array([[10], [20]])

np.hstack([a, b])
292/155:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

a = np.array([[1, 2], [2, 3]])
b = np.array([[10], [20]])

a.shape
292/156:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

a = np.array([[1, 2], [2, 3]])
b = np.array([[10], [20]])

b.shape
292/157:
wine = datasets.load_wine()
X = wine.data
Y = wine.target.reshape(:, 1)

Y
292/158:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.__class__
292/159:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.shape
292/160:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y
292/161:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.flatten.shape
292/162:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.reshape(Y, :, 1)
292/163:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

np.reshape(Y, 178, 1)
292/164:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

y = np.reshape(Y, 178, 1)
y.shape
292/165:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y
292/166:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.reshape(Y, (178, 1))
292/167:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.reshape(Y, (1, 0))
292/168:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.reshape(Y, (1, :))
292/169:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.reshape(Y, (1, ))
292/170:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.reshape(Y, (1))
292/171:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.reshape(Y, 1)
292/172:
wine = datasets.load_wine()
X = wine.data
Y = wine.target

Y.reshape(178, 1)
292/173:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack([X, Y.reshape(178, 1)])
292/174:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
np.hstack([X, Y.reshape(178, 1)])[0]
292/175:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
data = np.hstack([X, Y.reshape(178, 1)])

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

Y_test
292/176:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 100000)
softmax_reg.fit(X_train, Y_train)

softmax_reg.predict_proba()
292/177:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 100000)
softmax_reg.fit(X_train, Y_train)
292/178: softmax_reg.predict(X_test)
292/179:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
data = np.hstack([X, Y.reshape(178, 1)])
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.35, random_state=42)
292/180:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 100000)
softmax_reg.fit(X_train, Y_train)
292/181: softmax_reg.predict(X_test)
292/182:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
data = np.hstack([X, Y.reshape(178, 1)])
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.35, random_state=42)
Y_test
292/183:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 100000)
softmax_reg.fit(X_train, Y_train)
292/184: softmax_reg.predict(X_test)
292/185: softmax_reg.predict(X_test)
292/186: softmax_reg.score(X_test)
292/187: softmax_reg.score(X_test, Y_test)
292/188:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 1000)
softmax_reg.fit(X_train, Y_train)
292/189: softmax_reg.score(X_test, Y_test)
292/190: softmax_reg.score(X_test, Y_test)
292/191: softmax_reg.score(X_test, Y_test)
292/192:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
data = np.hstack([X, Y.reshape(178, 1)])
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.85, random_state=42)
Y_test
292/193:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 1000)
softmax_reg.fit(X_train, Y_train)
292/194: softmax_reg.score(X_test, Y_test)
292/195:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
data = np.hstack([X, Y.reshape(178, 1)])
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)
292/196:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 1000)
softmax_reg.fit(X_train, Y_train)
292/197: softmax_reg.score(X_test, Y_test)
292/198: softmax_reg.predict_proba(X_test, Y_test)
292/199: softmax_reg.predict_proba(X_test)
292/200:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)
292/201:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 1000)
softmax_reg.fit(X_train, Y_train)
292/202: softmax_reg.predict_proba(X_test)
292/203:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)
292/204:
softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10, max_iter = 1000)
softmax_reg.fit(X_train, Y_train)
292/205: softmax_reg.predict_proba(X_test)
292/206:
pd.set_option('display.float_format', lambda x: '%.3f' % x)
softmax_reg.predict_proba(X_test)
292/207: softmax_reg.predict_proba(X_test)
292/208:
np.set_printoptions(suppress=True)
softmax_reg.predict_proba(X_test)
295/1:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.model_selection import train_test_split
import numpy as np
295/2:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn import Linea
from sklearn.model_selection import train_test_split
import numpy as np
295/3:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split
import numpy as np
295/4:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
295/5:
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, Y_train)
295/6:
lda = LinearDiscriminantAnalysis(solver="eigen")
lda.fit(X_train, Y_train)
295/7:
lda = LinearDiscriminantAnalysis(solver="eigen", store_covariance=True)
lda.fit(X_train, Y_train)
295/8: lda.score(Y_train, Y_test)
295/9: lda.score(Y_train)
295/10: lda.score(Y_train, Y_train.reshape(178, 1))
295/11: lda.score(Y_train, Y_train.reshape(:, 1))
295/12: lda.score(X_train, Y_train)
295/13:
lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
lda.fit(X_train, Y_train)
295/14: lda.score(X_train, Y_train)
295/15:
iris = datasets.load_iris()
X = iris.data
Y = iris.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
295/16:
lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
lda.fit(X_train, Y_train)
295/17: lda.score(X_train, Y_train)
295/18: lda.decision_function
295/19:
iris = datasets.load_breast_cancer()
X = iris.data
Y = iris.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
295/20:
lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
lda.fit(X_train, Y_train)
295/21: lda.decision_function
295/22: lda.score(X_train, Y_train)
292/209:
breast_cancer = datasets.load_breast_cancer()
X = breast_cancer.data
Y = breast_cancer.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
lda.fit(X_train, Y_train)
lda.score(X_train, Y_train)
292/210:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
292/211:
breast_cancer = datasets.load_breast_cancer()
X = breast_cancer.data
Y = breast_cancer.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
lda.fit(X_train, Y_train)
lda.score(X_train, Y_train)
295/23:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
295/24:
lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
lda.fit(X_train, Y_train)
295/25: lda.score(X_train, Y_train)
295/26:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.model_selection import train_test_split
import numpy as np
295/27:
lda = QuadraticDiscriminantAnalysis(solver="svd", store_covariance=True)
lda.fit(X_train, Y_train)
295/28:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
295/29:
lda = QuadraticDiscriminantAnalysis()
lda.fit(X_train, Y_train)
295/30: lda.score(X_train, Y_train)
295/31:
breast_cancer = datasets.load_breast_cancer()
X = breast_cancer.data
Y = breast_cancer.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
295/32:
lda = QuadraticDiscriminantAnalysis()
lda.fit(X_train, Y_train)
295/33: lda.score(X_train, Y_train)
295/34:
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, Y_train)
295/35: lda.score(X_train, Y_train)
295/36:
def plot_ellipse(splot, mean, cov, color):
    v, w = linalg.eigh(cov)
    u = w[0] / linalg.norm(w[0])
    angle = np.arctan(u[1] / u[0])
    angle = 180 * angle / np.pi  # convert to degrees
    # filled Gaussian at 2 standard deviation
    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,
                              180 + angle, facecolor=color,
                              edgecolor='black', linewidth=2)
    ell.set_clip_box(splot.bbox)
    ell.set_alpha(0.2)
    splot.add_artist(ell)
    splot.set_xticks(())
    splot.set_yticks(())


    
def plot_lda_cov(lda, splot):
    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')
    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')


def plot_qda_cov(qda, splot):
    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')
    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')


plt.figure(figsize=(10, 8), facecolor='white')
plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis',
             y=0.98, fontsize=15)
for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):
    # Linear Discriminant Analysis
    lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
    y_pred = lda.fit(X, y).predict(X)
    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)
    plot_lda_cov(lda, splot)
    plt.axis('tight')

    # Quadratic Discriminant Analysis
    qda = QuadraticDiscriminantAnalysis(store_covariance=True)
    y_pred = qda.fit(X, y).predict(X)
    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)
    plot_qda_cov(qda, splot)
    plt.axis('tight')
plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()
295/37:

lda.store_covariance.
295/38:

lda.store_covariance
295/39:
lda = LinearDiscriminantAnalysis(store_covariance=True)
lda.fit(X_train, Y_train)
295/40:

lda.store_covariance
295/41:

lda.covariance_
295/42:
y_pred = lda.fit(X_train, Y_train).predict(X_test)
splot = plot_data(lda, X_train, Y_train, y_pred, fig_index=2 * i + 1)
295/43:
def plot_ellipse(splot, mean, cov, color):
    v, w = linalg.eigh(cov)
    u = w[0] / linalg.norm(w[0])
    angle = np.arctan(u[1] / u[0])
    angle = 180 * angle / np.pi  # convert to degrees
    # filled Gaussian at 2 standard deviation
    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,
                              180 + angle, facecolor=color,
                              edgecolor='black', linewidth=2)
    ell.set_clip_box(splot.bbox)
    ell.set_alpha(0.2)
    splot.add_artist(ell)
    splot.set_xticks(())
    splot.set_yticks(())


    
def plot_lda_cov(lda, splot):
    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')
    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')


def plot_qda_cov(qda, splot):
    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')
    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')


plt.figure(figsize=(10, 8), facecolor='white')
plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis',
             y=0.98, fontsize=15)
for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):
    # Linear Discriminant Analysis
    lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
    y_pred = lda.fit(X, y).predict(X)
    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)
    plot_lda_cov(lda, splot)
    plt.axis('tight')

    # Quadratic Discriminant Analysis
    qda = QuadraticDiscriminantAnalysis(store_covariance=True)
    y_pred = qda.fit(X, y).predict(X)
    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)
    plot_qda_cov(qda, splot)
    plt.axis('tight')
plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()
295/44:
def plot_ellipse(splot, mean, cov, color):
    v, w = linalg.eigh(cov)
    u = w[0] / linalg.norm(w[0])
    angle = np.arctan(u[1] / u[0])
    angle = 180 * angle / np.pi  # convert to degrees
    # filled Gaussian at 2 standard deviation
    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,
                              180 + angle, facecolor=color,
                              edgecolor='black', linewidth=2)
    ell.set_clip_box(splot.bbox)
    ell.set_alpha(0.2)
    splot.add_artist(ell)
    splot.set_xticks(())
    splot.set_yticks(())


    
def plot_lda_cov(lda, splot):
    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')
    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')


def plot_qda_cov(qda, splot):
    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')
    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')

# #############################################################################
# Plot functions
def plot_data(lda, X, y, y_pred, fig_index):
    splot = plt.subplot(2, 2, fig_index)
    if fig_index == 1:
        plt.title('Linear Discriminant Analysis')
        plt.ylabel('Data with\n fixed covariance')
    elif fig_index == 2:
        plt.title('Quadratic Discriminant Analysis')
    elif fig_index == 3:
        plt.ylabel('Data with\n varying covariances')

    tp = (y == y_pred)  # True Positive
    tp0, tp1 = tp[y == 0], tp[y == 1]
    X0, X1 = X[y == 0], X[y == 1]
    X0_tp, X0_fp = X0[tp0], X0[~tp0]
    X1_tp, X1_fp = X1[tp1], X1[~tp1]

    # class 0: dots
    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')
    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x',
                s=20, color='#990000')  # dark red

    # class 1: dots
    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')
    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x',
                s=20, color='#000099')  # dark blue

    # class 0 and 1 : areas
    nx, ny = 200, 100
    x_min, x_max = plt.xlim()
    y_min, y_max = plt.ylim()
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),
                         np.linspace(y_min, y_max, ny))
    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])
    Z = Z[:, 1].reshape(xx.shape)
    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',
                   norm=colors.Normalize(0., 1.), zorder=0)
    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='white')

    # means
    plt.plot(lda.means_[0][0], lda.means_[0][1],
             '*', color='yellow', markersize=15, markeredgecolor='grey')
    plt.plot(lda.means_[1][0], lda.means_[1][1],
             '*', color='yellow', markersize=15, markeredgecolor='grey')

    return splot
295/45:
y_pred = lda.fit(X_train, Y_train).predict(X_test)
splot = plot_data(lda, X_train, Y_train, y_pred, fig_index=2 * i + 1)
295/46:
y_pred = lda.fit(X_train, Y_train).predict(X_test)
splot = plot_data(lda, X_train, Y_train, y_pred, fig_index=2 * 1 + 1)
295/47:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.model_selection import train_test_split
import numpy as np
from matplotlib import pyplot as plt
295/48:
y_pred = lda.fit(X_train, Y_train).predict(X_test)
splot = plot_data(lda, X_train, Y_train, y_pred, fig_index=2 * 1 + 1)
295/49:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.model_selection import train_test_split
import numpy as np
from matplotlib import pyplot as plt
295/50:
breast_cancer = datasets.load_breast_cancer()
X = breast_cancer.data
Y = breast_cancer.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
295/51:
from scipy import linalg
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import colors

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

# #############################################################################
# Colormap
cmap = colors.LinearSegmentedColormap(
    'red_blue_classes',
    {'red': [(0, 1, 1), (1, 0.7, 0.7)],
     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],
     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})
plt.cm.register_cmap(cmap=cmap)


# #############################################################################
# Generate datasets
def dataset_fixed_cov():
    '''Generate 2 Gaussians samples with the same covariance matrix'''
    n, dim = 300, 2
    np.random.seed(0)
    C = np.array([[0., -0.23], [0.83, .23]])
    X = np.r_[np.dot(np.random.randn(n, dim), C),
              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]
    y = np.hstack((np.zeros(n), np.ones(n)))
    return X, y


def dataset_cov():
    '''Generate 2 Gaussians samples with different covariance matrices'''
    n, dim = 300, 2
    np.random.seed(0)
    C = np.array([[0., -1.], [2.5, .7]]) * 2.
    X = np.r_[np.dot(np.random.randn(n, dim), C),
              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]
    y = np.hstack((np.zeros(n), np.ones(n)))
    return X, y


# #############################################################################
# Plot functions
def plot_data(lda, X, y, y_pred, fig_index):
    splot = plt.subplot(2, 2, fig_index)
    if fig_index == 1:
        plt.title('Linear Discriminant Analysis')
        plt.ylabel('Data with\n fixed covariance')
    elif fig_index == 2:
        plt.title('Quadratic Discriminant Analysis')
    elif fig_index == 3:
        plt.ylabel('Data with\n varying covariances')

    tp = (y == y_pred)  # True Positive
    tp0, tp1 = tp[y == 0], tp[y == 1]
    X0, X1 = X[y == 0], X[y == 1]
    X0_tp, X0_fp = X0[tp0], X0[~tp0]
    X1_tp, X1_fp = X1[tp1], X1[~tp1]

    # class 0: dots
    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')
    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x',
                s=20, color='#990000')  # dark red

    # class 1: dots
    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')
    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x',
                s=20, color='#000099')  # dark blue

    # class 0 and 1 : areas
    nx, ny = 200, 100
    x_min, x_max = plt.xlim()
    y_min, y_max = plt.ylim()
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),
                         np.linspace(y_min, y_max, ny))
    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])
    Z = Z[:, 1].reshape(xx.shape)
    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',
                   norm=colors.Normalize(0., 1.), zorder=0)
    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='white')

    # means
    plt.plot(lda.means_[0][0], lda.means_[0][1],
             '*', color='yellow', markersize=15, markeredgecolor='grey')
    plt.plot(lda.means_[1][0], lda.means_[1][1],
             '*', color='yellow', markersize=15, markeredgecolor='grey')

    return splot


def plot_ellipse(splot, mean, cov, color):
    v, w = linalg.eigh(cov)
    u = w[0] / linalg.norm(w[0])
    angle = np.arctan(u[1] / u[0])
    angle = 180 * angle / np.pi  # convert to degrees
    # filled Gaussian at 2 standard deviation
    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,
                              180 + angle, facecolor=color,
                              edgecolor='black', linewidth=2)
    ell.set_clip_box(splot.bbox)
    ell.set_alpha(0.2)
    splot.add_artist(ell)
    splot.set_xticks(())
    splot.set_yticks(())


def plot_lda_cov(lda, splot):
    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')
    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')


def plot_qda_cov(qda, splot):
    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')
    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')


plt.figure(figsize=(10, 8), facecolor='white')
plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis',
             y=0.98, fontsize=15)
for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):
    # Linear Discriminant Analysis
    lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
    y_pred = lda.fit(X, y).predict(X)
    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)
    plot_lda_cov(lda, splot)
    plt.axis('tight')

    # Quadratic Discriminant Analysis
    qda = QuadraticDiscriminantAnalysis(store_covariance=True)
    y_pred = qda.fit(X, y).predict(X)
    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)
    plot_qda_cov(qda, splot)
    plt.axis('tight')
plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()
295/52:
from scipy import linalg
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import colors

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

# #############################################################################
# Colormap
cmap = colors.LinearSegmentedColormap(
    'red_blue_classes',
    {'red': [(0, 1, 1), (1, 0.7, 0.7)],
     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],
     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})
plt.cm.register_cmap(cmap=cmap)


# #############################################################################
# Generate datasets
def dataset_fixed_cov():
    '''Generate 2 Gaussians samples with the same covariance matrix'''
    n, dim = 300, 2
    np.random.seed(0)
    C = np.array([[0., -0.23], [0.83, .23]])
    X = np.r_[np.dot(np.random.randn(n, dim), C),
              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]
    y = np.hstack((np.zeros(n), np.ones(n)))
    return X, y


def dataset_cov():
    '''Generate 2 Gaussians samples with different covariance matrices'''
    n, dim = 300, 2
    np.random.seed(0)
    C = np.array([[0., -1.], [2.5, .7]]) * 2.
    X = np.r_[np.dot(np.random.randn(n, dim), C),
              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]
    y = np.hstack((np.zeros(n), np.ones(n)))
    return X, y


# #############################################################################
# Plot functions
def plot_data(lda, X, y, y_pred, fig_index):
    splot = plt.subplot(2, 2, fig_index)
    if fig_index == 1:
        plt.title('Linear Discriminant Analysis')
        plt.ylabel('Data with\n fixed covariance')
    elif fig_index == 2:
        plt.title('Quadratic Discriminant Analysis')
    elif fig_index == 3:
        plt.ylabel('Data with\n varying covariances')

    tp = (y == y_pred)  # True Positive
    tp0, tp1 = tp[y == 0], tp[y == 1]
    X0, X1 = X[y == 0], X[y == 1]
    X0_tp, X0_fp = X0[tp0], X0[~tp0]
    X1_tp, X1_fp = X1[tp1], X1[~tp1]

    # class 0: dots
    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')
    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x',
                s=20, color='#990000')  # dark red

    # class 1: dots
    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')
    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x',
                s=20, color='#000099')  # dark blue

    # class 0 and 1 : areas
    nx, ny = 200, 100
    x_min, x_max = plt.xlim()
    y_min, y_max = plt.ylim()
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),
                         np.linspace(y_min, y_max, ny))
    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])
    Z = Z[:, 1].reshape(xx.shape)
    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',
                   norm=colors.Normalize(0., 1.), zorder=0)
    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='white')

    # means
    plt.plot(lda.means_[0][0], lda.means_[0][1],
             '*', color='yellow', markersize=15, markeredgecolor='grey')
    plt.plot(lda.means_[1][0], lda.means_[1][1],
             '*', color='yellow', markersize=15, markeredgecolor='grey')

    return splot


def plot_ellipse(splot, mean, cov, color):
    v, w = linalg.eigh(cov)
    u = w[0] / linalg.norm(w[0])
    angle = np.arctan(u[1] / u[0])
    angle = 180 * angle / np.pi  # convert to degrees
    # filled Gaussian at 2 standard deviation
    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,
                              180 + angle, facecolor=color,
                              edgecolor='black', linewidth=2)
    ell.set_clip_box(splot.bbox)
    ell.set_alpha(0.2)
    splot.add_artist(ell)
    splot.set_xticks(())
    splot.set_yticks(())


def plot_lda_cov(lda, splot):
    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')
    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')


def plot_qda_cov(qda, splot):
    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')
    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')


plt.figure(figsize=(10, 8), facecolor='white')
plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis',
             y=0.98, fontsize=15)
for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):
    # Linear Discriminant Analysis
    lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
    y_pred = lda.fit(X, y).predict(X)
    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)
    plot_lda_cov(lda, splot)
    plt.axis('tight')

plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()
295/53:
from scipy import linalg
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import colors

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

# #############################################################################
# Colormap
cmap = colors.LinearSegmentedColormap(
    'red_blue_classes',
    {'red': [(0, 1, 1), (1, 0.7, 0.7)],
     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],
     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})
plt.cm.register_cmap(cmap=cmap)


# #############################################################################
# Generate datasets
def dataset_fixed_cov():
    '''Generate 2 Gaussians samples with the same covariance matrix'''
    n, dim = 300, 2
    np.random.seed(0)
    C = np.array([[0., -0.23], [0.83, .23]])
    X = np.r_[np.dot(np.random.randn(n, dim), C),
              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]
    y = np.hstack((np.zeros(n), np.ones(n)))
    return X, y


def dataset_cov():
    '''Generate 2 Gaussians samples with different covariance matrices'''
    n, dim = 300, 2
    np.random.seed(0)
    C = np.array([[0., -1.], [2.5, .7]]) * 2.
    X = np.r_[np.dot(np.random.randn(n, dim), C),
              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]
    y = np.hstack((np.zeros(n), np.ones(n)))
    return X, y


# #############################################################################
# Plot functions
def plot_data(lda, X, y, y_pred, fig_index):
    splot = plt.subplot(2, 2, fig_index)
    if fig_index == 1:
        plt.title('Linear Discriminant Analysis')
        plt.ylabel('Data with\n fixed covariance')
    elif fig_index == 2:
        plt.title('Quadratic Discriminant Analysis')
    elif fig_index == 3:
        plt.ylabel('Data with\n varying covariances')

    tp = (y == y_pred)  # True Positive
    tp0, tp1 = tp[y == 0], tp[y == 1]
    X0, X1 = X[y == 0], X[y == 1]
    X0_tp, X0_fp = X0[tp0], X0[~tp0]
    X1_tp, X1_fp = X1[tp1], X1[~tp1]

    # class 0: dots
    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')
    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x',
                s=20, color='#990000')  # dark red

    # class 1: dots
    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')
    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x',
                s=20, color='#000099')  # dark blue

    # class 0 and 1 : areas
    nx, ny = 200, 100
    x_min, x_max = plt.xlim()
    y_min, y_max = plt.ylim()
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),
                         np.linspace(y_min, y_max, ny))
    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])
    Z = Z[:, 1].reshape(xx.shape)
    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',
                   norm=colors.Normalize(0., 1.), zorder=0)
    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='white')

    # means
    plt.plot(lda.means_[0][0], lda.means_[0][1],
             '*', color='yellow', markersize=15, markeredgecolor='grey')
    plt.plot(lda.means_[1][0], lda.means_[1][1],
             '*', color='yellow', markersize=15, markeredgecolor='grey')

    return splot


def plot_ellipse(splot, mean, cov, color):
    v, w = linalg.eigh(cov)
    u = w[0] / linalg.norm(w[0])
    angle = np.arctan(u[1] / u[0])
    angle = 180 * angle / np.pi  # convert to degrees
    # filled Gaussian at 2 standard deviation
    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,
                              180 + angle, facecolor=color,
                              edgecolor='black', linewidth=2)
    ell.set_clip_box(splot.bbox)
    ell.set_alpha(0.2)
    splot.add_artist(ell)
    splot.set_xticks(())
    splot.set_yticks(())


def plot_lda_cov(lda, splot):
    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')
    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')


def plot_qda_cov(qda, splot):
    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')
    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')


plt.figure(figsize=(10, 8), facecolor='white')
plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis',
             y=0.98, fontsize=15)
for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):
    # Linear Discriminant Analysis
    lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
    y_pred = lda.fit(X, y).predict(X)
    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)


plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()
295/54:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
295/55:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
295/56:
lda = LinearDiscriminantAnalysis(store_covariance=True)
lda.fit(X_train, Y_train)
295/57:
y_pred = lda.fit(X_train, Y_train).predict(X_test)
splot = plot_data(lda, X_train, Y_train, y_pred, fig_index=2 * 1 + 1)
295/58: lda.predict(X_test)
296/1: import seaborn as sns
296/2:
import seaborn as sns
from matplotlib import pyplot as plt
import numpy as np
%matplotlib inline
296/3: sns.load_dataset("tips")
296/4: tips = sns.load_dataset("tips")
296/5:
tips = sns.load_dataset("tips")
sns.relplot(x="total_bill", y="tip", data=tips)
296/6:
tips = sns.load_dataset("tips")
sns.relplot(x="total_bill", y="tip", data=tips, hue="smoker")
296/7:
sns.set(style = 'darkgrid')
tips = sns.load_dataset("tips")
tips.tail
296/8:
sns.set(style = 'darkgrid')
tips = sns.load_dataset("tips")
tips.tail()
296/9:
tips = sns.load_dataset("tips")
sns.relplot(x="total_bill", y="tip", data=tips, hue="smoker")
297/1:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.model_selection import train_test_split
import numpy as np
from matplotlib import pyplot as plt
297/2:
wine = datasets.load_wine()
X = wine.data
Y = wine.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=42)
297/3:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.svm import SVM
from sklearn.model_selection import train_test_split
import numpy as np
from matplotlib import pyplot as plt
297/4:
%matplotlib inline
import sklearn.datasets as datasets
from sklearn.svm import SVM
from sklearn.model_selection import train_test_split
import numpy as np
from matplotlib import pyplot as plt
298/1:
%matplotlib inline
from sklearn.svm import LinearSVC
import numpy as np
from matplotlib import pyplot as plt
298/2:
X = [[1, 1], [2, 2], [2, 1], [1, 2]]
Y = [1, 1, 0, 0]
298/3:
plt.plt(X, Y)
plt.show()
298/4:
plt.plot(X, Y)
plt.show()
298/5:
plt.plot(X)
plt.show()
298/6:
plt.scatter(X)
plt.show()
298/7:
plt.scatter(X, Y)
plt.show()
298/8:
plt.scatter(X)
plt.show()
298/9:
%matplotlib inline
from sklearn.svm import LinearSVC
import numpy as np
from matplotlib import pyplot as plt
298/10:
clf = LinearSVC()
clf.p
298/11: clf = LinearSVC()
298/12:
clf = LinearSVC()
clf.predict(X, Y)
298/13:
%matplotlib inline
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
298/14:
clf = SVC()
clf.predict(X, y)
298/15:
X = [[1, 1], [2, 2], [2, 1], [1, 2]]
y = [1, 1, 0, 0]
298/16:
clf = SVC()
clf.predict(X, y)
298/17:
clf = LinearSVC()
clf.predict(X, y)
298/18:
clf = LinearSVC()
clf.fit(X, y)
298/19:
clf = LinearSVC()
clf.fit(X, y)
298/20: clf.class_weight
298/21: clf.coef_
298/22: clf.predict([1.4, 1.4])
298/23: clf.predict([1.4 1.4])
298/24: clf.predict([1, 1])
298/25: clf.predict([1., 1.])
298/26:
clf = LinearSVC()
clf.fit(X, y)
298/27: clf.predict([1., 1.])
298/28: clf.predict([[1.4, 1.4]])
298/29: clf.predict([[1, 1]])
298/30:
clf = SVC(kernel= 'rbf')
clf.fit(X, y)
298/31: clf.predict([[1, 1]])
298/32: clf.predict([[1.4, 1.4]])
298/33: clf.predict([[1.6, 1.6]])
298/34: clf.support_vectors_
298/35: plt.scatter(clf.support_vectors_)
298/36: clf.predict([[6.6, 1.6]])
298/37: clf.predict([[6.6, 6.6]])
298/38: clf.predict([[10, 100]])
298/39: clf.predict([[-10, 100]])
298/40: clf.predict([[-10, -100]])
298/41:
nonlinear = SVC(kernel= 'rbf')
nonlinear.fit(X, y)
298/42: nonlinear.support_vectors_
298/43:
support_vectors = nonlinear.support_vectors_

plot(support_vectors(:,1),support_vectors(:,2),'r.','MarkerSize',15)
298/44:
support_vectors = nonlinear.support_vectors_

plt.scatter(support_vectors(:,1),support_vectors(:,2))
298/45: support_vectors = nonlinear.support_vectors_
298/46:
support_vectors = nonlinear.support_vectors_
support_vectors.__class__
298/47:
support_vectors = nonlinear.support_vectors_
support_vectors(:,1)
298/48:
support_vectors = nonlinear.support_vectors_
support_vectors[:,1]
298/49:
support_vectors = nonlinear.support_vectors_
plt.scatter(support_vectors[:,1], support_vectors[:, 2])
298/50:
support_vectors = nonlinear.support_vectors_
plt.scatter(support_vectors[:,0], support_vectors[:, 1])
298/51:
support_vectors = nonlinear.support_vectors_
support_vectors
298/52:
support_vectors = nonlinear.support_vectors_
nonlinear.support_
298/53:
support_vectors = nonlinear.support_vectors_
nonlinear.classes_
298/54:
support_vectors = nonlinear.support_vectors_
nonlinear.support_vectors_.
298/55:
support_vectors = nonlinear.support_vectors_
nonlinear.decision_function_shape
298/56:
support_vectors = nonlinear.support_vectors_
nonlinear.decision_function
298/57: nonlinear.predict([[1, 1]])
298/58: nonlinear.predict([[10, 2]])
298/59: nonlinear.predict([[-1, -1]])
298/60:
nonlinear = SVC(kernel= 'linear')
nonlinear.fit(X, y)
298/61: nonlinear.predict([[-1, -1]])
298/62:
nonlinear = SVC(kernel= 'linear')
nonlinear.fit(X, y)
298/63: nonlinear.support_vectors_
298/64: nonlinear.predict([[-1, 0]])
298/65: nonlinear.predict([[-1, 0]]).head
298/66: nonlinear.predict([[-1, 0]]).__class__
298/67:
head, *tail = nonlinear.predict([[-1, 0]])
head
298/68:
head = nonlinear.predict([[-1, 0]])
head
298/69: nonlinear.predict([[-1, 0]])
298/70: nonlinear.predict([[2.5, 2.5]])
298/71: nonlinear.predict([[1.5, 1.5]])
298/72: nonlinear.predict([[1.8, 1]])
298/73: nonlinear.decision_function([[1, 0]])
298/74: nonlinear.predict([[1, 0]])
298/75: nonlinear.decision_function([1, 0], [1, 2])
298/76: nonlinear.decision_function([[1, 0], [1, 2]])
298/77:
%matplotlib inline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
298/78:
pred_decisions = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 0], pred_decision)
298/79:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 0], pred_decision)
298/80:
nonlinear = SVC(kernel= 'rbf')
nonlinear.fit(X, y)
298/81:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 0], pred_decision)
298/82:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 1], pred_decision)
298/83:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([0, 1], pred_decision)
298/84:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([0, 0], pred_decision)
298/85:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 0], pred_decision)
298/86:
nonlinear = SVC(kernel= 'poly', degree=2)
nonlinear.fit(X, y)
298/87:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 0], pred_decision)
298/88: nonlinear.predict([1, 0])
298/89: nonlinear.predict([[1, 0]])
298/90: nonlinear.predict([[1, 1]])
298/91:
nonlinear = SVC(kernel= 'poly', degree=2)
nonlinear.fit(X, y)
298/92:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 0], pred_decision)
298/93: nonlinear.predict([[1, 0]])
298/94: nonlinear.kernel
298/95:
%matplotlib inline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
298/96: pairwise_kernels(X)
298/97: pairwise_kernels(X, metric='linear', degree=1)
298/98: pairwise_kernels(X, metric='linear')
298/99:
nonlinear = SVC(kernel= 'poly', degree=1)
nonlinear.fit(X, y)
298/100:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 0], pred_decision)
298/101: pairwise_kernels(X, metric='linear')
298/102:
nonlinear = SVC(kernel= 'linear', degree=1)
nonlinear.fit(X, y)
298/103:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 0], pred_decision)
298/104:
pred_decision = nonlinear.decision_function([[1, 0], [1, 2]])
hinge_loss([1, 0], pred_decision)
298/105:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 0], pred_decision)
298/106:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 1], pred_decision)
298/107:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 1], pred_decision)
298/108:
nonlinear = SVC(kernel= 'linear', degree=2)
nonlinear.fit(X, y)
298/109:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 1], pred_decision)
298/110:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 1], pred_decision)
298/111:
nonlinear = SVC(kernel= 'poly', degree=2)
nonlinear.fit(X, y)
298/112:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 1], pred_decision)
298/113:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([0, 0], pred_decision)
298/114:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([0, 1], pred_decision)
298/115:
X = [[1, 1], [2, 2]]
y = [1, 0]
298/116:
clf = LinearSVC()
clf.fit(X, y)
298/117: clf.predict([[1, 1]])
298/118: clf.predict([[0, 1]])
298/119: clf.predict([[2, 5]])
298/120:
nonlinear = SVC(kernel= 'poly', degree=2)
nonlinear.fit(X, y)
298/121:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([0, 1], pred_decision)
298/122:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 1], pred_decision)
298/123:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 0], pred_decision)
298/124:
%matplotlib inline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
np.options.display.float_format = '{:.4f}'.format
298/125:
%matplotlib inline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
np.display.float_format = '{:.4f}'.format
298/126:
%matplotlib inline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
np.float_format = '{:.4f}'.format
298/127:
%matplotlib inline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
np.float_format = '{:.4f}'.format
298/128:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 0], pred_decision)
298/129:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 0], pred_decision)
298/130:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
hinge_loss([1, 0], pred_decision)
298/131:
%matplotlib inline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
float_formatter = "{:.2f}".format
298/132:
%matplotlib inline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
float_formatter = "{:.2f}".format
298/133:
%matplotlib inline
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
float_formatter = "{:.2f}".format
298/134:
pred_decision = nonlinear.decision_function([[1, 1], [2, 2]])
float_formatter(hinge_loss([1, 0], pred_decision))
299/1:
%matplotlib inline
from sklearn.linear_model import 
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
float_formatter = "{:.2f}".format
299/2:
%matplotlib inline
from sklearn.linear_model import Perceptron
import numpy as np
from matplotlib import pyplot as plt
from sklearn.metrics import hinge_loss
from sklearn.metrics.pairwise import pairwise_kernels
float_formatter = "{:.2f}".format
299/3:
%matplotlib inline
from sklearn.linear_model import Perceptron
import numpy as np
float_formatter = "{:.2f}".format
299/4:
X = [[1, 1], [2, 2]]
y = [1, 0]
299/5:
X = [[1, 1], [2, 2], [1, 2], [2, 1]]
y = [1, 1, 0, 0]
299/6: perceptron = Perceptron()
299/7:
perceptron = Perceptron()
perceptron.fit(X, y)
299/8: perceptron.score(X, y)
299/9:
X_Linear = [[1, 1], [2, 2]]
y_Linear = [1, 0]
299/10:
X_Linear = [[1, 1], [2, 2]]
y_Linear = [1, 0]
perceptron.fit(X_Linear, y_Linear)
299/11:
X_Linear = [[1, 1], [2, 2]]
y_Linear = [1, 0]
perceptron.fit(X_Linear, y_Linear)
perceptron.score(X_Linear, y_Linear)
299/12:
perceptron = Perceptron(iter = 1000)
perceptron.fit(X, y)
299/13:
perceptron = Perceptron(max_iter = 1000)
perceptron.fit(X, y)
299/14:
perceptron = Perceptron(max_iter = 10000)
perceptron.fit(X, y)
299/15:
perceptron = Perceptron(max_iter = 10000, n_iter_int = 1000)
perceptron.fit(X, y)
299/16:
perceptron = Perceptron()
perceptron.fit(X, y)
299/17:
X_Linear = [[1, 1], [2, 2]]
y_Linear = [1, 0]
perceptron.fit(X_Linear, y_Linear)
perceptron.score(X_Linear, y_Linear)
299/18:
X = [[1, 1], [2, 2], [1, 2], [2, 1]]
y = [1, 0, 0, 0]
299/19:
perceptron = Perceptron()
perceptron.fit(X, y)
299/20: perceptron.score(X, y)
299/21:
X = [[1, 1], [2, 2], [1, 2], [2, 1]]
y = [1, 1, 0, 0]
299/22:
perceptron = Perceptron()
perceptron.fit(X, y)
299/23: perceptron.score(X, y)
299/24:
X_Linear = [[1, 1], [2, 2]]
y_Linear = [1, 0]
perceptron.fit(X_Linear, y_Linear)
perceptron.score(X_Linear, y_Linear)
300/1:
sqlStr = "SELECT x.object_schema table_schema, x.object_name table_name, x.audit_start_timestamp, x.size_in_mb"
sqlStr += " FROM"
sqlStr += " (SELECT x.*, to_char(y.size_in_mb,'999,999,999,999.999') size_in_mb"
sqlStr += " FROM"
sqlStr += " (SELECT object_id, object_name, object_schema, MAX(audit_start_timestamp) audit_start_timestamp FROM user_audits WHERE object_type = 'TABLE' AND object_name IS NOT NULL GROUP BY object_id, object_name, object_schema) x"
sqlStr += " JOIN (SELECT object_id, audit_start_timestamp, SUM(size_bytes/1024/1024) size_in_mb FROM user_audits  WHERE object_type = 'TABLE' GROUP BY object_id, audit_start_timestamp) y "
sqlStr += " ON (y.object_id =  x.object_id AND y.audit_start_timestamp = x.audit_start_timestamp)) x"
300/2: sqlStr
301/1:
%matplotlib inline
import numpy as np
from matplotlib import pyplot as plt
float_formatter = "{:.2f}".format
301/2:
%matplotlib inline
import numpy as np
from matplotlib import pyplot as plt
float_formatter = "{:.2f}".format
from sklearn.neighbors import NearestNeighbors
301/3:
%matplotlib inline
import numpy as np
from matplotlib import pyplot as plt
float_formatter = "{:.2f}".format
from sklearn.neighbors import NearestNeighbors
import sklearn.datasets as datasets
301/4:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)
301/5:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)
301/6:
%matplotlib inline
import numpy as np
from matplotlib import pyplot as plt
float_formatter = "{:.2f}".format
from sklearn.neighbors import NearestNeighbors
import sklearn.datasets as datasets
from sklearn.model_selection import train_test_split
301/7:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)
301/8:
%matplotlib inline
import numpy as np
from matplotlib import pyplot as plt
float_formatter = "{:.2f}".format
from sklearn.neighbors import NearestNeighbors
import sklearn.datasets as datasets
from sklearn.model_selection import train_test_split
301/9:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.05, random_state=42)
301/10: n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35, )
301/11: n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35)
301/12:
n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35)
n.fit(X_train, X_test)
301/13: Y_train
301/14:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01, random_state=42)
301/15:
n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35)
n.fit(X_train, X_test)
301/16: Y_train
301/17: Y_train.shape
301/18:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.001, random_state=42)
301/19:
n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35)
n.fit(X_train, X_test)
301/20: Y_train.shape
301/21: Y_train.shape
301/22: digits.shape
301/23: digits.__class__
301/24: digits.data.shape
301/25: Y_train.shape
301/26:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1, random_state=42)
301/27:
n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35)
n.fit(X_train, X_test)
301/28: Y_train.shape
301/29: digits.data.__class__
301/30: digits.data.size
301/31: np.size(digits.data.size, 0)
301/32: np.size(digits.data.size, 1)
301/33: digits.data.shape
301/34: Y_train.shape
301/35: Y_train.take(1)
301/36: Y_train.take(11)
301/37:
n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35)
n.fit(X_train, X_test)
301/38:
n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35)
n.fit(X, Y)
301/39: X
301/40: X.take(1)
301/41: Y.take(1)
301/42:
digits = datasets.load_sample_image
digits
301/43:
digits = datasets.load_sample_image()
digits
301/44:
digits = datasets.digits()
X = digits.data
Y = digits.target
301/45:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
301/46:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X.shape
301/47:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X.take(:1, :)
301/48:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X.take(1:, :)
301/49:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X[1:, :]
301/50:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
X[:1, :]
301/51:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
test = X[:1, :]
301/52:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
test = X[:1, :]
test.shape
301/53:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
test = X[:1, :]
test.reshape(1, )
301/54:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
images_and_labels = list(zip(digits.images, digits.target))
for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):
    ax.set_axis_off()
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    ax.set_title('Training: %i' % label)
301/55:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
images_and_labels = list(zip(digits.images, digits.target))

images_and_labels
301/56:
digits = datasets.load_digits()
X = digits.data
Y = digits.target
301/57:
n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35)
n.fit(X, Y)
301/58:
n = NearestNeighbors(n_neighbors=2, radius=0.5, algorithm='kd_tree', leaf_size=35)
n.fit(X, Y)
301/59: n.kneighbors_graph
301/60: n.kneighbors_graph()
301/61: n.kneighbors(X[:1, :])
301/62:
n = NearestNeighbors(n_neighbors=1, radius=0.5, algorithm='kd_tree', leaf_size=35)
n.fit(X, Y)
301/63: n.kneighbors(X[:1, :])
301/64:
neighbours = n.kneighbors(X[:1, :])
neighbours.shape
301/65:
neighbours = n.kneighbors(X[:1, :])
neighbours.count
301/66:
neighbours = n.kneighbors(X[:1, :])
neighbours.count()
301/67:
neighbours = n.kneighbors(X[:1, :])
neighbours.__class__
301/68:
neighbours = n.kneighbors(X[:1, :])
neighbours[0]
301/69:
neighbours = n.kneighbors(X[:1, :])
neighbours[0]
301/70:
neighbours = n.kneighbors(X[:1, :])
neighbours[0].__class__
301/71:
neighbours = n.kneighbors(X[:1, :])
neighbours[0].shape
301/72:
neighbours = n.kneighbors(X[:1, :])
neighbours[0]
301/73:
neighbours = n.kneighbors(X[:2, :])
neighbours[0]
304/1: from sklearn import tree
304/2:
X = [[0, 0], [1, 1]]
Y = [0, 1]

clf = tree.DecisionTreeClassifier()
304/3:
X = [[0, 0], [1, 1]]
Y = [0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
304/4:
X = [[0, 0], [1, 1]]
Y = [0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([0, 1])
304/5:
X = [[0, 0], [1, 1]]
Y = [0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[0, 1]])
304/6:
X = [[0, 0], [1, 1]]
Y = [0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[0, -1]])
306/1: from sklearn.ensemble import RandomForestClassifier
306/2:
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
306/3: rfc = RandomForestClassifier()
306/4:
X = [[0, 0], [1, 1]]
Y = [0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[0, -1]])
306/5:
rfc = RandomForestClassifier()
rfc.fit(X, Y)
306/6:
rfc = RandomForestClassifier()
rfc.fit(X, Y)
rfc.oob_score_float
306/7:
rfc = RandomForestClassifier()
rfc.fit(X, Y)
rfc.oob_score
306/8:
rfc = RandomForestClassifier(oob_score=True)
rfc.fit(X, Y)
rfc.oob_score
306/9:
rfc = RandomForestClassifier(oob_score=True)
rfc.fit(X, Y)
rfc.oob_score_
306/10:
X = [[0, 0], [1, 1], [1, 1]]
Y = [0, 1, 0]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[0, -1]])
306/11:
rfc = RandomForestClassifier(oob_score=True)
rfc.fit(X, Y)
rfc.oob_score_
306/12:
rfc = RandomForestClassifier(oob_score=True)
rfc.fit(X, Y)
rfc.oob_score_
306/13:
X = [[0, 0], [1, 1], [1, 1], [0, 1]]
Y = [0, 1, 0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[0, -1]])
306/14:
rfc = RandomForestClassifier(oob_score=True)
rfc.fit(X, Y)
rfc.oob_score_
307/1:
import matplotlib.pyplot as plt
import numpy as np
307/2: plt.figure()
307/3: plt.figure()
307/4:
fig = plt.figure()
fig.suptitle('No axes on this figure')  #
307/5:
fig = plt.figure()
fig.suptitle('No axes on this figure')  
fig, ax_lst = plt.subplots(2, 2)
307/6: plt.figure().show()
307/7:
plt.plot([1, 2, 3, 4])
plt.ylabel('some numbers')
plt.show()
307/8:
plt.plot([1, 2, 3, 4])
plt.ylabel('some numbers')
plt.xlabel('some numbers x')
plt.show()
307/9:
plt.plot([1, 0, 3, 4])
plt.ylabel('some numbers')
plt.xlabel('some numbers x')
plt.show()
307/10:
plt.plot([[1, 0, 3, 4][0, 8, 9, 0]])
plt.ylabel('some numbers')
plt.xlabel('some numbers x')
plt.show()
307/11:
plt.plot(np.array([[1, 2, 3], [4, 5, 6]], np.int32))
plt.ylabel('some numbers')
plt.xlabel('some numbers x')
plt.show()
307/12:
# evenly sampled time at 200ms intervals
t = np.arange(0., 5., 0.2)

# red dashes, blue squares and green triangles
plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
plt.show()
308/1:
X = [[0, 0], [1, 1], [1, 1], [0, 1]]
Y = [0, 1, 0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[0, -1]])
308/2:
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
308/3:
X = [[0, 0], [1, 1], [1, 1], [0, 1]]
Y = [0, 1, 0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[0, -1]])
308/4:
rfc = RandomForestClassifier(oob_score=True)
rfc.fit(X, Y)
rfc.oob_score_
308/5:
X = [[0, 0], [1, 1], [1, 1], [0, 1]]
Y = [0, 1, 0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf.predict([[0, -1]])
308/6:
X = [[0, 0], [1, 1], [1, 1], [0, 1]]
Y = [0, 1, 0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
308/7:
X = [[0, 0], [1, 1], [1, 1], [0, 1]]
Y = [0, 1, 0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
308/8:
X = [[0, 0], [1, 1], [1, 1], [0, 1]]
Y = [0, 1, 0, 1]

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)

clf
309/1: import pandas as pd
309/2:
import pandas as pd
import sqlalchemy
309/3:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine
309/4:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine
309/5:
# DEFINE DATASOURCES URL STRINGS
redshift_engine_url = f"postgresql+psycopg2://{user}:{password}@{endpoint}:{port}/{db}"
vertica_engine_url = f"vertica+pyodbc://{user}:{password}@{endpoint}:{port}/{db}"
309/6:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica():
    print("Vertica")
309/7:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica():
    print("Vertica")
    

read_from_vertica()
309/8:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    
    

read_from_vertica()
309/9:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    print(username, password)
    
    

read_from_vertica()
309/10:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    print(username, password)
    
    

read_from_vertica("u", "p", "sql", "db")
309/11:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+pyodbc://{username}:{password}@{vertica.chotel.com}:{5433}/{database}"
    print(vertica_url)
    

read_from_vertica("u", "p", "sql", "db")
309/12:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+pyodbc://{username}:{password}@vertica.chotel.com:5433/{database}"
    print(vertica_url)
    

read_from_vertica("u", "p", "sql", "db")
309/13:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+pyodbc://{username}:{password}@vertica.chotel.com:5433/{database}".format(username=username, password= password, database= database)
    print(vertica_url)
    

read_from_vertica("u", "p", "sql", "db")
309/14:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+pyodbc://{username}:{password}@vertica.chotel.com:5433/{database}"
    .format(username=username, password= password, database= database)
    print(vertica_url)
    

read_from_vertica("u", "p", "sql", "db")
309/15:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+pyodbc://{username}:{password}@vertica.chotel.com:5433/{database}"
        .format(username=username, password= password, database= database)
    print(vertica_url)
    

read_from_vertica("u", "p", "sql", "db")
309/16:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+pyodbc://{username}:{password}@vertica.chotel.com:5433/{database}" \
        .format(username=username, password= password, database= database)
    print(vertica_url)
    

read_from_vertica("u", "p", "sql", "db")
309/17:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+pyodbc://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database= database if database != None)
    print(vertica_url)
    

read_from_vertica("u", "p", "sql", "db")
309/18:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+pyodbc://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    print(vertica_url)
    

read_from_vertica("u", "p", "sql", "db")
309/19:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+pyodbc://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    
    vertica_engine = create_engine(vertica_url)

read_from_vertica("u", "p", "sql", "db")
309/20:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+pyodbc://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    
    vertica_engine = create_engine(vertica_url)

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/21:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    
    vertica_engine = create_engine(vertica_url)

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/22:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine
import vertica_python

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    
    vertica_engine = create_engine(vertica_url)

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/23:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine
from sqlalchemy import vertica_python

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    
    vertica_engine = create_engine(vertica_url)

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/24:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine
from sqlalchemy import vertica_python

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    
    vertica_engine = create_engine(vertica_url)

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/25:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    
    vertica_engine = create_engine(vertica_url)

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/26:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    print(vertica_url)
    vertica_engine = create_engine(vertica_url)

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/27:
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
    print(vertica_url)
    vertica_engine = create_engine(vertica_url)

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/28:
import pandas as pd
import vertica_db_client
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database)
        sa.create_engine(sa.engine.url.URL(
            drivername='vertica+pyodbc',
            username='umasrivenkat_kannikanti',
            password='Lakshmi7997!',
            host='vertica.chotel.com',
            database='',

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/29:
import pandas as pd
import vertica_db_client
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database

read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/30:
import pandas as pd
import vertica_db_client
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database
309/31:
import pandas as pd
import vertica_db_client
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = "vertica://{username}:{password}@vertica.chotel.com:5433/{database}" \
                  .format(username=username, password= password, database=database
309/32:
import pandas as pd
import vertica_db_client
import sqlalchemy
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    print(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/33:
import pandas as pd
import vertica_db_client
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/34:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/35:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/36:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/37:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/38:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/39:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/40:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import sqlalchemy-vertica

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/41:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import sqlalchemy from vertica_url

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/42:
import pandas as pd
from sqlalchemy import create_engine
import sqlalchemy from vertica_url

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/43:
import pandas as pd
from sqlalchemy import create_engine
from vertica_url import sqlalchemy

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/44:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/45:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/46:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/47:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/48:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
309/49:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
310/1:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
310/2:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    sa.create_engine(vertica_url)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
310/3:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    engine = sa.create_engine(vertica_url)
    print(engine)
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "sql", "MDA")
310/4:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    engine = sa.create_engine(vertica_url)
    pd.read_sql(sql, )
    
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "select * from MDA.on_training1 limit 100", "MDA")
310/5:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, paramslist:None, parse_dateslist:None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    engine = sa.create_engine(vertica_url)
    df = pd.read_sql(sql=sqlQuery, con=engine, params=paramslist, parse_dateslist=parse_dateslist)
    df.head()
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "select * from MDA.on_training1 limit 100", "MDA")
310/6:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, paramslist=None, parse_dateslist=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    engine = sa.create_engine(vertica_url)
    df = pd.read_sql(sql=sqlQuery, con=engine, params=paramslist, parse_dateslist=parse_dateslist)
    df.head()
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "select * from MDA.on_training1 limit 100", "MDA")
310/7:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    engine = sa.create_engine(vertica_url)
    df = pd.read_sql(sql=sqlQuery, con=engine, params=params, parse_dates=parse_dates)
    df.head()
    
read_from_vertica("umasrivenkat_kannikanti", "Lakshmi7997!", "select * from MDA.on_training1 limit 100", "MDA")
310/8:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    engine = sa.create_engine(vertica_url)
    df = pd.read_sql(sql=sqlQuery, con=engine, params=params, parse_dates=parse_dates)
    df.head()
    
read_from_vertica("dbadmin", "Chuz9puB", "select * from MDA.on_training1 limit 100", "MDA")
310/9:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    engine = sa.create_engine(vertica_url)
    df = pd.read_sql(sql=sqlQuery, con=engine, params=params, parse_dates=parse_dates)
    df.head()
    
read_from_vertica("dbadmin", "Chuz9puB", "select * from on_training1 limit 100", "MDA")
310/10:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433"
    engine = sa.create_engine(vertica_url)
    df = pd.read_sql(sql=sqlQuery, con=engine, params=params, parse_dates=parse_dates)
    df.head()
    
read_from_vertica("dbadmin", "Chuz9puB", "select * from MDA.on_training1 limit 100", "MDA")
310/11:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433"
    engine = sa.create_engine(vertica_url)
    df = pd.read_sql(sql=sqlQuery, con=engine, params=params, parse_dates=parse_dates)
    df
    
read_from_vertica("dbadmin", "Chuz9puB", "select * from MDA.on_training1 limit 100", "MDA")
310/12: read_from_vertica("dbadmin", "Chuz9puB", "select * from MDA.on_training1 limit 100", "MDA")
310/13:
df = read_from_vertica("dbadmin", "Chuz9puB", "select * from MDA.on_training1 limit 100", "MDA")
print(df)
310/14:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433"
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df
310/15:
df = read_from_vertica("dbadmin", "Chuz9puB", "select * from MDA.on_training1 limit 100", "MDA")
print(df)
310/16: read_from_redshift("dbamin", "", "select * from MDA.on_training1 limit 100", "MDA")
310/17:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433"
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database, params=None, parse_dates=None):
    redshift_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433/{database}"
    print(redshift_url)
310/18: read_from_redshift("dbamin", "", "select * from MDA.on_training1 limit 100", "MDA")
310/19:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433"
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database, params=None, parse_dates=None):
    redshift_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433"+(database if not database)
    print(redshift_url)
310/20:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433"
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database, params=None, parse_dates=None):
    redshift_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else database)
    print(redshift_url)
310/21: read_from_redshift("dbamin", "p", "select * from MDA.on_training1 limit 100", "MDA")
310/22:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433"
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database, params=None, parse_dates=None):
    redshift_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/"+database)
    print(redshift_url)
310/23: read_from_redshift("dbamin", "p", "select * from MDA.on_training1 limit 100", "MDA")
310/24:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database, params=None, parse_dates=None):
    redshift_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    print(redshift_url)
310/25: read_from_redshift("dbamin", "p", "select * from MDA.on_training1 limit 100", "MDA")
310/26:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database, params=None, parse_dates=None):
    redshift_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    print(redshift_url)
310/27: read_from_redshift("dbamin", "p", "select * from MDA.on_training1 limit 100", "")
310/28: read_from_redshift("dbamin", "p", "select * from MDA.on_training1 limit 100")
310/29:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database, params=None, parse_dates=None):
    redshift_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    print(redshift_url)
310/30: read_from_redshift("dbamin", "p", "select * from MDA.on_training1 limit 100")
310/31:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    redshift_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    print(redshift_url)
310/32: read_from_redshift("dbamin", "p", "select * from MDA.on_training1 limit 100")
310/33:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    redshift_url = f"postgresql+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    print(redshift_url)
310/34: read_from_redshift("dbamin", "p", "select * from MDA.on_training1 limit 100")
310/35: read_from_redshift("dbamin", "p", "select * from MDA.on_training1 limit 100", database="dap")
310/36:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    redshift_url = f"postgresql+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df
310/37: read_from_redshift("umasrivenkat_kannikanti", "Lakshmi7997!", "SELECT * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10", database="dap")
310/38:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    redshift_url = f"postgresql+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df
310/39: read_from_redshift("umasrivenkat_kannikanti", "Lakshmi7997!", "SELECT * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10", database="dap")
310/40:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df
310/41: read_from_redshift("umasrivenkat_kannikanti", "Lakshmi7997!", "SELECT * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10", database="dap")
311/1:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df
311/2: read_from_redshift("umasrivenkat_kannikanti", "Lakshmi7997!", "SELECT * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10", database="dap")
311/3: read_from_redshift("big_etl", "swl9@DricRotRuqi!tep", "SELECT * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10", database="dap")
311/4:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine

def read_from_vertica(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sqlQuery, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sqlQuery, con=connection, params=params, parse_dates=parse_dates)
    return df
311/5: read_from_redshift("big_etl", "swl9@DricRotRuqi!tep", "SELECT * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10", database="dap")
311/6:
df = read_from_redshift("big_etl", "swl9@DricRotRuqi!tep", "SELECT * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10", database="dap")
df.head()
311/7:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def write_dataframe_to_s3(df, project_name):
311/8:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
311/9: write_dataframe_to_s3("", "", "PARQUET")
311/10:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    print(file_format)
311/11: write_dataframe_to_s3("", "", "PARQUET")
311/12: write_dataframe_to_s3("", "", "CSV")
311/13: write_dataframe_to_s3("", "", "CSV")
311/14: write_dataframe_to_s3("", "", "CSV")
311/15:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    if file_format == 'CSV':
        print("csv")
    if file_format == 'PARQUET'
        print("csv")
311/16: write_dataframe_to_s3("", "", "CSV")
311/17:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    if file_format == "CSV":
        print("csv")
    if file_format == "PARQUET":
        print("csv")
311/18: write_dataframe_to_s3("", "", "CSV")
311/19: write_dataframe_to_s3("", "", "PARQUET")
311/20: write_dataframe_to_s3("", "", "PARQUET")
311/21:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    if file_format == "CSV":
        print("csv")
    if file_format == "PARQUET":
        print("parquet")
311/22: write_dataframe_to_s3("", "", "PARQUET")
311/23: write_dataframe_to_s3("", "", "parquet")
311/24: write_dataframe_to_s3("", "", "parquet")
311/25:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    if file_format.upper() == "CSV":
        print("csv")
    if file_format.upper() == "PARQUET":
        print("parquet")
311/26: write_dataframe_to_s3("", "", "parquet")
311/27: write_dataframe_to_s3("", "", "paRquet")
311/28:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    if file_format.upper() == "CSV":
        print("csv")
    else if file_format.upper() == "PARQUET":
        print("parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
311/29:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    if file_format.upper() == "CSV":
        print("csv")
    elif file_format.upper() == "PARQUET":
        print("parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
311/30: write_dataframe_to_s3("", "", "")
311/31: write_dataframe_to_s3("", "", "CSV")
311/32: write_dataframe_to_s3("", "", "CSV")
311/33:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    if file_format.upper() == "CSV":
        write_as_csv(df, project_name)
    elif file_format.upper() == "PARQUET":
        write_as_csv(df, project_name)
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/34: write_dataframe_to_s3("", "", "CSV")
311/35: write_dataframe_to_s3("", "", "CSV")
311/36:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime


BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    location = BIG_MODELING_BUCKET.join("/", project_name, "/", )
    if file_format.upper() == "CSV":
        write_as_csv(df, project_name)
    elif file_format.upper() == "PARQUET":
        write_as_csv(df, project_name)
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/37:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime


BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M"
    location = BIG_MODELING_BUCKET.join("/", project_name, "/", timestampStr)
    if file_format.upper() == "CSV":
        write_as_csv(df, location)
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location)
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/38:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime


BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M"
    location = BIG_MODELING_BUCKET.join("/")
    if file_format.upper() == "CSV":
        write_as_csv(df, location)
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location)
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/39:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime


BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET.join("/", project_name, "/", timestampStr)
    if file_format.upper() == "CSV":
        write_as_csv(df, location)
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location)
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/40: write_dataframe_to_s3("", "", "CSV")
311/41:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime


BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET.join("/").join(project_name).join("/").join(timestampStr)
    if file_format.upper() == "CSV":
        write_as_csv(df, location)
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location)
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/42: write_dataframe_to_s3("", "", "CSV")
311/43:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime


BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location_strings = ["/", project_name, "/", timestampStr]
    location = BIG_MODELING_BUCKET.join(location_strings)
    if file_format.upper() == "CSV":
        write_as_csv(df, location)
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location)
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/44: write_dataframe_to_s3("", "", "CSV")
311/45:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime


BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location)
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location)
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/46: write_dataframe_to_s3("", "", "CSV")
311/47: write_dataframe_to_s3("df", "My_project", "CSV")
311/48:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime


BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df

def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
def write_dataframe_to_s3(df, project_name, file_format="CSV"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/49: write_dataframe_to_s3("df", "My_project", "CSV")
311/50: write_dataframe_to_s3("df", "My_project", "parquet")
311/51: write_dataframe_to_s3("df", "My_project")
311/52:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")

        
def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/53: write_dataframe_to_s3("df", "My_project")
312/1: %%writefile preprocessing.py
312/2:
%%writefile preprocessing.py
import argparse
import os
import warnings

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.compose import make_column_transformer

from sklearn.exceptions import DataConversionWarning
warnings.filterwarnings(action='ignore', category=DataConversionWarning)


columns = ['age', 'education', 'major industry code', 'class of worker', 'num persons worked for employer',
           'capital gains', 'capital losses', 'dividends from stocks', 'income']
class_labels = [' - 50000.', ' 50000+.']

def print_shape(df):
    negative_examples, positive_examples = np.bincount(df['income'])
    print('Data shape: {}, {} positive examples, {} negative examples'.format(df.shape, positive_examples, negative_examples))

if __name__=='__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)
    args, _ = parser.parse_known_args()
    
    print('Received arguments {}'.format(args))

    input_data_path = os.path.join('/opt/ml/processing/input', 'census-income.csv')
    
    print('Reading input data from {}'.format(input_data_path))
    df = pd.read_csv(input_data_path)
    df = pd.DataFrame(data=df, columns=columns)
    df.dropna(inplace=True)
    df.drop_duplicates(inplace=True)
    df.replace(class_labels, [0, 1], inplace=True)
    
    negative_examples, positive_examples = np.bincount(df['income'])
    print('Data after cleaning: {}, {} positive examples, {} negative examples'.format(df.shape, positive_examples, negative_examples))
    
    split_ratio = args.train_test_split_ratio
    print('Splitting data into train and test sets with ratio {}'.format(split_ratio))
    X_train, X_test, y_train, y_test = train_test_split(df.drop('income', axis=1), df['income'], test_size=split_ratio, random_state=0)

    preprocess = make_column_transformer(
        (['age', 'num persons worked for employer'], KBinsDiscretizer(encode='onehot-dense', n_bins=10)),
        (['capital gains', 'capital losses', 'dividends from stocks'], StandardScaler()),
        (['education', 'major industry code', 'class of worker'], OneHotEncoder(sparse=False))
    )
    print('Running preprocessing and feature engineering transformations')
    train_features = preprocess.fit_transform(X_train)
    test_features = preprocess.transform(X_test)
    
    print('Train data shape after preprocessing: {}'.format(train_features.shape))
    print('Test data shape after preprocessing: {}'.format(test_features.shape))
    
    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_features.csv')
    train_labels_output_path = os.path.join('/opt/ml/processing/train', 'train_labels.csv')
    
    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_features.csv')
    test_labels_output_path = os.path.join('/opt/ml/processing/test', 'test_labels.csv')
    
    print('Saving training features to {}'.format(train_features_output_path))
    pd.DataFrame(train_features).to_csv(train_features_output_path, header=False, index=False)
    
    print('Saving test features to {}'.format(test_features_output_path))
    pd.DataFrame(test_features).to_csv(test_features_output_path, header=False, index=False)
    
    print('Saving training labels to {}'.format(train_labels_output_path))
    y_train.to_csv(train_labels_output_path, header=False, index=False)
    
    print('Saving test labels to {}'.format(test_labels_output_path))
    y_test.to_csv(test_labels_output_path, header=False, index=False)
311/54: write_dataframe_to_s3("df", "My_project")
311/55:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

region = boto3.session.Session().region_name

role = get_execution_role()
sklearn_processor = SKLearnProcessor(framework_version='0.20.0',
                                     role=role,
                                     instance_type='ml.m5.xlarge',
                                     instance_count=1)

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)


def preprocess_data():
311/56:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

region = boto3.session.Session().region_name

role = get_execution_role()
sklearn_processor = SKLearnProcessor(framework_version='0.20.0',
                                     role=role,
                                     instance_type='ml.m5.xlarge',
                                     instance_count=1)

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/57:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

region = boto3.session.Session().region_name

role = get_execution_role()
sklearn_processor = SKLearnProcessor(framework_version='0.20.0',
                                     role=role,
                                     instance_type='ml.m5.xlarge',
                                     instance_count=1)

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/58:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/59: write_dataframe_to_s3("df", "My_project")
311/60: write_dataframe_to_s3("df", "My_project")
311/61: write_dataframe_to_s3("df", "My_project")
311/62: write_dataframe_to_s3("df", "My_project")
311/63:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    
    return location


def write_as_csv(df, location):
    print("")

    
def write_as_parquet(df, location):
    print("")
    

def pre_process_data():
    region = boto3.session.Session().region_name
    role = get_execution_role()
    sklearn_processor = SKLearnProcessor(framework_version='0.20.0',
                                         role=role,
                                         instance_type='ml.m5.xlarge',
                                         instance_count=1)
311/64: write_dataframe_to_s3("df", "My_project")
311/65:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    
    return location


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
    

def pre_process_data():
    region = boto3.session.Session().region_name
    role = get_execution_role()
    sklearn_processor = SKLearnProcessor(framework_version='0.20.0',
                                         role=role,
                                         instance_type='ml.m5.xlarge',
                                         instance_count=1)
311/66: write_dataframe_to_s3("df", "My_project")
311/67: write_dataframe_to_s3("df", "My_project")
311/68:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET", channel="train"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + channel + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    
    return location


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
    

def pre_process_data():
311/69:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET", channel="train"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + channel + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    
    return location


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/70: write_dataframe_to_s3("df", "My_project")
311/71: write_dataframe_to_s3("df", "My_project")
311/72:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, file_format="PARQUET", channel):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + channel + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    
    return location


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/73: write_dataframe_to_s3("df", "My_project")
311/74:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, channel, file_format="PARQUET"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + channel + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    
    return location


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
311/75: write_dataframe_to_s3("df", "My_project")
311/76: write_dataframe_to_s3("df", "My_project", "test")
311/77:
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

BIG_MODELING_BUCKET = 'BIG_MODELING_BUCKET'

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df
    
    
def write_dataframe_to_s3(df, project_name, channel, file_format="PARQUET"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + channel + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    return location


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)
    

    
def fit_model(train_path, test_path, hyperparameters):
    sklearn_estimator = SKLearn('sklearn-train.py',
                                train_instance_type='ml.m4.xlarge',
                                framework_version='0.22.1',
                                hyperparameters = {'epochs': 20, 'batch-size': 64, 'learning-rate': 0.1})
    sklearn_estimator.fit({'train': 's3://my-data-bucket/path/to/my/training/data',
                            'test': 's3://my-data-bucket/path/to/my/test/data'})
311/78: write_dataframe_to_s3("df", "My_project", "test")
311/79: write_dataframe_to_s3("df", "My_project", "test")
311/80:
%%writefile ml_helper.py
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor

def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database=None, params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_s3(bucket, key):
    s3 = boto3.client('s3')
    obj = s3.get_object(Bucket=bucket, Key=key)
    #pd.read_csv(obj['Body'])
    return obj
    
    
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + channel + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    return location


def write_as_csv(df, location):
    print(location)

    
def write_as_parquet(df, location):
    print(location)   

def __init__:
    print("Initialization..")
313/1: from ml_helper import read_from_vertica
313/2: from ml_helper import read_from_vertica
313/3: from ml_helper import read_from_vertica
313/4: from ml_helper import read_from_s3
313/5: read_from_s3("sagemaker-us-west-2-458377512243", "/choice-mlflow-input/data/iris.csv")
313/6: read_from_s3("sagemaker-us-west-2-458377512243", "choice-mlflow-input/data/iris.csv")
315/1:
# Read from Vertica
SQL_QUERY_TO_FETCH_DATA = "select * from MDA.on_training1 limit 10"
df = read_from_vertica("dbadmin", "Chuz9puB", SQL_QUERY_TO_FETCH_DATA)
df.head()
315/2:
from ml_helper import read_from_vertica
from ml_helper import read_from_redshift
from ml_helper import read_from_s3
315/3:
# Read from Vertica
SQL_QUERY_TO_FETCH_DATA = "select * from MDA.on_training1 limit 10"
df = read_from_vertica("dbadmin", "Chuz9puB", SQL_QUERY_TO_FETCH_DATA)
df.head()
315/4:
SQL_QUERY_TO_FETCH_DATA = "select * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10;"
df = read_from_vertica("big_etl", "swl9@DricRotRuqi!tep", SQL_QUERY_TO_FETCH_DATA)
df.head()
315/5:
SQL_QUERY_TO_FETCH_DATA = "select * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10;"
df = read_from_redshift("big_etl", "swl9@DricRotRuqi!tep", SQL_QUERY_TO_FETCH_DATA)
df.head()
316/1:
SQL_QUERY_TO_FETCH_DATA = "select * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10;"
df = read_from_redshift("big_etl", "swl9@DricRotRuqi!tep", SQL_QUERY_TO_FETCH_DATA)
df.head()
316/2:
from ml_helper import read_from_vertica
from ml_helper import read_from_redshift
from ml_helper import read_from_s3
316/3:
SQL_QUERY_TO_FETCH_DATA = "select * from big_util_local.prd_omniture_hit_data_error_tracking_spark_test limit 10;"
df = read_from_redshift("big_etl", "swl9@DricRotRuqi!tep", SQL_QUERY_TO_FETCH_DATA)
df.head()
316/4:
# Read from Vertica
SQL_QUERY_TO_FETCH_DATA = "select * from MDA.on_training1 limit 10"
df = read_from_vertica("dbadmin", "Chuz9puB", SQL_QUERY_TO_FETCH_DATA)
df.head()
316/5:
from ml_helper import read_from_vertica
from ml_helper import read_from_redshift
from ml_helper import read_from_s3
import pandas as pd
316/6:
# Read from Vertica
SQL_QUERY_TO_FETCH_DATA = "select * from MDA.on_training1 limit 10"
df = read_from_vertica("my_username", "password", SQL_QUERY_TO_FETCH_DATA)
df.head()
316/7:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    dateTimeObj = datetime.now()
    timestampStr = dateTimeObj.strftime("%m%d%Y%H%M")
    location = BIG_MODELING_BUCKET + "/" + project_name + "/" + channel + "/" + timestampStr
    if file_format.upper() == "CSV":
        write_as_csv(df, location + ".csv")
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, location + ".parquet")
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    return location
316/8:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    dateTimeObj = datetime.now()
    print(dateTimeObj)
316/9: write_dataframe_to_s3("df", "oversell", "train", "CSV")
316/10:
from ml_helper import write_dataframe_to_s3
from datetime import datetime
316/11:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    dateTimeObj = datetime.now()
    print(dateTimeObj)
316/12: write_dataframe_to_s3("df", "oversell", "train", "CSV")
316/13:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    dateTimeObj = datetime.now().strftime("%m%d%Y%H%M")
    print(dateTimeObj)
316/14: write_dataframe_to_s3("df", "oversell", "train", "CSV")
316/15:
from ml_helper import write_dataframe_to_s3
from datetime import datetime
316/16:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    dateTimeObj = datetime.now().strftime("%m%d%Y%H%M")
    print(dateTimeObj)
316/17: write_dataframe_to_s3("df", "oversell", "train", "CSV")
316/18: write_dataframe_to_s3("df", "oversell", "train", "CSV")
316/19:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    dateTimeObj = datetime.now().strftime("%m%d%Y%H%M")
    print(dateTimeObj)
316/20: write_dataframe_to_s3("df", "oversell", "train", "CSV")
316/21:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = project_name + "/" + channel + channel +"_" + datetimestr + "." + file_format
    print(key)
316/22: write_dataframe_to_s3("df", "oversell", "train", "CSV")
316/23:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = project_name + "/" + channel + "/" + channel +"_" + datetimestr + "." + file_format
    print(key)
316/24: write_dataframe_to_s3("df", "oversell", "train", "CSV")
316/25:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel, channel, datetimestr, file_format)
    print(key)
316/26: write_dataframe_to_s3("df", "oversell", "train", "CSV")
316/27: write_dataframe_to_s3("df", "oversell", "train", "PARQUET")
316/28:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel, channel, datetimestr, file_format.toLower())
    print(key)
316/29: write_dataframe_to_s3("df", "oversell", "train", "PARQUET")
316/30:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel, channel, datetimestr, file_format.lower())
    print(key)
316/31: write_dataframe_to_s3("df", "oversell", "train", "PARQUET")
316/32:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print(key)
316/33: write_dataframe_to_s3("df", "oversell", "TRAIN", "PARQUET")
316/34: write_dataframe_to_s3("df", "oversell", "TRAIN", "PARQUET")
316/35:
def write_dataframe_to_s3(df, project_name, channel, file_format="CSV"):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    if file_format.upper() == "CSV":
        write_as_csv(df, key, BUCKET_NAME)
    elif file_format.upper() == "PARQUET":
        write_as_parquet(df, key, BUCKET_NAME)
    else:
        raise ValueError(f"Given file_format: {file_format} is not supported!!")
    return key
316/36: write_dataframe_to_s3("df", "oversell", "TRAIN", "PARQUET")
317/1:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
317/2:
aws_secret_access_key= "dP8dt58ncOH6gs7zsD7fjAxH6EtfgDMCHoekTQ7l"
aws_access_key_id = "AKIAWVOLNYUZYVGHZ5FX"
317/3:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv", header=True)
df.head()
317/4:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df.head()
317/5:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_dataframe_to_s3(df, "testing", "train")
317/6:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/7:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/8:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/9:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
import boto3
317/10:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
import boto3
317/11:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_dataframe_to_s3(df, "testing", "train")
317/12:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/13:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", region_name="us-west-2", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/14:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", region_name="us-west-2", aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/15:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", region_name="us-west-2", aws_access_key_id = "AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key = "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")
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/16:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", region_name="us-west-2", aws_access_key_id = "AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key = "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")
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/17:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", region_name="us-west-2", aws_access_key_id = "AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key='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')
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/18:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", region_name="us-west-2", aws_access_key_id = "AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key='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')
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/19:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3", region_name="us-west-2", aws_access_key_id = "AKIAWVOLNYUZYVGHZ5FX")
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/20:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3")
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/21:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
import boto3
317/22:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)         
    s3 = boto3.client("s3")
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/23:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)  
    session = boto3.Session(aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX",aws_secret_access_key="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")
    s3 = session.resource('s3')
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/24:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)  
    session = boto3.Session(aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    s3 = session.resource('s3')
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/25:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)  
    session = boto3.Session(aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    s3 = session.resource('s3')
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/26:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key)
317/27:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key))
    session = boto3.Session(aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    s3 = session.resource('s3')
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
317/28:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_dataframe_to_s3(df, "testing", "train")
317/29:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/1:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key))
    session = boto3.Session(aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    s3 = session.resource('s3')
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
318/2:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/3:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
import boto3
318/4:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, file_format.lower())
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key))
    session = boto3.Session(aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    s3 = session.resource('s3')
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
318/5:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/6:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/7:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), bucket_name, key))
318/8:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/9:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
318/10:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/11:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/12:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    session = boto3.Session(aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    s3 = session.resource('s3')
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
318/13:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/14:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
318/15:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/16:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/17:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
318/18:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    response = s3.put_object(Body=df, Bucket=BUCKET_NAME, Key=key)
    print(response)
318/19:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/20:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    s3.Object(BUCKET_NAME, key).put(Body=csv_buffer.getvalue())
    print(response)
318/21:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/22:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
import boto3
import StringIO
318/23:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
import boto3
from io import StringIO
318/24:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    s3.Object(BUCKET_NAME, key).put(Body=csv_buffer.getvalue())
    print(response)
318/25:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/26:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=key)
    print(response)
318/27:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/28:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=key)
    print(response)
318/29:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/30:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}_{}.{}".format(project_name, channel.lower(), channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=key)
    print(response)
318/31:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/32:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}".format(project_name, channel.lower())
    if delete_exist_files :
        print("Deleting existing files under {}/{}".format())
318/33:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/34:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}".format(project_name, channel.lower())
    if delete_exist_files :
        print("testing..")
318/35:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/36:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}".format(project_name, channel.lower())
318/37:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/38:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}".format(project_name, channel.lower())
318/39:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/40:
def write_df_to_s3_csv(df, project_name, channel, sep=","):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}/{}".format(project_name, channel.lower())
318/41:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/42:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/43: df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
318/44:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format())
318/45:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
318/46:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format())
318/47:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/48:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
318/49:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/50:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())

    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)
318/51:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/52:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/53:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=key)
    print(response)
318/54:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/55:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, key))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=key)
    print(response)
318/56:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/57:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=key)
    print(response)
318/58:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=file_location)
    print(response)
318/59:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train", delete_exist_files=True)
318/60:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
len(df)
318/61:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
df = df.append(df).append(df)
len(df)
318/62:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
df = df.append(df).append(df).append(df).append(df)
write_dataframe_to_s3(df, "testing", "train")
318/63:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3:{}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=file_location)
    print(response)
318/64:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/65:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/66:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    
    records_cnt = len(df)
        
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    if records_cnt > 200000 :
        local_file_path = "./{}".format(file_name)
        # Use multipart upload
        print("Writing {} records to local directory for multipart upload to S3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
        df.to_csv(local_file_path, sep=sep, index=False)
        print("Uploading {} file to S3 - {}/{}".format(file_name, BUCKET_NAME, path))
        s3.upload_file(local_file_path, BUCKET_NAME, path)
        os.remove(local_file_path)
        print("File uploaded to S3 & removed from local directory..")
        
    else:
        print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, sep=sep, index=False)
        response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=path)
        print(response)
318/67:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/68:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/69:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    
    records_cnt = len(df)
        
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    if records_cnt > 200000 :
        local_file_path = "./{}".format(file_name)
        # Use multipart upload
        print("Writing {} records to local directory for multipart upload to S3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
        df.to_csv(local_file_path, sep=sep, index=False)
        print("Uploading {} file to S3 - {}/{}".format(file_name, BUCKET_NAME, path))
        s3.upload_file(local_file_path, BUCKET_NAME, path)
        os.remove(local_file_path)
        print("File uploaded to S3 & removed from local directory..")
        
    else:
        print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, path))
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, sep=sep, index=False)
        response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=path)
        print(response)
318/70:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/71:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    
    records_cnt = len(df)
        
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    if records_cnt > 200000 :
        local_file_path = "./{}".format(file_name)
        # Use multipart upload
        print("Writing {} records to local directory for multipart upload to S3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
        df.to_csv(local_file_path, sep=sep, index=False)
        print("Uploading {} file to S3 - {}/{}".format(file_name, BUCKET_NAME, path))
        s3.upload_file(local_file_path, BUCKET_NAME, path)
        os.remove(local_file_path)
        print("File uploaded to S3 & removed from local directory..")
        
    else:
        print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, path))
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, sep=sep, index=False)
        response = s3.put_object(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=path)
        print("File uploaded to S3..")
318/72:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/73:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(Body=csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/74:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/75:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/76:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/77:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(io.BytesIO(buff.getvalue().encode()), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/78:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(io.BytesIO(buff.getvalue().encode()), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/79:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/80:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
import boto3
from io import StringIO
from io import BytesIO
import os
318/81:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/82:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(BytesIO(buff.getvalue().encode()), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/83:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/84:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(BytesIO(csv_buffer.getvalue().encode()), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/85:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/86:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = BytesIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer, Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/87:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/88:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = BytesIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer.getValue(), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/89:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/90:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = BytesIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    print(csv_buffer.getValue())
    response = s3.upload_fileobj(csv_buffer.getValue(), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/91:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/92:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = BytesIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
318/93:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/94:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    print(csv_buffer.getValue())
    response = s3.upload_fileobj(csv_buffer.getValue(), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/95:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/96:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    print(csv_buffer)
    response = s3.upload_fileobj(csv_buffer.getValue(), Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/97:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/98:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    BUCKET_NAME = "choice-mlflow-input"
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    file_location = "{}/{}_{}.{}".format(key, channel.lower(), datetimestr, "csv")
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer, Bucket=BUCKET_NAME, Key=file_location)
    print("File uploaded to S3..")
318/99:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
318/100:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
319/1:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
import boto3
from io import StringIO
from io import BytesIO
import os
from boto3.s3.transfer import TransferConfig
319/2:
from ml_helper import write_dataframe_to_s3
from datetime import datetime 
import pandas as pd
import boto3
from io import StringIO
from io import BytesIO
import os
from boto3.s3.transfer import TransferConfig
319/3:
from datetime import datetime 
import pandas as pd
import boto3
from io import StringIO
from io import BytesIO
import os
from boto3.s3.transfer import TransferConfig
319/4:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    GB = 1024 ** 3
    config = TransferConfig(multipart_threshold=1*GB)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, path))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer, Bucket=BUCKET_NAME, Key=path, Config=config)
    print("File uploaded to S3..")
319/5:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
df = df.append(df).append(df).append(df).append(df)
write_df_to_s3_csv(df, "testing", "train")
319/6:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
write_df_to_s3_csv(df, "testing", "train")
319/7:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, path))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer, Bucket=BUCKET_NAME, Key=path, Config=config)
    print("File uploaded to S3..")
319/8:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
write_df_to_s3_csv(df, "testing", "train")
319/9:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
319/10:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, path))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=path, Config=config)
    print("Uploaded to S3..")
319/11:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
319/12:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, path))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer.getvalue(), Bucket=BUCKET_NAME, Key=path, Config=config)
    print("Uploaded to S3..")
319/13:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
319/14:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, path))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer.seek(0), Bucket=BUCKET_NAME, Key=path, Config=config)
    print("Uploaded to S3..")
319/15:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
320/1:
from datetime import datetime 
import pandas as pd
import boto3
from io import StringIO
from io import BytesIO
import os
from boto3.s3.transfer import TransferConfig
320/2:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
        
    print("Writing {} records to s3 - {}/{}".format(len(df), BUCKET_NAME, path))
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, sep=sep, index=False)
    response = s3.upload_fileobj(csv_buffer.seek(0), Bucket=BUCKET_NAME, Key=path, Config=config)
    print("Uploaded to S3..")
320/3:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
320/4:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to replace with upload_fileobj)
    print("Writing {} records to local directory for multipart upload to S3 - {}/{}".format(len(df), BUCKET_NAME, file_location))
    df.to_csv(local_file_path, sep=sep, index=False)
320/5:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
320/6:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to replace with upload_fileobj)
    print("Writing {} records to local directory for multipart upload to S3 - {}/{}".format(len(df), BUCKET_NAME, path))
    df.to_csv(local_file_path, sep=sep, index=False)
320/7:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to replace with upload_fileobj)
    print("Writing {} records to local directory for multipart upload to S3 - {}/{}".format(len(df), BUCKET_NAME, path))
    df.to_csv(local_file_path, sep=sep, index=False)
320/8:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
320/9:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to replace with upload_fileobj)
    print("Writing {} records to local directory for multipart upload to S3 - {}/{}".format(len(df), BUCKET_NAME, path))
    df.to_csv(local_file_path, sep=sep, index=False)
    print("Uploaded {} file to S3 ".format(file_name))
320/10:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
320/11:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to replace with upload_fileobj)
    print("Writing {} records to local directory for multipart upload to S3 - {}/{}".format(len(df), BUCKET_NAME, path))
    df.to_csv(local_file_path, sep=sep, index=False)
    print("Uploaded {} file to S3 ".format(file_name))
320/12:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
320/13:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to use file object type with upload_fileobj)
    print("Writing {} records to S3 - {}/{}".format(len(df), BUCKET_NAME, path))
    df.to_csv(local_file_path, sep=sep, index=False)
    os.remove(local_file_path)
    print("Uploaded {} file to S3 ".format(file_name))
320/14:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
320/15:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to use file object type with upload_fileobj)
    print("Writing {} records to S3 - {}/{}".format(len(df), BUCKET_NAME, path))
    df.to_csv(local_file_path, sep=sep, index=False)
    os.remove(local_file_path)
    print("Uploaded {} file to S3 ".format(file_name))
320/16:
df = pd.read_csv("/Users/ukannika/Desktop/test.csv")
write_df_to_s3_csv(df, "testing", "train")
320/17:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to use file object type with upload_fileobj)
    print("Writing {} records to S3 - {}/{}".format(len(df), BUCKET_NAME, path))
    df.to_csv(local_file_path, sep=sep, index=False)
    print("Uploaded {} file to S3 ".format(file_name))
320/18:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
write_df_to_s3_csv(df, "testing", "train")
320/19:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to use file object type with upload_fileobj)
    print("Writing {} records to S3 - {}/{}".format(len(df), BUCKET_NAME, path))
    df.to_csv(local_file_path, sep=sep, index=False)
    s3.upload_file(local_file_path, BUCKET_NAME, path)
    print("Uploaded {} file to S3 ".format(file_name))
320/20:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=1*1024 * 1024)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to use file object type with upload_fileobj)
    print("Writing {} records to S3 - {}/{} Started at: {}".format(len(df), BUCKET_NAME, path, datetime.now()))
    df.to_csv(local_file_path, sep=sep, index=False)
    s3.upload_file(local_file_path, BUCKET_NAME, path)
    print("Uploaded {} file to S3 at {}".format(file_name, datetime.now()))
320/21:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
write_df_to_s3_csv(df, "testing", "train")
320/22:
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=50000)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to use file object type with upload_fileobj)
    print("Writing {} records to S3 - {}/{} Started at: {}".format(len(df), BUCKET_NAME, path, datetime.now()))
    df.to_csv(local_file_path, sep=sep, index=False)
    s3.upload_file(local_file_path, BUCKET_NAME, path)
    print("Uploaded {} file to S3 at {}".format(file_name, datetime.now()))
320/23:
df = pd.read_csv("/Users/ukannika/Desktop/cbg_b25.csv")
write_df_to_s3_csv(df, "testing", "train")
320/24: from ml_helper import write_df_to_s3_csv
323/1:
import pandas as pd
import os

from sklearn import tree
from sklearn.externals import joblib
323/2: from sklearn.datasets import load_boston
323/3:
from sklearn.datasets import load_boston
df = load_boston()
df.head()
323/4:
from sklearn.datasets import load_boston
df = load_boston()
df.head()
323/5:
from sklearn.datasets import load_boston
df = load_boston()
df.__class__
323/6:
from sklearn.datasets import load_boston
df = load_boston().datasets
df.__class__
323/7:
from sklearn.datasets import load_boston
df = load_boston().data
df.__class__
323/8:
from sklearn.datasets import load_boston
array = load_boston().data
numpy.savetxt("boston.csv", array, delimiter=",")
323/9:
from sklearn.datasets import load_boston
array = load_boston().data
np.savetxt("boston.csv", array, delimiter=",")
323/10:
import pandas as pd
import os

from sklearn import tree
from sklearn.externals import joblib
import numpy as np
323/11:
from sklearn.datasets import load_boston
array = load_boston().data
np.savetxt("boston.csv", array, delimiter=",")
323/12:
from sklearn.datasets import load_boston
array = load_boston().data
np.savetxt("boston.csv", array, delimiter=",")
323/13:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})
array = load_boston().data
np.savetxt("boston.csv", array, delimiter=",")
323/14:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})
array = load_boston().data
array
323/15:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})
array = load_boston().data
np.savetxt("boston.csv", array, fmt='%.3e', delimiter=",")
323/16:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})
array = load_boston().data
np.savetxt("boston.csv", array, fmt='0:0.3f', delimiter=",")
323/17:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
array = load_boston().data
np.savetxt("boston.csv", array, fmt='0:0.3f', delimiter=",")
323/18:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
array = load_boston().data
np.savetxt("boston.csv", array, fmt='%d', delimiter=",")
323/19:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
array = load_boston().data
np.savetxt("boston.csv", array, fmt='%f', delimiter=",")
323/20: from sklearn.gaussian_process import GaussianProcessRegressor
323/21:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
323/22:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
data = load_boston().data
targets = load_boston().target
targets
323/23:
from sklearn.datasets import load_boston
data = load_boston().data
targets = load_boston().target
targets
323/24:
from sklearn.datasets import load_boston
data = load_boston().data
targets = load_boston().target
targets
325/1:
from sklearn.datasets import load_boston
data = load_boston().data
targets = load_boston().target
targets
325/2:
from sklearn.datasets import load_boston
data = load_boston().data
targets = load_boston().target
targets.shape()
325/3:
from sklearn.datasets import load_boston
data = load_boston().data
targets = load_boston().target
targets.shape
325/4:
from sklearn.datasets import load_boston
data = load_boston().data
targets = load_boston().target
targets
325/5:
from sklearn.datasets import load_boston
data = load_boston().data
targets = load_boston().target
np.column_stack((data, targets))
325/6:
import pandas as pd
import os

from sklearn import tree
from sklearn.externals import joblib
import numpy as np
325/7:
from sklearn.datasets import load_boston
data = load_boston().data
targets = load_boston().target
np.column_stack((data, targets))
325/8:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
data = load_boston().data
targets = load_boston().target
df = np.column_stack((data, targets))
np.savetxt("boston.csv", df, fmt='%f', delimiter=",")
325/9:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
data = load_boston().data
targets = load_boston().target
df = np.column_stack((data, targets))
np.savetxt("boston.csv", df, fmt='%f', delimiter=",")
325/10:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
data = load_boston().data
targets = load_boston().target
data.shape
targets.shape
df = np.column_stack((data, targets))
df.shape
np.savetxt("boston.csv", df, fmt='%f', delimiter=",")
325/11:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
data = load_boston().data
targets = load_boston().target
print(data.shape)
print(targets.shape)
df = np.column_stack((data, targets))
print(df.shape)
np.savetxt("boston.csv", df, fmt='%f', delimiter=",")
325/12:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd
325/13:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning")
df.head(n=10)
325/14:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning")
df.head(10)
325/15:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning")
df.head()
325/16:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning", engine='python')
df.head()
325/17:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python')
df.head()
325/18:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
df.head()
325/19:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
# Instantiate a Gaussian Process model
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
325/20:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
training_data = df.to_numpy()
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
325/21:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
training_data = df.to_numpy()
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
325/22:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
print(df.__class__)
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
325/23:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
print(df.to_numpy())
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
326/1:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
print(df.to_numpy())
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
326/2:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
print(df.to_numpy())
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/1:
import pandas as pd
import os

from sklearn import tree
from sklearn.externals import joblib
import numpy as np
327/2:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
print(df.to_numpy())
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/3:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
training_data = df.to_numpy()
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/4:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
data[:, 1].head()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/5:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
print(data[:, 1])

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/6:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
print(data[:, 10])

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/7:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
print(data[:, 14])

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/8:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
print(data[:, 13])

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/9:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
print(data[:, 12])

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/10:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
X = data[:, 12]
Y = data[:, 12:]

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/11:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
X = data[:, 12]
Y = data[:, 12:]

print(Y)
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/12:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
X = data[:, 12]
Y = data[:, 13:]

print(Y)
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/13:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()
X = data[:, 12]
Y = data[:, 13:]


print(Y)
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/14:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, 12]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

train_array = np.column_stack((X_train, X_test))
327/15:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, 12]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

X_train.shape
327/16:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, 12]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

Y_train.
327/17:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, 12]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

Y_train.shape
327/18:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, 12]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

y_train.shape
327/19:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, 12]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

X_train
327/20:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, 12]
Y = data[:, 13:]

X.shape
327/21:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :12]
Y = data[:, 13:]

X.shape
327/22:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :12]
Y = data[:, 13:]

y.shape
327/23:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :12]
Y = data[:, 13:]

Y.shape
327/24:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

Y.shape
327/25:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

data.shape
X = data[:, :13]
Y = data[:, 13:]

Y.shape
327/26:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

print(data.shape)
X = data[:, :13]
Y = data[:, 13:]

Y.shape
327/27:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

print(data.shape)
print(X.shape)
print(Y.shape)
327/28:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X
327/29:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

Y
327/30:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)
327/31:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

train = np.column_stack((X_train, y_train))
327/32:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

train = np.column_stack((X_train, y_train))
train.shape
327/33:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

train = np.column_stack((X_train, y_train))
np.savetxt("boston_train.csv", df, fmt='%f', delimiter=",")
327/34:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

train = np.column_stack((X_test, y_test))
np.savetxt("boston_test.csv", df, fmt='%f', delimiter=",")
327/35:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
data = load_boston().data
targets = load_boston().target
print(data.shape)
print(targets.shape)
df = np.column_stack((data, targets))
print(df.shape)
np.savetxt("boston.csv", df, fmt='%f', delimiter=",")
327/36:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

train = np.column_stack((X_test, y_test))
np.savetxt("boston_test.csv", df, fmt='%f', delimiter=",")
328/1:
%%writefile ml_helper.py
import pandas as pd
import sqlalchemy as sa
from sqlalchemy import create_engine
import boto3
from io import StringIO
from datetime import datetime
import boto3
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.processing import SKLearnProcessor


def read_from_vertica(username, password, sql_query, database=None, params=None, parse_dates=None):
    vertica_url = f"vertica+vertica_python://{username}:{password}@vertica.chotel.com:5433" + ("" if not database else "/" + database)
    engine = sa.create_engine(vertica_url)
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_redshift(username, password, sql_query, database="dap", params=None, parse_dates=None):
    redshift_url = f"redshift+psycopg2://{username}:{password}@dap-redshift-proxy.prod.aws.chotel.com:{5439}" + ("" if not database else "/" + database)
    engine = sa.create_engine(redshift_url, connect_args={'sslmode': 'prefer'})
    connection = engine.connect()
    df = pd.read_sql(sql=sql_query, con=connection, params=params, parse_dates=parse_dates)
    return df


def read_from_s3(bucket, key):
    s3 = boto3.client('s3')
    obj = s3.get_object(Bucket=bucket, Key=key)
    #pd.read_csv(obj['Body'])
    return obj
    
    
def write_df_to_s3_csv(df, project_name, channel, sep=",", delete_exist_files=False):
    BUCKET_NAME = "choice-mlflow-input"
    config = TransferConfig(multipart_threshold=50000)
    s3 = boto3.client('s3', aws_access_key_id="AKIAWVOLNYUZYVGHZ5FX", aws_secret_access_key="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")
    datetimestr = datetime.now().strftime("%m%d%Y%H%M")
    key = "{}/{}".format(project_name, channel.lower())
    file_name = "{}_{}.csv".format(channel.lower(), datetimestr)
    path = "{}/{}".format(key, file_name)
    local_file_path = "./{}".format(file_name)
            
    if delete_exist_files:
        print("Deleting existing files under {}/{}".format(BUCKET_NAME, key))
        s3.delete_object(Bucket=BUCKET_NAME, Key=key)   
    
    # Use multipart upload (This may change in future to use file object type with upload_fileobj)
    print("Writing {} records to S3 - {}/{} Started at: {}".format(len(df), BUCKET_NAME, path, datetime.now()))
    df.to_csv(local_file_path, sep=sep, index=False)
    s3.upload_file(local_file_path, BUCKET_NAME, path)
    os.remove(local_file_path)
    print("Uploaded {} file to S3 at {}".format(file_name, datetime.now()))
327/37:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
data = load_boston().data
targets = load_boston().target
print(data.shape)
print(targets.shape)
df = np.column_stack((data, targets))
print(df.shape)
np.savetxt("boston.csv", df, fmt='%f', delimiter=",")
327/38:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

train = np.column_stack((X_train, y_train))
np.savetxt("boston_test.csv", df, fmt='%f', delimiter=",")
327/39:
from sklearn.datasets import load_boston
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
data = load_boston().data
targets = load_boston().target
print(data.shape)
print(targets.shape)
df = np.column_stack((data, targets))
print(df.shape)
np.savetxt("boston.csv", df, fmt='%f', delimiter=",")
327/40:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

train = np.column_stack((X_train, y_train))
np.savetxt("boston_train.csv", df, fmt='%f', delimiter=",")
327/41:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

df = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston.csv", engine='python', header=None)
data = df.to_numpy()

X = data[:, :13]
Y = data[:, 13:]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)

train = np.column_stack((X_test, y_test))
np.savetxt("boston_test.csv", df, fmt='%f', delimiter=",")
327/42:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None)
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None)
327/43:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None)
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None)

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X = training_data.to_numpy()
327/44:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None)
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None)

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X = training_data.to_numpy()
327/45:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
327/46:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]


gp.fit(X_train, y_train)
327/47:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

print(X_train.shape)
print(Y_train.shape)
327/48:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)
327/49:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma, cov = gp.predict(X_test, return_cov=True, return_std=True)
327/50:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma, cov = gp.predict(X_test, return_cov=True)
327/51:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, cov = gp.predict(X_test, return_cov=True)
327/52: print(cov)
329/1:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, cov = gp.predict(X_test, return_cov=True)
329/2: print(cov)
329/3: print(y_pred)
329/4: print(y_pred, sigma)
329/5: print(np.column_stack((y_pred, sigma))
329/6: print(np.column_stack((y_pred, sigma)))
329/7:
import pandas as pd
import os

from sklearn import tree
from sklearn.externals import joblib
import numpy as np
329/8: print(np.column_stack((y_pred, sigma)))
329/9:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
329/10: print(np.column_stack((y_pred, sigma)))
329/11:
np.set_printoptions(formatter={'float': lambda x: "{0.3f}".format(x)})
print(np.column_stack((y_pred, sigma)))
329/12: print(np.column_stack((y_pred, sigma)))
330/1:
import pandas as pd
import os

from sklearn import tree
from sklearn.externals import joblib
import numpy as np
330/2:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/3: print(np.column_stack((y_pred, sigma)))
330/4: print(np.column_stack((y_pred, np.round(sigma)))
330/5: print(np.column_stack((y_pred, np.round(sigma))))
330/6:
np.set_printoptions(threshold=sys.maxsize)
np.column_stack((y_pred, np.round(sigma)))
330/7:
import pandas as pd
import os

from sklearn import tree
from sklearn.externals import joblib
import numpy as np
import sys
330/8:
np.set_printoptions(threshold=sys.maxsize)
np.column_stack((y_pred, np.round(sigma)))
330/9:
np.set_printoptions(threshold=sys.maxsize)
np.column_stack((y_pred, sigma))
330/10:
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})

np.column_stack((y_pred, sigma))
330/11: gp.plot()
330/12:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = C(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/13:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/14:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/15:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = RBF(length_scale=1, length_scale_bounds=10)
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/16:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1e3))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/17:
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})

np.column_stack((y_pred, sigma))
330/18:
np.random.seed(1)


def f(x):
    """The function to predict."""
    return x * np.sin(x)

# ----------------------------------------------------------------------
#  First the noiseless case
X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T

# Observations
y = f(X).ravel()

# Mesh the input space for evaluations of the real function, the prediction and
# its MSE
x = np.atleast_2d(np.linspace(0, 10, 1000)).T

# Instantiate a Gaussian Process model
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)

# Fit to data using Maximum Likelihood Estimation of the parameters
gp.fit(X, y)

# Make the prediction on the meshed x-axis (ask for MSE as well)
y_pred, sigma = gp.predict(x, return_std=True)
330/19:
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})

np.column_stack((y_pred, sigma))
330/20: from sklearn.linear_model import Ridge
330/21:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = gp.kernels.ConstantKernel(1.0, (1e-1, 1e3)) * gp.kernels.RBF(10.0, (1e-3, 1e3))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/22:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split

import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = ConstantKernel(1.0, (1e-1, 1e3)) * RBF(10.0, (1e-3, 1e3))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/23:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = ConstantKernel(1.0, (1e-1, 1e3)) * RBF(10.0, (1e-3, 1e3))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/24:
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})

np.column_stack((y_pred, sigma))
330/25:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = ConstantKernel(1.0, (1e-1, 1e3)) * RBF(13.0, (1e-3, 1e3))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/26:
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})

np.column_stack((y_pred, sigma))
330/27:
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(formatter={'float': lambda x: "{0:0.3f}".format(x)})

np.column_stack((y_pred, sigma))
330/28:
np.set_printoptions(threshold=sys.maxsize)
np.column_stack((y_pred, sigma))
330/29:
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(formatter={'float': lambda x: "{0:0.5f}".format(x)})

np.column_stack((y_pred, sigma))
330/30:
np.set_printoptions(threshold=sys.maxsize)
np.column_stack((y_pred, sigma))
330/31:
np.set_printoptions(threshold=sys.maxsize)
np.set_printoptions(formatter={'float': lambda x: "{0:0.5f}".format(x)})
np.column_stack((y_pred, sigma))
330/32:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = ConstantKernel(1.0, (1e-1, 1e3)) * RBF(13.0, (1e-3, 1e3))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, clf.predict(X_test))
print("MSE: %.4f" % mse)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/33:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error¶


training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = ConstantKernel(1.0, (1e-1, 1e3)) * RBF(13.0, (1e-3, 1e3))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, clf.predict(X_test))
print("MSE: %.4f" % mse)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/34:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error


training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = ConstantKernel(1.0, (1e-1, 1e3)) * RBF(13.0, (1e-3, 1e3))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, clf.predict(X_test))
print("MSE: %.4f" % mse)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/35:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error


training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

kernel = ConstantKernel(1.0, (1e-1, 1e3)) * RBF(13.0, (1e-3, 1e3))
gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3)

X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

gp.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, gp.predict(X_test))
print("MSE: %.4f" % mse)

y_pred, sigma = gp.predict(X_test, return_std=True)
330/36:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

ridge = Ridge()
330/37:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

ridge = Ridge(alpha=0.1, max_iter=100, solver='sag')
X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

ridge.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, gp.predict(X_test))
print("MSE: %.4f" % mse)
330/38:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

ridge = Ridge(alpha=0.1, max_iter=10000, solver='sag')
X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

ridge.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, gp.predict(X_test))
print("MSE: %.4f" % mse)
330/39:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

ridge = Ridge(alpha=0.1, max_iter=10000, solver='sag')
X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

ridge.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, gp.predict(X_test))
print("MSE: %.4f" % mse)

np.array([0.006320,18.000000,2.310000,0.000000,0.538000,6.575000,65.200000,4.090000,1.000000,296.000000,15.300000,396.900000,4.980000]).shape
330/40:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

ridge = Ridge(alpha=0.1, max_iter=10000, solver='sag')
X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

ridge.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, ridge.predict(X_test))
print("MSE: %.4f" % mse)

test_point = np.array([0.006320,18.000000,2.310000,0.000000,0.538000,6.575000,65.200000,4.090000,1.000000,296.000000,15.300000,396.900000,4.980000]).shape
330/41:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

ridge = Ridge(alpha=0.1, max_iter=10000, solver='sag')
X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

ridge.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, ridge.predict(X_test))
print("MSE: %.4f" % mse)

test_point = np.array([0.006320,18.000000,2.310000,0.000000,0.538000,6.575000,65.200000,4.090000,1.000000,296.000000,15.300000,396.900000,4.980000]).shape
ridge.predict(test_point)
330/42:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

ridge = Ridge(alpha=0.1, max_iter=10000, solver='sag')
X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

ridge.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, ridge.predict(X_test))
print("MSE: %.4f" % mse)

X_test.shape()
330/43:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

ridge = Ridge(alpha=0.1, max_iter=10000, solver='sag')
X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

ridge.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, ridge.predict(X_test))
print("MSE: %.4f" % mse)

X_test.shape
330/44:
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.gaussian_process.kernels import ConstantKernel
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge

training_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_train.csv", engine='python', header=None).to_numpy()
test_data = pd.read_csv("/Users/ukannika/work/personal/machine-learning/boston_test.csv", engine='python', header=None).to_numpy()

ridge = Ridge(alpha=0.1, max_iter=10000, solver='sag')
X_train = training_data[:, :13]
Y_train = training_data[:, 13:]

X_test = test_data[:, :13]
Y_test = test_data[:, 13:]

ridge.fit(X_train, Y_train)
mse = mean_squared_error(Y_test, ridge.predict(X_test))
print("MSE: %.4f" % mse)


test_point = np.array([[0.006320,18.000000,2.310000,0.000000,0.538000,6.575000,65.200000,4.090000,1.000000,296.000000,15.300000,396.900000,4.980000]])
ridge.predict(test_point)
331/1:
def define_hyperparameters(parser):
     parser.add_argument('--alpha', type=float, default=0.1)
331/2:
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    define_hyperparameters(parser=parser)
331/3:
import pandas as pd
import os
from sklearn import tree
from sklearn.externals import joblib
import numpy as np
import sys
331/4:
import pandas as pd
import os
from sklearn import tree
from sklearn.externals import joblib
import numpy as np
import sys
331/5:
import argparse
import pandas as pd
import os

from sklearn import tree
from sklearn.externals import joblib
331/6:
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    define_hyperparameters(parser=parser)
331/7:
def define_hyperparameters(parser):
    # Hyperparameters are defined here. In this simple example we are just including one hyperparameter.
    parser.add_argument('--alpha', type=float, default=0.1)
        
def define_directories(parser):
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
331/8:
def define_hyperparameters(parser):
    # Hyperparameters are defined here. In this simple example we are just including one hyperparameter.
    parser.add_argument('--alpha', type=float, default=0.1)
        
def define_data_directories(parser):
    #A string representing the path to the directory to write model artifacts to. 
    #Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    
    #A string representing the filesystem path to write output artifacts to. 
    #Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts.
    #These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    
    #A string representing the path to the directory containing data in the 'train' channel
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    
    #A string representing the path to the directory containing data in the 'test' channel
    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
    

def
331/9:
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    define_hyperparameters(parser=parser)
    define_data_di
331/10:
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    define_hyperparameters(parser=parser)
    define_data_directories(parser=parser)
331/11:
def define_hyperparameters(parser):
    # Hyperparameters are defined here. In this simple example we are just including one hyperparameter.
    parser.add_argument('--alpha', type=float, default=0.1)
        
def define_data_directories(parser):
    #A string representing the path to the directory to write model artifacts to. 
    #Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    
    #A string representing the filesystem path to write output artifacts to. 
    #Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts.
    #These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    
    #A string representing the path to the directory containing data in the 'train' channel
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    
    #A string representing the path to the directory containing data in the 'test' channel
    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
331/12:
def define_hyperparameters(parser):
    # Hyperparameters are defined here. In this simple example we are just including one hyperparameter.
    parser.add_argument('--alpha', type=float, default=0.1)
        
def define_data_directories(parser):
    #A string representing the path to the directory to write model artifacts to. 
    #Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    
    #A string representing the filesystem path to write output artifacts to. 
    #Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts.
    #These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    
    #A string representing the path to the directory containing data in the 'train' channel
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    
    #A string representing the path to the directory containing data in the 'test' channel
    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
331/13:
import argparse
import pandas as pd
import os

from sklearn import tree
from sklearn.externals import joblib
331/14:
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    define_hyperparameters(parser=parser)
    define_data_directories(parser=parser)
331/15:
def define_hyperparameters(parser):
    # Hyperparameters are defined here. In this simple example we are just including one hyperparameter.
    parser.add_argument('--alpha', type=float, default=0.1)
        
def define_data_directories(parser):
    #A string representing the path to the directory to write model artifacts to. 
    #Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    
    #A string representing the filesystem path to write output artifacts to. 
    #Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts.
    #These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    
    #A string representing the path to the directory containing data in the 'train' channel
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    
    #A string representing the path to the directory containing data in the 'test' channel
    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])

def get_train_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]
    if len(input_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.train, "train"))
    raw_data = [ pd.read_csv(file, header=None, engine="python") for file in input_files ]
    train_data = pd.concat(raw_data)
    return train_data
        
def get_test_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_files = [ os.path.join(args.test, file) for file in os.listdir(args.train) ]
    if len(input_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.train, "train"))
    raw_data = [ pd.read_csv(file, header=None, engine="python") for file in input_files ]
    train_data = pd.concat(raw_data)
    return train_data
331/16:
def define_hyperparameters(parser):
    # Hyperparameters are defined here. In this simple example we are just including one hyperparameter.
    parser.add_argument('--alpha', type=float, default=0.1)
        
def define_data_directories(parser):
    #A string representing the path to the directory to write model artifacts to. 
    #Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    
    #A string representing the filesystem path to write output artifacts to. 
    #Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts.
    #These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    
    #A string representing the path to the directory containing data in the 'train' channel
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    
    #A string representing the path to the directory containing data in the 'test' channel
    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])

def get_train_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_training_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]
    if len(input_training_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.train, "train"))
    train_raw_data = [ pd.read_csv(file, header=None, engine="python") for file in input_training_files ]
    train_data = pd.concat(train_raw_data)
    return train_data
        
def get_test_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_testing_files = [ os.path.join(args.test, file) for file in os.listdir(args.test) ]
    if len(input_testing_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.test, "test"))
    test_raw_data = [ pd.read_csv(file, header=None, engine="python") for file in input_testing_files ]
    test_data = pd.concat(test_raw_data)
    return test_data
331/17:
import argparse
import pandas as pd
import os
from sklearn import tree
from sklearn.externals import joblib
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge
331/18:
import argparse
import pandas as pd
import os
from sklearn import tree
from sklearn.externals import joblib
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge
331/19:
def define_hyperparameters(parser):
    # Hyperparameters are defined here. In this simple example we are just including one hyperparameter.
    parser.add_argument('--alpha', type=float, default=0.1)
        
def define_data_directories(parser):
    #A string representing the path to the directory to write model artifacts to. 
    #Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    
    #A string representing the filesystem path to write output artifacts to. 
    #Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts.
    #These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    
    #A string representing the path to the directory containing data in the 'train' channel
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    
    #A string representing the path to the directory containing data in the 'test' channel
    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])

def get_train_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_training_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]
    if len(input_training_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.train, "train"))
    train_raw_data = [ pd.read_csv(file, header=None, engine="python") for file in input_training_files ]
    train_data = pd.concat(train_raw_data)
    return train_data
        
def get_test_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_testing_files = [ os.path.join(args.test, file) for file in os.listdir(args.test) ]
    if len(input_testing_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.test, "test"))
    test_raw_data = [ pd.read_csv(file, header=None, engine="python") for file in input_testing_files ]
    test_data = pd.concat(test_raw_data)
    return test_data

def train():
331/20:
%%writefile training_helper.py
def define_data_directories(parser):
    #A string representing the path to the directory to write model artifacts to. 
    #Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    
    #A string representing the filesystem path to write output artifacts to. 
    #Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts.
    #These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.
    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])
    
    #A string representing the path to the directory containing data in the 'train' channel
    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    
    #A string representing the path to the directory containing data in the 'test' channel
    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])
    
    #A string representing the path to the directory containing data in the 'validation' channel
    parser.add_argument('--validation', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])

    
def get_train_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_training_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]
    if len(input_training_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.train, "train"))
    train_raw_data = [ pd.read_csv(file, header=None, engine="python") for file in input_training_files ]
    train_data = pd.concat(train_raw_data)
    return train_data
        
    
def get_test_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_testing_files = [ os.path.join(args.test, file) for file in os.listdir(args.test) ]
    if len(input_testing_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.test, "test"))
    test_raw_data = [ pd.read_csv(file, header=None, engine="python") for file in input_testing_files ]
    test_data = pd.concat(test_raw_data)
    return test_data


def get_validation_data():
    # Take the set of files and read them all into a single pandas dataframe
    input_validation_files = [ os.path.join(args.validation, file) for file in os.listdir(args.validation) ]
    if len(input_validation_files) == 0:
        raise ValueError(('There are no files in {}.\n' +
                          'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                          'the data specification in S3 was incorrectly specified or the role specified\n' +
                          'does not have permission to access the data.').format(args.test, "validation"))
    validation_raw_data = [ pd.read_csv(file, header=None, engine="python") for file in input_testing_files ]
    validation_data = pd.concat(validation_raw_data)
    return validation_data
331/21:
from training_helper import get_validation_data
from training_helper import get_train_data
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge
331/22:
def define_hyperparameters(parser):
    # Hyperparameters are defined here. In this simple example we are just including one hyperparameter.
    parser.add_argument('--alpha', type=float, default=0.1)
        

def train():
    ridge = Ridge(alpha=0.1, max_iter=10000, solver='sag')
    training_data = get_train_data().to_numpy()
    validation_data = get_validation_data().to_numpy()
    X_train = training_data[:, :13]
    Y_train = training_data[:, 13:]
    
    X_validation = validation_data[:, :13]
    Y_validation = validation_data[:, 13:]

    ridge.fit(X_train, Y_train)
    mse = mean_squared_error(Y_validation, ridge.predict(X_validation))
    print("MSE: %.4f" % mse)
330/45: import os
336/1:
from datasource_helper import read_from_s3
from datasource_helper import read_from_redshift
from datasource_helper import read_from_vertica
from datasource_helper import write_df_to_s3_csv
from datasource_helper import print_file_locations
from datasource_helper import download_trained_model_from_s3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

from sagemaker.sklearn.estimator import SKLearn
from sagemaker import get_execution_role
import sagemaker
import boto3
336/2: project_name = "pricing_demo"
336/3: project_name = "pricing_demo"
336/4:
# Read data from Vertica
SQL_QUERY = "select * from MDA.on_training1 limit 10"
username = getpass.getpass()
password = getpass.getpass()
df = read_from_vertica(username=username, password=password, sql_query=SQL_QUERY)
df.head(1)
336/5:
from datasource_helper import read_from_s3
from datasource_helper import read_from_redshift
from datasource_helper import read_from_vertica
from datasource_helper import write_df_to_s3_csv
from datasource_helper import print_file_locations
from datasource_helper import download_trained_model_from_s3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

from sagemaker.sklearn.estimator import SKLearn
from sagemaker import get_execution_role
import sagemaker
import boto3
import getpass
336/6:
# Read data from Vertica
SQL_QUERY = "select * from MDA.on_training1 limit 10"
username = getpass.getpass()
password = getpass.getpass()
df = read_from_vertica(username=username, password=password, sql_query=SQL_QUERY)
df.head(1)
336/7:
# Read data from S3 
s3Obj = read_from_s3("choice-mlflow-input", "sagemaker-test/boston.csv")
df = pd.read_csv(s3Obj['Body'], header=None)
df.head(2)
336/8:
# Read data from Vertica
SQL_QUERY = "select * from MDA.on_training1 limit 10"
username = getpass.getpass()
password = getpass.getpass()
df = read_from_vertica(username=username, password=password, sql_query=SQL_QUERY)
df.head(1)
337/1:
from datasource_helper import read_from_s3
from datasource_helper import read_from_redshift
from datasource_helper import read_from_vertica
from datasource_helper import write_df_to_s3_csv
from datasource_helper import print_file_locations
from datasource_helper import download_trained_model_from_s3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

from sagemaker.sklearn.estimator import SKLearn
from sagemaker import get_execution_role
import sagemaker
import boto3
import getpass
337/2: project_name = "pricing_demo"
337/3:
# Read data from Vertica
SQL_QUERY = "select * from MDA.on_training1 limit 10"
username = getpass.getpass()
password = getpass.getpass()
df = read_from_vertica(username=username, password=password, sql_query=SQL_QUERY)
df.head(1)
338/1:
import matplotlib.pyplot as plt
import numpy as np
338/2:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
338/3:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
fig.show
339/1:
import matplotlib.pyplot as plt
import numpy as np
339/2:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
ax
339/3:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
339/4:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
print(ax_lst.__class__)
339/5:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
ax_lst.size
339/6:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
ax_lst.dim
339/7:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
ax_lst.shape()
339/8:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
ax_lst.shape
339/9:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
339/10:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
339/11:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
340/1:
x = np.linspace(0, 2, 100)
plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')
plt.xlabel('x label')
plt.ylabel('y label')
plt.title("Simple Plot")
plt.legend()
plt.show()
340/2:
import matplotlib.pyplot as plt
import numpy as np
340/3:
x = np.linspace(0, 2, 100)
plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')
plt.xlabel('x label')
plt.ylabel('y label')
plt.title("Simple Plot")
plt.legend()
plt.show()
340/4:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic1')

plt.xlabel('x label')
plt.ylabel('y label')
plt.title("Simple Plot")
plt.legend()
plt.show()
340/5:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")

plt.show()
340/6:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()

plt.show()
340/7:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
341/1:
import matplotlib.pyplot as plt
import numpy as np
341/2:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.show()
341/3:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
341/4:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.show()
341/5:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
342/1:
import matplotlib.pyplot as plt
import numpy as np
342/2:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is

fig, ax_lst = plt.subplots(2, 2)
342/3:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
342/4:
x = np.linspace(0, 2, 100)
print(x)
plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
342/5:
x = np.linspace(0, 2, 100)
print(x.shape)
plt.plot(x, x, label='linear')
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
343/1:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
343/2:
import matplotlib.pyplot as plt
import numpy as np
343/3:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
343/4: fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
343/5: fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
343/6:
import matplotlib.pyplot as plt
import numpy as np
343/7:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
343/8: fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
343/9:
####Axis  

These are the number-line-like objects. They take care of setting the graph limits
and generating the ticks (the marks on the axis) and ticklabels (strings labeling the ticks). 
The location of the ticks is determined by a Locator object and the ticklabel strings are formatted by a Formatter. 
The combination of the correct Locator and Formatter gives very fine control over the tick locations and labels.
343/10:
#### Axis  

These are the number-line-like objects. They take care of setting the graph limits
and generating the ticks (the marks on the axis) and ticklabels (strings labeling the ticks). 
The location of the ticks is determined by a Locator object and the ticklabel strings are formatted by a Formatter. 
The combination of the correct Locator and Formatter gives very fine control over the tick locations and labels.
343/11:
#### Axis  

These are the number line like objects. They take care of setting the graph limits
and generating the ticks (the marks on the axis) and ticklabels (strings labeling the ticks). 
The location of the ticks is determined by a Locator object and the ticklabel strings are formatted by a Formatter. 
The combination of the correct Locator and Formatter gives very fine control over the tick locations and labels.
343/12:
x = np.linspace(0, 2, 100)
plt.plot(x, x, label='linear', )
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
343/13:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()


ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

ax.xlabel('x label')
ax.ylabel('y label')

ax.title("Simple Plot")
ax.legend()
ax.show()
343/14:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()


ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')


ax.title("Simple Plot")
ax.legend()
ax.show()
343/15:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()


ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

plt.show()
343/16:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()


ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
343/17:
import matplotlib.pyplot as plt
import numpy as np
343/18:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()


ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
343/19:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()


ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')
343/20:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()

ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

ax.axis.xlabel('x label')
ax.axis.ylabel('y label')
343/21:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()

ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

ax.set_xlabel('x label')
ax.set_xlabel('y label')
343/22:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()

ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

ax.set_xlabel('x label')
ax.set_ylabel('y label')
343/23:
x = np.arange(0, 10, 0.2)
y = np.sin(x)
fig, ax = plt.subplots()
ax.plot(x, y)
plt.show()
343/24:
import matplotlib.pyplot as plt
import numpy as np
343/25:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
343/26: fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
343/27:
x = np.linspace(0, 2, 100)

plt.plot(x, x, label='linear', )
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
343/28:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots()

ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

ax.set_xlabel('x label')
ax.set_ylabel('y label')
343/29:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots(2, 2)

ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

ax.set_title("simple plot")
ax.set_xlabel('x label')
ax.set_ylabel('y label')
343/30:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots(2, 2)

ax[0].plot(x, x, label='linear', )
343/31:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots(2, 2)

ax[0].plot(x, x, label='linear', )
ax[0].plot(x, x**2, label='quadratic')
ax[0].plot(x, x**3, label='cubic')

ax[0].set_title("simple plot")
ax[0].set_xlabel('x label')
ax[0].set_ylabel('y label')
343/32:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots(2, 2)

ax[0].plot(x, x, label='linear', )
ax[0].plot(x, x**2, label='quadratic')
ax[0].plot(x, x**3, label='cubic')

ax[0].set_title("simple plot")
ax[0].set_xlabel('x label')
ax[0].set_ylabel('y label')
343/33:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots(2, 2)

ax[0].plot(x, x)
343/34:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots(2, 2)
343/35:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots(2, 2)

ax[0].plot(x, x, label='linear', )
343/36:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots(2, 2)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
343/37:
x = np.linspace(0, 2, 100)
fig, ax = plt.subplots(2, 2)

ax[0, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
343/38:
plt.ion()
plt.plot([1.6, 2.7])
343/39: plt.plot([1.6, 2.7])
343/40:
plt.ion()
plt.plot([1.6, 2.7])
343/41:
plt.ion()
plt.plot([1.6, 2.7])
plt.title("interactive test")
plt.xlabel("index")
plt.title("interactive test")
plt.xlabel("index")
ax = plt.gca()
ax.plot([3.1, 2.2])
343/42:
plt.ion()
plt.plot([1.6, 2.7])
plt.title("interactive test")
plt.xlabel("index")
plt.title("interactive test")
plt.xlabel("index")
ax = plt.gca()
ax.plot([3.1, 2.2])
plt.draw()
343/43: #### interactive mode on (No need to use plt.draw() or plt.show to view figure)
343/44: #### interactive mode on (No need to use plt.draw() or plt.show to view figure)
343/45: #### interactive mode on (No need to use plt.draw() or plt.show to view figure)
343/46:
plt.ion()
plt.plot([1.6, 2.7])
plt.title("interactive test")
plt.xlabel("index")
plt.title("interactive test")
plt.xlabel("index")
ax = plt.gca()
ax.plot([3.1, 2.2])
343/47:
plt.ioff()
plt.plot([1.6, 2.7])

# To view use plt.show()
343/48:
plt.ioff()
plt.plot([1.6, 2.7])

# To view use plt.show()
343/49:
plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')
plt.axis([0, 6, 0, 20])
plt.show()
343/50:
#  evenly sampled time at 200ms intervals
t = np.arange(0., 5., 0.2)

# red dashes, blue squares and green triangles
plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
plt.show()
343/51:
#  evenly sampled time at 200ms intervals
t = np.arange(0., 5., 0.2)

# red dashes, blue squares and green triangles
plt.plot(t, t, 'r', t, t**2, 'bs', t, t**3, 'g^')
plt.show()
343/52:
#  evenly sampled time at 200ms intervals
t = np.arange(0., 5., 0.2)

# red dashes, blue squares and green triangles
plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
plt.show()
343/53:
# The axis() command in the example above takes a list of [xmin, xmax, ymin, ymax] 
# and specifies the viewport of the axes.
plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')
plt.axis([0, 6, 0, 20])
plt.show()
343/54:
data = {'a': np.arange(50),
        'c': np.random.randint(0, 50, 50),
        'd': np.random.randn(50)}
data['b'] = data['a'] + 10 * np.random.randn(50)
data['d'] = np.abs(data['d']) * 100
343/55: data
343/56:
data = {'a': np.arange(50),
        'c': np.random.randint(0, 50, 50),
        'd': np.random.randn(50)}
data['b'] = data['a'] + 10 * np.random.randn(50)
data['d'] = np.abs(data['d']) * 100

plt.scatter('a', 'b', c='c', s='d', data=data)
plt.xlabel('entry a')
plt.ylabel('entry b')
plt.show()
343/57:
data = {'a': np.arange(50),
        'c': np.random.randint(0, 50, 50),
        'd': np.random.randn(50)}
data['b'] = data['a'] + 10 * np.random.randn(50)
data['d'] = np.abs(data['d']) * 100

plt.scatter('a', 'b', c='b', s='d', data=data)
plt.xlabel('entry a')
plt.ylabel('entry b')
plt.show()
343/58:
data = {'a': np.arange(50),
        'c': np.random.randint(0, 50, 50),
        'd': np.random.randn(50)}
data['b'] = data['a'] + 10 * np.random.randn(50)
data['d'] = np.abs(data['d']) * 100

plt.scatter('a', 'b', c='a', s='d', data=data)
plt.xlabel('entry a')
plt.ylabel('entry b')
plt.show()
343/59:
data = {'a': np.arange(50),
        'c': np.random.randint(0, 50, 50),
        'd': np.random.randn(50)}
data['b'] = data['a'] + 10 * np.random.randn(50)
data['d'] = np.abs(data['d']) * 100

plt.scatter('a', 'b', c='a', s='a', data=data)
plt.xlabel('entry a')
plt.ylabel('entry b')
plt.show()
343/60:
data = {'a': np.arange(50),
        'c': np.random.randint(0, 50, 50),
        'd': np.random.randn(50)}
data['b'] = data['a'] + 10 * np.random.randn(50)
data['d'] = np.abs(data['d']) * 100

plt.scatter('a', 'a', c='a', s='a', data=data)
plt.xlabel('entry a')
plt.ylabel('entry b')
plt.show()
343/61:
data = {'a': np.arange(50),
        'c': np.random.randint(0, 50, 50),
        'd': np.random.randn(50)}
data['b'] = data['a'] + 10 * np.random.randn(50)
data['d'] = np.abs(data['d']) * 100

plt.scatter('a', 'b', c='c', s='d', data=data)
plt.xlabel('entry a')
plt.ylabel('entry b')
plt.show()
343/62:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(131)
plt.bar(names, values)
plt.subplot(132)
plt.scatter(names, values)
plt.subplot(133)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/63:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(1, 31)
plt.bar(names, values)
plt.subplot(132)
plt.scatter(names, values)
plt.subplot(133)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/64:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(131)
plt.bar(names, values)
plt.subplot(132)
plt.scatter(names, values)
plt.subplot(133)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/65:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 1000]

plt.figure(figsize=(9, 3))

plt.subplot(131)
plt.bar(names, values)

plt.subplot(132)
plt.scatter(names, values)
plt.subplot(133)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/66:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(131)
plt.bar(names, values)

plt.subplot(132)
plt.scatter(names, values)
plt.subplot(133)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/67:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(1)
plt.bar(names, values)

plt.subplot(1)
plt.scatter(names, values)

plt.subplot(1)
plt.plot(names, values)

plt.suptitle('Categorical Plotting')
plt.show()
343/68:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(100)
plt.bar(names, values)

plt.subplot(100)
plt.scatter(names, values)

plt.subplot(100)
plt.plot(names, values)

plt.suptitle('Categorical Plotting')
plt.show()
343/69:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(111)
plt.bar(names, values)

plt.subplot(111)
plt.scatter(names, values)

plt.subplot(111)
plt.plot(names, values)

plt.suptitle('Categorical Plotting')
plt.show()
343/70:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(111)
plt.bar(names, values)

plt.subplot(111)
plt.scatter(names, values)

plt.subplot(111)
plt.plot(names, values)

plt.suptitle('Categorical Plotting')
plt.show()
343/71:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(111)
plt.bar(names, values)

plt.subplot(112)
plt.scatter(names, values)

plt.subplot(113)
plt.plot(names, values)

plt.suptitle('Categorical Plotting')
plt.show()
343/72:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(131)
plt.bar(names, values)

plt.subplot(132)
plt.scatter(names, values)

plt.subplot(133)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/73:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9, 3))

plt.subplot(311)
plt.bar(names, values)

plt.subplot(312)
plt.scatter(names, values)

plt.subplot(313)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/74:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(3, 9))

plt.subplot(311)
plt.bar(names, values)

plt.subplot(312)
plt.scatter(names, values)

plt.subplot(313)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/75:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(3, 9))
plt.figure(figsize=(9, 3))

plt.subplot(311)
plt.bar(names, values)

plt.subplot(312)
plt.scatter(names, values)

plt.subplot(313)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/76:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(3, 9))

# Either a 3-digit integer or three separate integers describing the position of the subplot. 
# If the three integers are nrows, ncols, and index in order, the subplot 
# will take the index position on a grid with nrows rows and ncols columns. 
# index starts at 1 in the upper left corner and increases to the right.

plt.subplot(311)
plt.bar(names, values)

plt.subplot(312)
plt.scatter(names, values)

plt.subplot(313)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/77:
plt.figure(figsize=(9, 3))

# Either a 3-digit integer or three separate integers describing the position of the subplot. 
# If the three integers are nrows, ncols, and index in order, the subplot 
# will take the index position on a grid with nrows rows and ncols columns. 
# index starts at 1 in the upper left corner and increases to the right.

# 131 => One Row + 3 Columns + index 1
# 132 => One Row + 3 Columns + index 2
# 132 => One Row + 3 Columns + index 3

plt.subplot(131)
plt.bar(names, values)

plt.subplot(132)
plt.scatter(names, values)

plt.subplot(133)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
343/78:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

ax = plt.subplot(111)
line, = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='black', shrink=0.05),
             )

plt.ylim(-2, 2)
plt.show()
343/79:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line, = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='black', shrink=0.05),
             )

plt.ylim(-2, 2)
plt.show()
343/80:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='black', shrink=0.05),
             )

plt.ylim(-2, 2)
plt.show()
343/81:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='red', shrink=0.05),
             )

plt.ylim(-2, 2)
plt.show()
343/82:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='red', shrink=0.15),
             )

plt.ylim(-2, 2)
plt.show()
343/83:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='red', shrink=10.15),
             )

plt.ylim(-2, 2)
plt.show()
343/84:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='red', shrink=100.15),
             )

plt.ylim(-2, 2)
plt.show()
343/85:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='red', shrink=0.05),
             )

plt.show()
343/86:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='red', shrink=0.05),
             )

plt.ylim(-2, 2)
plt.show()
343/87:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='red', shrink=0.05),
             )

plt.ylim(-1, 1)
plt.show()
343/88:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='black', shrink=0.05),
             )

plt.ylim(-2, 2)
plt.show()
343/89:
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
343/90:
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
%matplotlib inline
343/91:
img = mpimg.imread('image.jpeg')
print(img)
343/92:
img = mpimg.imread('image.png')
print(img)
343/93:
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
import matplotlib as mpl
%matplotlib inline
343/94: print(plt.style.available)
344/1: >>> plt.style.use(['seaborn-darkgrid', 'seaborn-dark-palette'])
344/2:
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
import matplotlib as mpl
%matplotlib inline
344/3:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
344/4: >>> plt.style.use(['seaborn-darkgrid', 'seaborn-dark-palette'])
344/5: plt.plot(np.sin(np.linspace(0, 2 * np.pi)), 'r-o')
344/6: >>> plt.style.use(['seaborn-darkgrid'])
344/7: plt.plot(np.sin(np.linspace(0, 2 * np.pi)), 'r-o')
344/8: >>> plt.style.use(['seaborn-dark-palette'])
344/9: plt.plot(np.sin(np.linspace(0, 2 * np.pi)), 'r-o')
344/10: >>> plt.style.use(['dark-palette'])
344/11: >>> plt.style.use(['dark_background'])
344/12: plt.plot(np.sin(np.linspace(0, 2 * np.pi)), 'r-o')
344/13: line, = ax.plot([1, 2, 3], label='Inline label')
344/14:
# object-oriented (OO) interface
fig, ax = plt.subplots(2, 2)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
344/15:
# state-based interface
plt.plot(x, x, label='linear', )
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
344/16:
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
import matplotlib as mpl
%matplotlib inline
344/17:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
344/18:
fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
x = np.linspace(0, 2, 100)
344/19:
# state-based interface
plt.plot(x, x, label='linear', )
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
345/1:
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
import matplotlib as mpl
%matplotlib inline
345/2:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
345/3:
fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
x = np.linspace(0, 2, 100)
345/4:
# state-based interface
plt.plot(x, x, label='linear', )
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
345/5:
# object-oriented (OO) interface
fig, ax = plt.subplots(2, 2)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
345/6: line, = ax.plot([1, 2, 3], label='Inline label')
345/7:
ax = plt.subplot(111)
line, = ax.plot([1, 2, 3], label='Inline label')
345/8:
ax = plt.subplot(111)
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
345/9:
ax = plt.subplot(111)
line, = ax.plot([1, 2], label='Inline label')
ax.legend()
345/10:
ax = plt.subplot(111)
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
345/11:
ax = plt.subplot(111)
line, = ax.plot([1, 2, 5], label='Inline label')
ax.legend()
345/12:
ax = plt.subplot(111)
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
345/13:
ax = plt.subplot(111)
line, = ax.plot([1, 2, 3], label='Inline label')
line1, = ax.plot([1, 4, 3], label='Inline1 label')
ax.legend()
345/14:
line, = ax.plot([1, 2, 3])
line.set_label('Label via method')
ax.legend()
345/15:
line, = ax.plot([1, 2, 3])
line.set_label('Label via method')
ax.legend()
345/16: ax.legend(['A simple line'])
345/17: legend((line1, line2, line3), ('label1', 'label2', 'label3'))
345/18:
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
import matplotlib.pyplot.legend as legend
import matplotlib as mpl
%matplotlib inline
345/19:
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
from matplotlib.pyplot import legend 
import matplotlib as mpl
%matplotlib inline
345/20: legend((line1, line2, line3), ('label1', 'label2', 'label3'))
345/21: legend((line), ('label1'))
345/22:
# Automatic detection of elements to be shown in the legend


ax.plot([1,2,3], [1,2,3], 'go-', label='line 1', linewidth=2)
345/23:
# Automatic detection of elements to be shown in the legend
ax = plt.subplot(111)
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
345/24:
line, = ax.plot([1, 2, 3])
line.set_label('Label via method')
ax.legend()
345/25: ax.legend(['There is only one legend in this Axes'])
345/26:
ax.legend(['There is only one legend in this Axes'])
plt.show()
345/27:
ax.legend(['There is only one legend in this Axes'])
plt.show()
345/28:
ax.legend(['There is only one legend in this Axes'])
fig.show()
345/29:
# Automatic detection of elements to be shown in the legend
ax = plt.subplot(111)
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
345/30:
line, = ax.plot([1, 2, 3])
line.set_label('Label via method')
ax.legend()
345/31:
line, = ax.plot([1, 2, 3])
line.set_label('Label via method')
ax.legend()
plt.show()
345/32:
line, = ax.plot([1, 2, 3])
line.set_label('Label via method')
ax.legend()
plt.show()
346/1:
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
from matplotlib.pyplot import legend 
import matplotlib as mpl
346/2: fig, ax_legend_test = plt.subplots(111)
346/3: fig, ax_legend_test = plt.subplot(111)
346/4: ax_legend_test = plt.subplot(311)
346/5:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax.legend()
346/6:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
346/7:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
ax_legend_test.show()
346/8:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
plt.show()
346/9:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
line.show()
346/10:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
346/11:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
print(ax_legend_test)
346/12:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
ax_legend_test.plot.show()
346/13:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
ax_legend_test.plot().show()
346/14:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
ax_legend_test.plot.show()
346/15:
# Automatic detection of elements to be shown in the legend
line, = ax_legend_test.plot([1, 2, 3], label='Inline label')
ax_legend_test.legend()
346/16: ax_legend_test = plt.subplot(511)
346/17: ax_legend_test = plt.subplot(111)
346/18: ax_legend_test = plt.subplot(211)
346/19: ax_legend_test = plt.subplot(121)
346/20: ax_legend_test = plt.subplot(321)
346/21:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(3, 9))

# Either a 3-digit integer or three separate integers describing the position of the subplot. 
# If the three integers are nrows, ncols, and index in order, the subplot 
# will take the index position on a grid with nrows rows and ncols columns. 
# index starts at 1 in the upper left corner and increases to the right.

plt.subplot(311)
plt.bar(names, values)

plt.subplot(312)
plt.scatter(names, values)

plt.subplot(313)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
346/22:
plt1.figure(figsize=(1, 1))
ax_legend_test = plt1.subplot(111)
346/23:
plt.figure(figsize=(1, 1))
ax_legend_test = plt1.subplot(111)
346/24:
plt.figure(figsize=(1, 1))
ax_legend_test = plt.subplot(111)
346/25:
plt.figure(figsize=(9, 1))
ax_legend_test = plt.subplot(111)
346/26: plt.figure(figsize=(3, 9))
346/27:
plt.figure(figsize=(3, 9))
plt.subplot(311)
346/28: axes = plt.subplots(2, 2)
346/29:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
axes[1, 1].legend()
346/30:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
axes.legend()
346/31:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
346/32:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot(1, 2, label='Inline label')
346/33:
# Automatic detection of elements to be shown in the legend
line, = axes.plot([1, 2, 3], label='Inline label')
346/34:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
346/35:
# Automatic detection of elements to be shown in the legend
line = axes[1, 1].plot([1, 2, 3], label='Inline label')
346/36:
fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
x = np.linspace(0, 2, 100)
346/37:
# Automatic detection of elements to be shown in the legend
line = axes[1, 1].plot(x, x, label='Inline label')
346/38:
# Automatic detection of elements to be shown in the legend
line = axes[1, 1].plot(x, x, label='Inline label', )
346/39:
# Automatic detection of elements to be shown in the legend
axes[1, 1].plot(x, x, label='linear', )
346/40: fig, axes = plt.subplots(2, 2)
346/41:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
346/42:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
plt.show()
346/43:
# object-oriented (OO) interface
fig, ax = plt.subplots(2, 2)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
346/44:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
figure.show()
346/45: figure, axes = plt.subplots(2, 2)
346/46:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
figure.show()
347/1:
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
from matplotlib.pyplot import legend 
import matplotlib as mpl
347/2:
fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
x = np.linspace(0, 2, 100)
347/3: figure, axes = plt.subplots(2, 2)
347/4:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
347/5:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
347/6:
# Automatic detection of elements to be shown in the legend
line, = axes[1, 1].plot([1, 2, 3], label='Inline label')
figure.show()
347/7:
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
from matplotlib.pyplot import legend 
import matplotlib as mpl
347/8:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
347/9:
fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
x = np.linspace(0, 2, 100)
347/10:
# state-based interface
plt.plot(x, x, label='linear', )
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
plt.show()
347/11:
# state-based interface
plt.plot(x, x, label='linear', )
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
347/12:
# object-oriented (OO) interface
fig, ax = plt.subplots(2, 2)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/13:
# object-oriented (OO) interface
fig, ax = plt.subplots(2, 2)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
ax[1, 1]
347/14:
# object-oriented (OO) interface
fig, ax = plt.subplots(2, 2)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/15:
# object-oriented (OO) interface
fig, ax = plt.subplots(1, 1)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/16:
# object-oriented (OO) interface
fig, ax = plt.subplots(1,)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/17:
# object-oriented (OO) interface
fig, ax = plt.subplot(1,)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/18:
# object-oriented (OO) interface
fig, ax = plt.subplot(1, 1)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/19:
# object-oriented (OO) interface
fig, ax = plt.subplot()

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/20:
# object-oriented (OO) interface
fig, ax = plt.subplot()

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/21:
# object-oriented (OO) interface
fig, ax = plt.subplot2grid()

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/22:
# object-oriented (OO) interface
fig, ax = plt.subplot

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/23:
# object-oriented (OO) interface
fig, ax = plt.subplot(1)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/24:
# object-oriented (OO) interface
fig, ax = plt.subplot(111)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/25:
# object-oriented (OO) interface
fig, ax = plt.subplot([111])

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/26:
# object-oriented (OO) interface
fig, ax = plt.subplot(1, 1)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/27:
# object-oriented (OO) interface
fig, ax = plt.subplots(1, 1)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/28:
# object-oriented (OO) interface
fig, ax = plt.subplots()
347/29:
# object-oriented (OO) interface
fig, ax = plt.subplots(2, 2)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
347/30:
# object-oriented (OO) interface
fig, ax = plt.subplot()

ax.plot(x, x, label='linear', )
ax.plot(x, x**2, label='quadratic')
ax.plot(x, x**3, label='cubic')

ax.set_title("simple plot")
ax.set_xlabel('x label')
ax.set_ylabel('y label')
347/31:
# object-oriented (OO) interface
fig, ax = plt.subplot()

ax.plot(x, x, label='linear', )
347/32:
# object-oriented (OO) interface
fig, ax = plt.subplot()
347/33:
# object-oriented (OO) interface
fig, axES = plt.subplot()
347/34:
# object-oriented (OO) interface
fig, ax = plt.subplot(1)
347/35:
# object-oriented (OO) interface
fig, ax = plt.subplot()
347/36:
# object-oriented (OO) interface
fig, ax = plt.subplot()
347/37:
# object-oriented (OO) interface
fig, axES = plt.subplot()
347/38:
# object-oriented (OO) interface
figure = plt.figure()
figure, axes = plt.subplot()
347/39:
# object-oriented (OO) interface
fig, axes = plt.subplot
347/40:
# object-oriented (OO) interface
fig, axes = plt.subplot()
347/41:
# object-oriented (OO) interface
fig, axes = plt.subplot(1, 1)
347/42: figure, axes = plt.subplot(111)
347/43:  axes = plt.subplot(111)
347/44: ax = plt.subplot(111)
347/45:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
347/46:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(3, 9))

# Either a 3-digit integer or three separate integers describing the position of the subplot. 
# If the three integers are nrows, ncols, and index in order, the subplot 
# will take the index position on a grid with nrows rows and ncols columns. 
# index starts at 1 in the upper left corner and increases to the right.

plt.subplot(311)
plt.bar(names, values)

plt.subplot(312)
plt.scatter(names, values)

plt.subplot(313)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
347/47:
plt.figure(figsize=(9, 3))

# Either a 3-digit integer or three separate integers describing the position of the subplot. 
# If the three integers are nrows, ncols, and index in order, the subplot 
# will take the index position on a grid with nrows rows and ncols columns. 
# index starts at 1 in the upper left corner and increases to the right.

# 131 => One Row + 3 Columns + index 1
# 132 => One Row + 3 Columns + index 2
# 132 => One Row + 3 Columns + index 3

plt.subplot(131)
plt.bar(names, values)

plt.subplot(132)
plt.scatter(names, values)

plt.subplot(133)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
347/48:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
plt.show()
347/49:
line, = ax.plot([1, 2, 3])
line.set_label('Label via method')
ax.legend()
plt.show()
347/50:
# Automatic detection of elements to be shown in the legend
line, = ax.plot(x, x, label='Inline label')
plt.show()
347/51:
# Automatic detection of elements to be shown in the legend
line, = ax.plot(x, x, label='Inline label')
plt.show()
ax.legend()
347/52:
# Automatic detection of elements to be shown in the legend
line, = ax.plot(x, x, label='Inline label')
ax.legend()
plt.show()
347/53:
# Automatic detection of elements to be shown in the legend
line, = ax.plot(x, x, label='Inline label')
ax.legend()
347/54:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
347/55:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
347/56:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
print(matplotlib.backends.backend)
347/57:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
print(plt.backends.backend)
347/58:
%matplotlib inline
import matplotlib as matplotlib
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
from matplotlib.pyplot import legend 
import matplotlib as mpl
347/59:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
print(plt.backends.backend)
347/60:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
print(matplotlib.backends.backend)
347/61:
# Automatic detection of elements to be shown in the legend
%matplotlib inline
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
print(matplotlib.backends.backend)
347/62:
line, = ax.plot([1, 2, 3])
line.set_label('Label via method')
ax.legend()
plt.show()
347/63:
# Automatic detection of elements to be shown in the legend
%matplotlib notebook
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
print(matplotlib.backends.backend)
347/64:
# Automatic detection of elements to be shown in the legend
%matplotlib notebook
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
347/65:
# Automatic detection of elements to be shown in the legend
%matplotlib notebook
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
plt.show()
347/66:
# Automatic detection of elements to be shown in the legend
%matplotlib notebook
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
plt.show()
347/67:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
347/68:
line, = ax.plot([1, 2, 3])
line.set_label('Label via method')
ax.legend()
plt.show()
347/69: ax.legend(['There is only one legend in this Axes'])
347/70:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3])
ax.legend()
347/71:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3])
347/72:
# Automatic detection of elements to be shown in the legend

names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(3, 9))

# Either a 3-digit integer or three separate integers describing the position of the subplot. 
# If the three integers are nrows, ncols, and index in order, the subplot 
# will take the index position on a grid with nrows rows and ncols columns. 
# index starts at 1 in the upper left corner and increases to the right.

plt.subplot(311)
plt.bar(names, values)

plt.subplot(312)
plt.scatter(names, values)

plt.subplot(313)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
347/73:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
plt.show()
347/74: ax = plt.subplot(111)
348/1:
import matplotlib as matplotlib
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.image as mpimg
from matplotlib.pyplot import legend 
import matplotlib as mpl
348/2:
fig = plt.figure()  # an empty figure with no axes
fig.suptitle('No axes on this figure')  # Add a title so we know which it is
348/3:
fig, ax_lst = plt.subplots(2, 2)  # a figure with a 2x2 grid of Axes
x = np.linspace(0, 2, 100)
348/4:
# state-based interface
plt.plot(x, x, label='linear', )
plt.plot(x, x**2, label='quadratic')
plt.plot(x, x**3, label='cubic')

plt.xlabel('x label')
plt.ylabel('y label')

plt.title("Simple Plot")
plt.legend()
348/5:
# object-oriented (OO) interface
fig, ax = plt.subplots(2, 2)

ax[1, 1].plot(x, x, label='linear', )
ax[1, 1].plot(x, x**2, label='quadratic')
ax[1, 1].plot(x, x**3, label='cubic')

ax[1, 1].set_title("simple plot")
ax[1, 1].set_xlabel('x label')
ax[1, 1].set_ylabel('y label')
348/6:
# The axis() command in the example above takes a list of [xmin, xmax, ymin, ymax] 
# and specifies the viewport of the axes.
plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')
plt.axis([0, 6, 0, 20])
plt.show()
348/7:
# plotting several lines with different format styles in one command using arrays.
#  evenly sampled time at 200ms intervals
t = np.arange(0., 5., 0.2)

# red dashes, blue squares and green triangles
plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
plt.show()
348/8:
data = {'a': np.arange(50),
        'c': np.random.randint(0, 50, 50),
        'd': np.random.randn(50)}
data['b'] = data['a'] + 10 * np.random.randn(50)
data['d'] = np.abs(data['d']) * 100

plt.scatter('a', 'b', c='c', s='d', data=data)
plt.xlabel('entry a')
plt.ylabel('entry b')
plt.show()
348/9:
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(3, 9))

# Either a 3-digit integer or three separate integers describing the position of the subplot. 
# If the three integers are nrows, ncols, and index in order, the subplot 
# will take the index position on a grid with nrows rows and ncols columns. 
# index starts at 1 in the upper left corner and increases to the right.

plt.subplot(311)
plt.bar(names, values)

plt.subplot(312)
plt.scatter(names, values)

plt.subplot(313)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
348/10:
plt.figure(figsize=(9, 3))

# Either a 3-digit integer or three separate integers describing the position of the subplot. 
# If the three integers are nrows, ncols, and index in order, the subplot 
# will take the index position on a grid with nrows rows and ncols columns. 
# index starts at 1 in the upper left corner and increases to the right.

# 131 => One Row + 3 Columns + index 1
# 132 => One Row + 3 Columns + index 2
# 132 => One Row + 3 Columns + index 3

plt.subplot(131)
plt.bar(names, values)

plt.subplot(132)
plt.scatter(names, values)

plt.subplot(133)
plt.plot(names, values)
plt.suptitle('Categorical Plotting')
plt.show()
348/11:
t = np.arange(0.0, 5.0, 0.01)
s = np.cos(2*np.pi*t)

line = plt.plot(t, s, lw=2)

plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),
             arrowprops=dict(facecolor='black', shrink=0.05),
             )

plt.ylim(-2, 2)
plt.show()
348/12: plt.plot(np.sin(np.linspace(0, 2 * np.pi)), 'r-o')
348/13: ax = plt.subplot(111)
348/14:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
plt.show()
348/15: fig, ax = plt.subplot(111)
348/16:
plt.figure(figsize=(9, 3))
ax = plt.subplot(111)
348/17:
plt.figure(figsize=(9, 3))
ax = plt.subplot(111)
348/18:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
plt.show()
348/19:
# Automatic detection of elements to be shown in the legend
plt.plot(np.sin(np.linspace(0, 2 * np.pi)), 'r-o')
348/20:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
348/21:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
line.draw
348/22:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
348/23:
plt.figure(figsize=(9, 3))
ax = plt.subplot(111)
348/24:
plt.figure(figsize=(4, 3))
ax = plt.subplot(111)
348/25:
# Automatic detection of elements to be shown in the legend
line, = plt.plot([1, 2, 3], label='Inline label')
ax.legend()
348/26:
# Automatic detection of elements to be shown in the legend
line, = plt.plot([1, 2, 3], label='Inline label')
plt.legend()
348/27:
line, = plt.plot([1, 2, 3])
line.set_label('Label via method')
plt.legend()
348/28: plt.legend(['There is only one legend in this Axes'])
348/29: ax.legend(['There is only one legend in this Axes'])
348/30:
# Automatic detection of elements to be shown in the legend
line, = plt.plot([1, 2, 3], label='Inline label')
plt.legend()
348/31:
plt.figure(figsize=(4, 3))
ax = plt.subplot(111)
348/32:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
348/33:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], label='Inline label')
ax.legend()
plt.plot(x, x)
348/34:
# Automatic detection of elements to be shown in the legend
line, = plt.plot([1, 2, 3], [1, 2, 3], label='Inline label')
plt.legend()
348/35:
# Automatic detection of elements to be shown in the legend
line, = plt.plot([1, 2, 3], [1, 9, 3], label='Inline label')
plt.legend()
348/36:
line, = plt.plot([1, 2, 3], [1, 2, 3])
line.set_label('Label via method')
plt.legend()
348/37: plt.legend(['There is only one legend in this Axes'])
348/38: plt.axes
348/39: plt.axes.legend(['A simple line'])
348/40: plt.axes().legend(['A simple line'])
348/41:
axes = plt.plot([1, 2, 3]).axes()
axes.legend(['A simple line'])
348/42:
axes = plt.plot([1, 2, 3]).axes
axes.legend(['A simple line'])
348/43:
plt.plot([1, 2, 3])
plt.axes.legend(['A simple line'])
348/44: legend(line, ['test'])
348/45: legend([line], ['test'])
348/46:
line, = plt.plot([1, 2, 3], [1, 2, 3])
line.set_label('Label via method')
plt.legend()
348/47: legend([line], ['test'])
348/48: legend((line), ('test')
348/49: legend((line), ('test'))
348/50: legend(('line'), ('test'))
348/51:
# Automatic detection of elements to be shown in the legend
line, = ax.plot([1, 2, 3], [1, 9, 3], label='Inline label')
ax.legend()
348/52:
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles, labels)
348/53:
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles, labels)
348/54:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
plt.legend(handles=[line_up, line_down])
348/55:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
plt.legend([line_up, line_down], ['Line Up', 'Line Down'])
348/56:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
plt.legend(handles=[line_up, line_down])
348/57:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
plt.legend(handles=[line_up, line_down])
plt.legend([line_up, line_down], ['Line Up', 'Line Down'])
348/58:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
348/59:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
plt.legend()
348/60: plt.legend(['line_one', 'line_two'])
348/61: plt.legend([line_up, line_down], ['Line Up', 'Line Down'])
348/62:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
plt.legend()
348/63: plt.legend([line_up, line_down], ['Line Up', 'Line Down'])
348/64:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
plt.legend([line_up, line_down], ['Line Up', 'Line Down'])
348/65:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
line_up.legend(['Line Up'])
line_down.legend(['Line Up'])
348/66:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
plt.legend(['Line Up'])
plt.legend(['Line Up'])
348/67:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
plt.legend([line_up, line_down], ['Line Up', 'Line Down'])
348/68:
line_up, = plt.plot([1,2,3], label='Line 2')
line_down, = plt.plot([3,2,1], label='Line 1')
legend([line_up, line_down], ['Line Up', 'Line Down'])
349/1: from sklearn.datasets import load_digits
349/2: load_digits.shape
349/3: digits = load_digits()
349/4:
digits = load_digits()
digits.shape
349/5:
digits = load_digits()
digits.shape()
349/6:
digits = load_digits()
digits.data.shape()
349/7:
digits = load_digits()
digits.data.shape
349/8:
digits = load_digits()
digits.data.shape
349/9: from sklearn.datasets import load_digits
349/10:
digits = load_digits()
digits.data.shape
349/11:
from sklearn.datasets import load_digits
from sklearn.cluster import KMeans
349/12: data.head
349/13: digits.data.head
349/14: digits.data.head()
349/15: digits.data.show()
349/16: print(digits.data)
349/17:
digits = load_digits()
data = digits.data
349/18: data.__class__
349/19: data[1:]
349/20: data.shape
349/21:
digits = load_digits()
data = digits.data
data.shape
349/22: X_digits, y_digits = load_digits(return_X_y=True)
349/23:
X_digits, y_digits = load_digits(return_X_y=True)
data = scale(X_digits)
349/24:
from sklearn.datasets import load_digits
from sklearn.cluster import KMeans
from sklearn.preprocessing import scale
349/25:
X_digits, y_digits = load_digits(return_X_y=True)
data = scale(X_digits)
349/26:
X_digits, y_digits = load_digits(return_X_y=True)
data = scale(X_digits)

n_samples, n_features = data.shape
print(n_samples)
print(n_features)
349/27:
X_digits, y_digits = load_digits(return_X_y=True)
data = scale(X_digits)

n_samples, n_features = data.shape

n_digits = len(np.unique(y_digits))
print(n_digits)
349/28:
from sklearn.datasets import load_digits
from sklearn.cluster import KMeans
from sklearn.preprocessing import scale
import numpy as np
349/29:
X_digits, y_digits = load_digits(return_X_y=True)
data = scale(X_digits)

n_samples, n_features = data.shape

n_digits = len(np.unique(y_digits))
print(n_digits)
349/30:
X_digits, y_digits = load_digits(return_X_y=True)
data = scale(X_digits)

n_samples, n_features = data.shape
n_digits = len(np.unique(y_digits))
labels = y_digits
print(y_digits)
349/31:
X_digits, y_digits = load_digits(return_X_y=True)
data = scale(X_digits)

n_samples, n_features = data.shape
n_digits = len(np.unique(y_digits))
labels = y_digits
print(np.unique(y_digits))
349/32:
X_digits, y_digits = load_digits(return_X_y=True)
data = scale(X_digits)

n_samples, n_features = data.shape
n_digits = len(np.unique(y_digits))
labels = y_digits
np.unique(y_digits)
349/33:
X_digits, y_digits = load_digits(return_X_y=True)
data = scale(X_digits)

n_samples, n_features = data.shape
n_digits = len(np.unique(y_digits))
labels = y_digits
print(np.unique(y_digits))
349/34:
print("n_digits: %d, \t n_samples %d, \t n_features %d"
      % (n_digits, n_samples, n_features))
349/35: print("n_digits: %d, \t n_samples %d, \t n_features %d" % (n_digits, n_samples, n_features))
349/36: print("n_digits: %d,  n_samples %d, \t n_features %d" % (n_digits, n_samples, n_features))
349/37: print("n_digits: %d, \t n_samples %d, \t n_features %d" % (n_digits, n_samples, n_features))
349/38: print("n_digits: %s, \t n_samples %d, \t n_features %d" % (n_digits, n_samples, n_features))
349/39: print("n_digits: %d, \t n_samples %d, \t n_features %d" % (n_digits, n_samples, n_features))
349/40: print("n_digits: %d, \t n_samples %d, \t n_features %d"  (n_digits, n_samples, n_features))
349/41: print("n_digits: %d, \t n_samples %d, \t n_features %d" % (n_digits, n_samples, n_features))
349/42:

print(82 * '_')
print('init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette')
349/43: y_pred = KMeans(n_clusters=n_digits, random_state=random_state).fit_predict(X_digits)
349/44:
from sklearn.datasets import load_digits
from sklearn.cluster import KMeans
from sklearn.preprocessing import scale
import numpy as np
from sklearn.cluster import KMeans
349/45: y_pred = KMeans(n_clusters=n_digits, random_state=random_state).fit_predict(X_digits)
349/46: y_pred = KMeans(n_clusters=n_digits, init="k-means++").fit_predict(X_digits)
349/47:
# init -> selects initial cluster centers for k-mean clustering ('k-means++', 'random')

y_pred = KMeans(n_clusters=n_digits, init="k-means++").fit_predict(X_digits)
349/48:
# init -> selects initial cluster centers for k-mean clustering ('k-means++', 'random')
y_pred = KMeans(n_clusters=n_digits, init="k-means++", n_init=10).fit_predict(X_digits)
349/49:
# init -> selects initial cluster centers for k-mean clustering ('k-means++', 'random')
y_pred = KMeans(n_clusters=n_digits, init="k-means++", n_init=10).fit_predict(X_digits)
349/50: y_pred
349/51:
from sklearn.datasets import load_digits
from sklearn.cluster import KMeans
from sklearn.preprocessing import scale
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
349/52: plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
349/53: plt.scatter(X_digits[:, 0], X_digits[:, 1], c=y_pred, s=50, cmap='viridis')
349/54:
plt.scatter(X_digits[:, 0], X_digits[:, 1], c=y_pred, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
349/55:
plt.scatter(X_digits[:, 0], X_digits[:, 1], c=y_pred, s=50, cmap='viridis')
centers = y_pred.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
349/56:
# init -> selects initial cluster centers for k-mean clustering ('k-means++', 'random')
kmeans = KMeans(n_clusters=n_digits, init="k-means++", n_init=10)
349/57:
# init -> selects initial cluster centers for k-mean clustering ('k-means++', 'random')
kmeans = KMeans(n_clusters=n_digits, init="k-means++", n_init=10)
y_pred = kmeans.fit_predict(X_digits)
349/58: y_pred
349/59:
plt.scatter(X_digits[:, 0], X_digits[:, 1], c=y_pred, s=50, cmap='viridis')
centers = y_pred.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
349/60:
plt.scatter(X_digits[:, 0], X_digits[:, 1], c=y_pred, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
349/61: kmeans.cluster_centers_.shape
349/62:
plt.scatter(X_digits[:, 0], X_digits[:, 1], c=y_pred, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);

fig, ax = plt.subplots(2, 5, figsize=(8, 3))
centers = kmeans.cluster_centers_.reshape(10, 8, 8)
for axi, center in zip(ax.flat, centers):
    axi.set(xticks=[], yticks=[])
    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)
349/63:
fig, ax = plt.subplots(2, 5, figsize=(8, 3))
centers = kmeans.cluster_centers_.reshape(10, 8, 8)
for axi, center in zip(ax.flat, centers):
    axi.set(xticks=[], yticks=[])
    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)
349/64: import pandas as pd
349/65:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|", header=None)
df.head()
349/66:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|", header=True)
df.head()
349/67:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df.head()
349/68:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df.size()
349/69:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df.size
349/70:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df.size
349/71:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df.head(400)
349/72:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df.size
349/73:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df.shape
349/74:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df.head()
349/75:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df.drop(labels='Unnamed: 6', axis=1)
349/76:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
349/77:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df.shape
349/78:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df.head()
349/79:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df['email'] = df['owner_name']
df.head()
349/80:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df.head()
349/81:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df['email'] = df['owner_name'] + 'test'
df.head()
349/82:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df.head()
349/83:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df['email'] = df['owner_name']
df.head()
349/84:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df.columns
349/85:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
df.columns
349/86:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
df.head()
349/87:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df.head()
349/88:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
df['email'] = df['owner_name'] + '@choicehotels.com'
df.head(10)
349/89:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
df['email'] = df['owner_name'] + '@choicehotels.com'
df.head(10)
349/90:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
df['email'] = df['owner_name'] + '@choicehotels.com'
df.head(10)
353/1:
import pandas as pd
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
df['email'] = df['owner_name'] + '@choicehotels.com'
df.head(10)
353/2: import pandas as pd
353/3: import pandas as pd
354/1: import pandas as pd
354/2:
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
df['email'] = df['owner_name'] + '@choicehotels.com'
df.head(10)
354/3: import pandas as pd
354/4:
df = pd.read_csv("/Users/ukannika/Desktop/tables.csv", sep="|")
df = df.drop(labels='Unnamed: 6', axis=1)
df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
df['email'] = df['owner_name'] + '@choicehotels.com'
df.head(10)
354/5: df['owner_name'].unique()
354/6: print(df['owner_name'].unique())
354/7: df[(df[create_time] <= '2019-01-01 00:00:00')]
354/8: df[(df['create_time'] <= '2019-01-01 00:00:00')]
355/1:
Map high dimensional data to low dimensional data using below algorithms

Linear:
    PCA
355/2:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
355/3:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn import datasets
355/4: digits = load_digits()
355/5:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
355/6: digits = load_digits()
355/7:
digits = load_digits()
X = digits.data
y = digits.target
355/8:
fig = plt.figure(1, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
355/9:
fig = plt.figure(1, figsize=(4, 3))
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
355/10:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
355/11:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 10, .95, 1], elev=48, azim=134)
355/12:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[10, 10, .95, 1], elev=48, azim=134)
355/13:
fig = plt.figure(1, figsize=(4, 3))
#Clear
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
355/14:
fig = plt.figure(1, figsize=(4, 3))
#Clear
plt.clf()
ax = Axes3D(fig, rect=[10, 0, .95, 1], elev=48, azim=134)
355/15:
fig = plt.figure(1, figsize=(4, 3))
#Clear
plt.clf()
ax = Axes3D(fig, rect=[1011, 0, .95, 1], elev=48, azim=134)
355/16:
fig = plt.figure(1, figsize=(4, 3))
#Clear
plt.clf()
ax = Axes3D(fig, rect=[1011, 110, .95, 1], elev=48, azim=134)
355/17:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
355/18:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)
355/19:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)

X.shape
355/20:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()


pca = decomposition.PCA(n_components=3)
# Fit the model
pca.fit(X) 
# Apply on Data X
X = pca.transform(X)
X.shape()
355/21:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()


pca = decomposition.PCA(n_components=3)
# Fit the model
pca.fit(X) 
# Apply on Data X
X = pca.transform(X)
X.shape
355/22:
pca = decomposition.PCA(n_components=3)
# Fit the model
pca.fit(X) 
# Apply on Data X
X = pca.transform(X)
X.shape
355/23:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()



for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    ax.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])

plt.show()
355/24:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()



for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    ax.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))


plt.show()
355/25:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()



for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    ax.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))


# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
    
plt.show()
355/26:
pca = decomposition.PCA(n_components=3)
# Fit the model
pca.fit_transform(X) 
X.shape
355/27:
pca = decomposition.PCA(n_components=3)
# Fit the model
pca.fit_transform(X) 
X.shape
355/28:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape
355/29:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()



for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    ax.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))


# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
    
plt.show()
355/30:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()



for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    print(name + "  "+label)
    ax.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))


# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
    
plt.show()
355/31:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()



for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    print(name + "  "+label)
    ax.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
355/32:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()



for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    print(name)
    ax.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
355/33:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape

X.mean()
355/34:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape

X[y == label, 0].mean()
355/35:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape

X.__class__
355/36:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape

X[0, 1].mean()
355/37:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape

X[1, 1].mean()
355/38:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y
355/39:

np.random.seed(5)

centers = [[1, 1], [-1, -1], [1, -1]]
iris = datasets.load_iris()
X = iris.data
y = iris.target

fig = plt.figure(1, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

plt.cla()
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)

# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])

plt.show()
355/40:

np.random.seed(5)

centers = [[1, 1], [-1, -1], [1, -1]]
iris = datasets.load_iris()
X = iris.data
y = iris.target

fig = plt.figure(1, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

plt.cla()
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)

y
355/41:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0, 3, 4, 5, 6, 7, 8, 9]).astype(np.float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
    
plt.show()
355/42:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.unique(y)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
    
plt.show()
355/43:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
y = np.unique(y)
y
355/44:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.float)
y
355/45:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y
355/46:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
y
355/47:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.choose(y.astype(np.int64), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y
355/48:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.unique(y)
y
355/49:
digits = load_digits()
X = digits.data
y = digits.target
355/50:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.unique(y)
y
355/51:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.choose(y.astype(np.int64), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y
355/52:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

y

# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y
355/53:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y
355/54:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape

X[1, 1].mean()
355/55:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y
355/56:
digits = load_digits()
X = digits.data
y = digits.target
355/57:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape

X[1, 1].mean()
355/58:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y
355/59:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y = np.unique(y)
355/60:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y = np.unique(y)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')
355/61:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y = np.unique(y)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=[0, 0, 0], cmap=plt.cm.nipy_spectral,
           edgecolor='k')
355/62:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y = np.unique(y)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')
355/63:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y = np.unique(y)
y
355/64:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2), 
                    ('Setosa', 3), ('Versicolour', 4), ('Virginica', 5), 
                    ('Setosa', 6), ('Versicolour', 7), ('Virginica', 8),
                    ('Setosa', 9)]:
    ax.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
y = np.unique(y)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
           edgecolor='k')
355/65:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/66:
digits = load_digits()
X = digits.data
y = digits.target
355/67:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape

X[1, 1].mean()
355/68:
fig = plt.figure(1, figsize=(4, 3))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/69:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/70: pca.explained_variance_ratio_
355/71:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
355/72:
digits = load_digits()
X = digits.data
y = digits.target

iris = load_iris()
X_iris = iris.data
y_iris = iris.target
355/73:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape
355/74:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 
X_iris.shape
355/75:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X_iris[:, 0], X_iris[:, 1], X_iris[:, 2], c=y_iris, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/76:
iris_pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = iris_pca.fit_transform(X_iris) 
X_iris.shape
355/77: iris_pca.explained_variance_
355/78: iris_pca.explained_variance_ratio_
355/79: # PCA (Principal Component Analysis)
355/80: #### MDA (Multi Dimensional Scaling)
355/81:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.manifold import MDS
355/82:
mds = MDS(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = mds.fit_transform(X) 
X.shape
355/83:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/84:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1])
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/85:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=18, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/86:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=60, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/87: mds.dissimilarity_matrix_
355/88:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=60, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/89: mds.stress_
355/90:
digits = load_digits()
X = digits.data
y = digits.target

iris = load_iris()
X_iris = iris.data
y_iris = iris.target
355/91:
# Plot results
def plot_3d():
    print("3d")
    
def plot_2d():
    print("2d")
355/92: plot_2d()
355/93: plot_3d()
355/94:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.manifold import MDS
355/95:
# Next line to silence pyflakes. This import is needed.
Axes3D

n_points = 1000
X, color = datasets.make_s_curve(n_points, random_state=0)
n_neighbors = 10
n_components = 2

# Create figure
fig = plt.figure(figsize=(15, 8))
fig.suptitle("Manifold Learning with %i points, %i neighbors"
             % (1000, n_neighbors), fontsize=14)

# Add 3d scatter plot
ax = fig.add_subplot(251, projection='3d')
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax.view_init(4, -72)
355/96:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples==1000, noise=0.0, random_state=0)
n_neighbors = 10
n_components = 2
355/97:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_points=1000, noise=0.0, random_state=0)
n_neighbors = 10
n_components = 2
355/98:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
355/99:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_points=1000, noise=0.0, random_state=0)
n_neighbors = 10
n_components = 2
355/100:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)
n_neighbors = 10
n_components = 2
355/101:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)
355/102:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

X_digits.shape()
355/103:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

X_digits.shape
355/104:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1 Dim: " % X_digits.shape)
355/105:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1 Dim: " + X_digits.shape)
355/106:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1 Dim: " % (X_digits.shape))
355/107:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1 Dim: %s" % (X_digits.shape))
355/108:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
355/109:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
355/110:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
n_points = 1000
X, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_digits.shape, ))
print("Dataset3: %s" % (X_digits.shape, ))
355/111:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_digits.shape, ))
355/112:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
355/113:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
print(color)
355/114:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
355/115:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape
355/116:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
355/117:
X = np.arange(-5, 5, 0.25)
Y = np.arange(-5, 5, 0.25)
X, Y = np.meshgrid(X, Y)
R = np.sqrt(X**2 + Y**2)
Z = np.sin(R)
355/118:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X = pca.fit_transform(X) 
X.shape
357/1:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
357/2:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = datasets.make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
357/3:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
357/4:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
357/5:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 
X_digits.shape
357/6: plot_3d(X_digits, y_digits,X_digits, y_digits,X_digits, y_digits, 'Title')
357/7:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
357/8: plot_3d(X_digits, y_digits,X_digits, y_digits,X_digits, y_digits, 'Title')
357/9:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
357/10: plot_3d(X_digits, y_digits,X_digits, y_digits,X_digits, y_digits, 'Title')
357/11: plot_3d(X_digits, y_digits,X_digits, y_digits,X_digits, y_digits, 'Title')
357/12:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 2, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 2, 2, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 2, 3, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
357/13: plot_3d(X_digits, y_digits,X_digits, y_digits,X_digits, y_digits, 'Title')
357/14:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
357/15: plot_3d(X_digits, y_digits,X_digits, y_digits,X_digits, y_digits, 'Title')
357/16:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 
X_digits.shape

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 
X_iris.shape

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 
X_digits.shape
357/17: plot_3d(X_digits, y_digits,X_iris, y_iris,X_digits, y_digits, 'Title')
357/18: plot_3d(X_digits, y_digits, X_iris, y_iris, X_s_curve, color, 'Title')
357/19:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
357/20:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
357/21:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
357/22:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
357/23: plot_3d(X_digits, y_digits, X_iris, y_iris, X_s_curve, color, 'Title')
357/24:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
357/25:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
357/26: plot_3d(X_digits, y_digits, X_iris, y_iris, X_s_curve, color, 'Title')
357/27: def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
357/28:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
357/29:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
357/30:
%matplotlib notebook
plot_3d(X_digits, y_digits, X_iris, y_iris, X_s_curve, color, 'Title')
357/31:
%matplotlib inline
plot_3d(X_digits, y_digits, X_iris, y_iris, X_s_curve, color, 'Title')
357/32: plot_3d(X_digits, y_digits, X_iris, y_iris, X_s_curve, color, 'Title')
357/33:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
358/1:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
358/2:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
358/3:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
359/1:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
359/2:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
359/3:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
359/4:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
359/5:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
359/6:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
        rows=1, cols=3,
        column_widths=[0.6, 0.4],
        row_heights=[0.4, 0.6],
        specs=[[{"type": "scatter", "rowspan": 1}, {"type": "scatter"}],
               [            None                    , {"type": "scatter"}]])

    # Add scatter plot for dataset1
    fig.add_trace(
        go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
        row=1, col=1
    )
359/7:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
        rows=1, cols=3,
        column_widths=[0.6, 0.4],
        row_heights=[0.4, 0.6],
        specs=[[{"type": "scatter", "rowspan": 1}, {"type": "scatter"}],
               [            None                    , {"type": "scatter"}]])

    # Add scatter plot for dataset1
    fig.add_trace(
        go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers'),
        row=1, col=1
    )
359/8:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
359/9: plot_3d_interactive(X_digits, y_digits)
359/10: px.data.iris()
359/11:
def plot_3d_interactive(X_dataset1, y_dataset1):
    pd.DataFrame(data=X_dataset1[1:,1:], index=X_dataset1[1:,0], columns=X_dataset1[0,1:])
359/12: plot_3d_interactive(X_digits, y_digits)
359/13:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
359/14: plot_3d_interactive(X_digits, y_digits)
359/15:
def plot_3d_interactive(X_dataset1, y_dataset1):
    df = pd.DataFrame(data=X_dataset1[1:,1:], index=X_dataset1[1:,0], columns=X_dataset1[0,1:]) 
    df.head()
359/16: plot_3d_interactive(X_digits, y_digits)
359/17:
def plot_3d_interactive(X_dataset1, y_dataset1):
    df = pd.DataFrame(data=X_dataset1[1:,1:], index=X_dataset1[1:,0], columns=X_dataset1[0,1:]) 
    df.head()
359/18:
df = plot_3d_interactive(X_digits, y_digits)
df.head()
359/19:
df = plot_3d_interactive(X_digits, y_digits)
df.__class__
359/20:
df = plot_3d_interactive(X_digits, y_digits)
df
359/21:
df = plot_3d_interactive(X_digits, y_digits)
df
359/22:
def plot_3d_interactive(X_dataset1, y_dataset1):
    return pd.DataFrame(data=X_dataset1[1:,1:], index=X_dataset1[1:,0], columns=X_dataset1[0,1:])
359/23:
df = plot_3d_interactive(X_digits, y_digits)
df
359/24:
def plot_3d_interactive(X_dataset1, y_dataset1):
    return pd.DataFrame(data=X_dataset1[:,:])
359/25:
df = plot_3d_interactive(X_digits, y_digits)
df
359/26:
def plot_3d_interactive(X_dataset1, y_dataset1):
    df = pd.DataFrame(data=X_dataset1[:,:])
    # Initialize figure with subplots
    fig = make_subplots(
        rows=1, cols=3,
        column_widths=[0.6, 0.4],
        row_heights=[0.4, 0.6],
        specs=[[{"type": "scatter", "rowspan": 1}, {"type": "scatter"}],
               [            None                    , {"type": "scatter"}]])

    # Add scatter plot for dataset1
    fig.add_trace(
        go.Scatter3d(df, x='0', y='1', z='2', mode='markers'),
        row=1, col=1
    )
359/27: plot_3d_interactive(X_digits, y_digits)
359/28:
def plot_3d_interactive(X_dataset1, y_dataset1):
    df = pd.DataFrame(data=X_dataset1[:,:])
    # Initialize figure with subplots
    fig = make_subplots(
        rows=1, cols=3,
        column_widths=[0.6, 0.4],
        row_heights=[0.4, 0.6],
        specs=[[{"type": "scatter", "rowspan": 1}, {"type": "scatter"}],
               [            None                    , {"type": "scatter"}],
               [            None                    , {"type": "scatter"}]])

    # Add scatter plot for dataset1
    fig.add_trace(
        go.Scatter3d(df, x='0', y='1', z='2', mode='markers'),
        row=1, col=1
    )
359/29: plot_3d_interactive(X_digits, y_digits)
359/30:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
359/31:
def plot_3d_interactive(X_dataset1, y_dataset1):
     trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
     trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
359/32:
def plot_3d_interactive(X_dataset1, y_dataset1):
    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
359/33:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    column_widths=[0.6, 0.4],
    row_heights=[0.4, 0.6],
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
359/34:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    column_widths=[0.6, 0.4],
    row_heights=[0.4, 0.6],
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.add_trace(trace0, 1, 1)
    fig.add_trace(trace1, 1, 1)
359/35:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
359/36: plot_3d_interactive(X_digits, y_digits)
359/37:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.add_trace(trace0, 1, 1)
    fig.add_trace(trace1, 1, 1)
359/38: plot_3d_interactive(X_digits, y_digits)
359/39:
plot_3d_interactive(X_digits, y_digits)
fig.show()
359/40:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.add_trace(trace0, 1, 1)
    fig.add_trace(trace1, 1, 1)
    fig.show()
359/41: plot_3d_interactive(X_digits, y_digits)
359/42:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
359/43:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
359/44:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
359/45:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.add_trace(trace0, 1, 1)
    fig.add_trace(trace1, 1, 1)
    fig.show()
359/46:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
359/47: plot_3d_interactive(X_digits, y_digits)
359/48:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 1)
    fig.show()
359/49: plot_3d_interactive(X_digits, y_digits)
359/50:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 1)
    fig.show()
359/51:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
359/52:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
359/53:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
359/54:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 1)
    fig.show()
359/55:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
359/56: plot_3d(X_digits, y_digits, X_iris, y_iris, X_s_curve, color, 'Title')
359/57: plot_3d_interactive(X_digits, y_digits)
359/58:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 1)
    fig.show()
359/59: plot_3d_interactive(X_digits, y_digits)
359/60:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 1)
    fig.show()
359/61:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/62: plot_3d_interactive(X_digits, y_digits)
359/63:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', color='species')
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/64:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
359/65: plot_3d_interactive(X_digits, y_digits)
359/66:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/67: plot_3d_interactive(X_digits, y_digits)
359/68: plot_3d_interactive(X_digits, y_digits)
359/69:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers')
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/70:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
359/71: plot_3d_interactive(X_digits, y_digits)
359/72:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=1,
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
359/73: plot_3d_interactive(X_digits, y_digits)
359/74:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=1,
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
359/75: plot_3d_interactive(X_digits, y_digits)
359/76:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    column_widths=[0.6, 0.4],
    row_heights=[0.4, 0.6],
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/77: plot_3d_interactive(X_digits, y_digits)
359/78:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    column_widths=[0.6, 0.4],
    row_heights=[0.6],
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/79: plot_3d_interactive(X_digits, y_digits)
359/80:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    column_widths=[1.6, 0.4],
    row_heights=[0.6],
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/81: plot_3d_interactive(X_digits, y_digits)
359/82:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=1, cols=2,
    column_widths=[1.0, 0.5],
    row_heights=[0.6],
    specs=[[{"type": "scatter3d"}, {"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/83: plot_3d_interactive(X_digits, y_digits)
359/84:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=1,
    column_widths=[1.0,1.0],
    row_heights=[0.6],
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/85: plot_3d_interactive(X_digits, y_digits)
359/86:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=1,
    column_widths=[1.0],
    row_heights=[1.0, 1.0, 1.0],
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/87: plot_3d_interactive(X_digits, y_digits)
359/88:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=1,
    column_widths=[1.0],
    row_heights=[1.0, 1.0],
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 1, 2)
    fig.show()
359/89: plot_3d_interactive(X_digits, y_digits)
359/90:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=1,
    column_widths=[1.0],
    row_heights=[1.0, 1.0, 1.0],
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    trace2 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace1, 3, 1)
    fig.show()
359/91: plot_3d_interactive(X_digits, y_digits)
359/92:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=1,
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    trace2 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace1, 3, 1)
    fig.show()
359/93: plot_3d_interactive(X_digits, y_digits)
359/94:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=1,
    column_widths=[1.0],
    row_heights=[0.4, 0.6],
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace1, 3, 1)
    fig.show()
359/95: plot_3d_interactive(X_digits, y_digits)
359/96:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=1,
    column_widths=[1.0],
    row_heights=[0.4, 0.6],
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
359/97: plot_3d_interactive(X_digits, y_digits)
359/98:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=1,
    column_widths=[1.0],
    row_heights=[0.8, 0.6],
    specs=[[{"type": "scatter3d"}], [{"type": "scatter3d"}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', )
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
359/99: plot_3d_interactive(X_digits, y_digits)
359/100:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=1,
    column_widths=[1.0],
    row_heights=[0.8, 0.6],
    specs=[[{"type": "scatter3d", "colspan": 2}], [{"type": "scatter3d", "colspan": 2}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
359/101: plot_3d_interactive(X_digits, y_digits)
360/1: plot_3d_interactive(X_digits, y_digits)
360/2:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
360/3:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
360/4:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
360/5:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
360/6:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=1,
    column_widths=[1.0],
    row_heights=[0.8, 0.6],
    specs=[[{"type": "scatter3d"} "colspan": 2}], [{"type": "scatter3d", "colspan": 2}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
360/7:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=1,
    column_widths=[1.0],
    row_heights=[0.8, 0.6],
    specs=[[{"type": "scatter3d", "colspan": 2}], [{"type": "scatter3d", "colspan": 2}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
360/8:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
360/9: plot_3d_interactive(X_digits, y_digits)
360/10:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=3,
    column_widths=[1.0],
    row_heights=[0.8, 0.6],
    specs=[[{"type": "scatter3d", "colspan": 2}], [{"type": "scatter3d", "colspan": 2}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
360/11:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
360/12: plot_3d_interactive(X_digits, y_digits)
360/13:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}], [{"type": "scatter3d", "colspan": 2}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
360/14: plot_3d_interactive(X_digits, y_digits)
360/15:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, {"type": "scatter3d", "colspan": 2}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
360/16: plot_3d_interactive(X_digits, y_digits)
360/17:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, {"type": "scatter3d", "colspan": 2}],
           [{"type": "scatter3d", "colspan": 2}, {"type": "scatter3d", "colspan": 2}]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
360/18: plot_3d_interactive(X_digits, y_digits)
360/19:
fig = plt.figure(1, figsize=(9, 9))
#Clear the current figure
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
# Clear the current axes
plt.cla()

# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).astype(np.int64)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor='k')
360/20:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
360/21: plot_3d_interactive(X_digits, y_digits)
360/22:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    column_widths=[0.6, 0.4],
    row_heights=[0.4, 0.6],
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
360/23: plot_3d_interactive(X_digits, y_digits)
361/1:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
361/2:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
361/3:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
361/4:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
361/5:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
361/6: plot_3d_interactive(X_digits, y_digits)
361/7:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    column_widths=[0.6, 0.4],
    row_heights=[0.4, 0.6],
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
361/8: plot_3d_interactive(X_digits, y_digits)
361/9:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    column_widths=[0.6, 0.4],
    row_heights=[1, 1],
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.show()
361/10: plot_3d_interactive(X_digits, y_digits)
361/11:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=800
    )
    
    fig.show()
361/12: plot_3d_interactive(X_digits, y_digits)
361/13:
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900
    )
    
    fig.show()
361/14: plot_3d_interactive(X_digits, y_digits)
361/15:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0)
    )
    
    fig.show()
361/16: plot_3d_interactive(X_digits, y_digits)
361/17:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0)
    )
    
    fig.show()
361/18: plot_3d_interactive(X_digits, y_digits)
361/19:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
361/20:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/21: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris, X_s_curve, color, 'Title')
361/22: plot_3d_interactive(X_digits, y_digits)
361/23:
pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca.fit_transform(X_digits) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca.fit_transform(X_iris) 

pca = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca.fit_transform(X_s_curve)
361/24:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False,
        showticklabels=False
    )
    
    fig.show()
361/25: plot_3d_interactive(X_digits, y_digits)
361/26:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]])

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False,
        tickmode = None,
    )
    
    fig.show()
361/27: plot_3d_interactive(X_digits, y_digits)
361/28:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=3,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2", "Dataset 3"))
    )

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    trace1 = go.Scatter3d(x=X_dataset3[:, 0], y=X_dataset3[:, 1], z=X_dataset3[:, 2], mode='markers', marker_color=y_dataset3)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace1, 3, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False,
        tickmode = None,
    )
    
    fig.show()
361/29:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=3,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2", "Dataset 3"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    trace1 = go.Scatter3d(x=X_dataset3[:, 0], y=X_dataset3[:, 1], z=X_dataset3[:, 2], mode='markers', marker_color=y_dataset3)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace1, 3, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False,
        tickmode = None,
    )
    
    fig.show()
361/30: plot_3d_interactive(X_digits, y_digits)
361/31: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris, X_s_curve, color)
361/32:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=3,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2", "Dataset 3"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    trace2 = go.Scatter3d(x=X_dataset3[:, 0], y=X_dataset3[:, 1], z=X_dataset3[:, 2], mode='markers', marker_color=y_dataset3)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace2, 3, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False,
        tickmode = None,
    )
    
    fig.show()
361/33: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris, X_s_curve, color)
361/34:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2", "Dataset 3"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    trace2 = go.Scatter3d(x=X_dataset3[:, 0], y=X_dataset3[:, 1], z=X_dataset3[:, 2], mode='markers', marker_color=y_dataset3)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace2, 3, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False,
        tickmode = None,
    )
    
    fig.show()
361/35: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris, X_s_curve, color)
361/36:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2", "Dataset 3"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    trace2 = go.Scatter3d(x=X_dataset3[:, 0], y=X_dataset3[:, 1], z=X_dataset3[:, 2], mode='markers', marker_color=y_dataset3)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace2, 3, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=800,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/37: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris, X_s_curve, color)
361/38:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2", "Dataset 3"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    trace2 = go.Scatter3d(x=X_dataset3[:, 0], y=X_dataset3[:, 1], z=X_dataset3[:, 2], mode='markers', marker_color=y_dataset3)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace2, 3, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=9000,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/39: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris, X_s_curve, color)
361/40: plot_3d(X_digits, y_digits, X_iris, y_iris, X_s_curve, color, 'Title')
361/41:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2", "Dataset 3"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    trace2 = go.Scatter3d(x=X_dataset3[:, 0], y=X_dataset3[:, 1], z=X_dataset3[:, 2], mode='markers', marker_color=y_dataset3)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace2, 3, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/42: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris, X_s_curve, color)
361/43:
pca_digits = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca_digits.fit_transform(X_digits) 

pca_iris = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca_iris.fit_transform(X_iris) 

pca_s_curve = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca_s_curve.fit_transform(X_s_curve)
361/44: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris, X_s_curve, color)
361/45: print("Explained vartiation in digits dataset: %s" % pca_digits.explained_variance_ratio_)
361/46: print("Explained vartiation in digits dataset: %s" % pca_digits.explained_variance_)
361/47: print("Explained vartiation in digits dataset: %s" % pca_digits.explained_variance_ratio_)
361/48:
print("Explained vartiation in digits dataset: %s" % pca_digits.explained_variance_ratio_)
print("Explained vartiation in iris dataset: %s" % pca_iris.explained_variance_ratio_)
print("Explained vartiation in s-curve dataset: %s" % pca_s_curve.explained_variance_ratio_)
361/49:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
361/50:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
361/51:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2", "Dataset 3"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    trace2 = go.Scatter3d(x=X_dataset3[:, 0], y=X_dataset3[:, 1], z=X_dataset3[:, 2], mode='markers', marker_color=y_dataset3)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace2, 3, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/52:
mds_digits = MDS(n_components=3)
# Fit the data from X, and returns the embedded coordinates
X_digits = mds_digits.fit_transform(X_digits) 

mds_iris = MDS(n_components=3)
# Fit the data from X, and returns the embedded coordinates
X_iris = mds_iris.fit_transform(X_iris) 

mds_s_curve = MDS(n_components=3)
# Fit the data from X, and returns the embedded coordinates
X_s_curve = mds_s_curve.fit_transform(X_s_curve)
361/53:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components for digits dataset: %s" % pca_digits.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components for iris dataset: %s" % pca_iris.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components for dataset: %s" % pca_s_curve.explained_variance_ratio_)
361/54:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components(digits dataset): %s" % pca_digits.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components for iris dataset: %s" % pca_iris.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components for dataset: %s" % pca_s_curve.explained_variance_ratio_)
361/55:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components (digits dataset): %s" % pca_digits.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components (iris dataset): %s" % pca_iris.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components (s-curve dataset): %s" % pca_s_curve.explained_variance_ratio_)
361/56:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components(digits dataset): %s" % pca_digits.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components(iris dataset): %s" % pca_iris.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components(s-curve dataset): %s" % pca_s_curve.explained_variance_ratio_)
361/57:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components(digits dataset): %s" % np.around(pca_digits.explained_variance_ratio_, 2)
print("Percentage of variance explained by each of the selected components(iris dataset): %s" % pca_iris.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components(s-curve dataset): %s" % pca_s_curve.explained_variance_ratio_)
361/58:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components(digits dataset): %s" % np.around(pca_digits.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(iris dataset): %s" % pca_iris.explained_variance_ratio_)
print("Percentage of variance explained by each of the selected components(s-curve dataset): %s" % pca_s_curve.explained_variance_ratio_)
361/59:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components(digits dataset): %s" % np.around(pca_digits.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(iris dataset): %s" % np.around(pca_iris.explained_variance_ratio_))
print("Percentage of variance explained by each of the selected components(s-curve dataset): %s" % np.around(pca_s_curve.explained_variance_ratio_))
361/60:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components(digits dataset): %s" % np.around(pca_digits.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(iris dataset): %s" % np.around(pca_iris.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(s-curve dataset): %s" % np.around(pca_s_curve.explained_variance_ratio_, 2))
361/61:
mds_digits = MDS(n_components=3)
# Fit the data from X, and returns the embedded coordinates
X_digits = mds_digits.fit_transform(X_digits) 

mds_iris = MDS(n_components=3)
# Fit the data from X, and returns the embedded coordinates
X_iris = mds_iris.fit_transform(X_iris) 

mds_s_curve = MDS(n_components=3)
# Fit the data from X, and returns the embedded coordinates
X_s_curve = mds_s_curve.fit_transform(X_s_curve)
361/62: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris, X_s_curve, color)
361/63: mds_digits.stress_
361/64: mds_iris.stress_
361/65: mds_s_curve.stress_
361/66: mds_iris.stress_
361/67:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
361/68:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

# Dataset4
X_swiss_role,y_swiss_role = make_swiss_roll(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
print("Dataset4: %s" % (X_swiss_role.shape, ))
361/69:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components(digits dataset): %s" % np.around(pca_digits.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(iris dataset): %s" % np.around(pca_iris.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(s-curve dataset): %s" % np.around(pca_s_curve.explained_variance_ratio_, 2))
361/70:
## Using plotly
def plot_2d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=3, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2", "Dataset 3"))
    

    trace0 = go.Scatter(x=X_dataset1[:, 0], y=X_dataset1[:, 1], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter(x=X_dataset2[:, 0], y=X_dataset2[:, 1], mode='markers', marker_color=y_dataset2)
    trace2 = go.Scatter(x=X_dataset3[:, 0], y=X_dataset3[:, 1], mode='markers', marker_color=y_dataset3)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    fig.append_trace(trace2, 3, 1)
    
    fig.update_layout(
        title_text='2D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/71:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/72:
## Using plotly
def plot_2d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter", "colspan": 2}, None],
           [{"type": "scatter", "colspan": 2}, None],
           [{"type": "scatter", "colspan": 2}, None]],
    subplot_titles=("Dataset 3", "Dataset 4"))

    trace0 = go.Scatter(x=X_dataset1[:, 0], y=X_dataset1[:, 1], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter(x=X_dataset2[:, 0], y=X_dataset2[:, 1], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='2D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/73: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
361/74: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
361/75:
pca_digits = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca_digits.fit_transform(X_digits) 

pca_iris = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca_iris.fit_transform(X_iris) 

pca_s_curve = decomposition.PCA(n_components=2)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca_s_curve.fit_transform(X_s_curve) 

pca_swiss_roll = decomposition.PCA(n_components=2)
# Fit the model with X and apply the dimensionality reduction on X.
X_swiss_role = pca_swiss_roll.fit_transform(X_swiss_role)
361/76:
pca_digits = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca_digits.fit_transform(X_digits) 

pca_iris = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca_iris.fit_transform(X_iris) 

pca_s_curve = decomposition.PCA(n_components=2)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca_s_curve.fit_transform(X_s_curve) 

pca_swiss_roll = decomposition.PCA(n_components=2)
# Fit the model with X and apply the dimensionality reduction on X.
X_swiss_role = pca_swiss_roll.fit_transform(X_swiss_role)
361/77: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
361/78: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
361/79:
pca_digits = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_digits = pca_digits.fit_transform(X_digits) 

pca_iris = decomposition.PCA(n_components=3)
# Fit the model with X and apply the dimensionality reduction on X.
X_iris = pca_iris.fit_transform(X_iris) 

pca_s_curve = decomposition.PCA(n_components=2)
# Fit the model with X and apply the dimensionality reduction on X.
X_s_curve = pca_s_curve.fit_transform(X_s_curve) 

pca_swiss_roll = decomposition.PCA(n_components=2)
# Fit the model with X and apply the dimensionality reduction on X.
X_swiss_role = pca_swiss_roll.fit_transform(X_swiss_role)
361/80:
## Using plotly
def plot_2d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter", "colspan": 2}, None],
           [{"type": "scatter", "colspan": 2}, None],
           [{"type": "scatter", "colspan": 2}, None]],
    subplot_titles=("Dataset 3", "Dataset 4"))

    trace0 = go.Scatter(x=X_dataset1[:, 0], y=X_dataset1[:, 1], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter(x=X_dataset2[:, 0], y=X_dataset2[:, 1], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='2D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/81: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
361/82:
## Using plotly
def plot_2d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter", "colspan": 2}, None],
           [{"type": "scatter", "colspan": 2}, None]],
    subplot_titles=("Dataset 3", "Dataset 4"))

    trace0 = go.Scatter(x=X_dataset1[:, 0], y=X_dataset1[:, 1], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter(x=X_dataset2[:, 0], y=X_dataset2[:, 1], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='2D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/83: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
361/84: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
361/85:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components(digits dataset): %s" % np.around(pca_digits.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(iris dataset): %s" % np.around(pca_iris.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(s-curve dataset): %s" % np.around(pca_s_curve.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(swiss-roll dataset): %s" % np.around(pca_s_curve.explained_variance_ratio_, 2))
361/86: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
361/87: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
361/88:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
361/89:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

# Dataset4
X_swiss_role,y_swiss_role = make_swiss_roll(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
print("Dataset4: %s" % (X_swiss_role.shape, ))
361/90:
mds_digits = MDS(n_components=3)
# Fit the data from X, and returns the embedded coordinates
X_digits = mds_digits.fit_transform(X_digits) 

mds_iris = MDS(n_components=3)
# Fit the data from X, and returns the embedded coordinates
X_iris = mds_iris.fit_transform(X_iris) 

mds_s_curve = MDS(n_components=2)
# Fit the data from X, and returns the embedded coordinates
X_s_curve = mds_s_curve.fit_transform(X_s_curve) 

mds_swiss_role = MDS(n_components=2)
# Fit the data from X, and returns the embedded coordinates
X_swiss_role = mds_swiss_role.fit_transform(X_s_curve)
361/91:
#Percentage of variance explained by each of the selected components.
print("Percentage of variance explained by each of the selected components(digits dataset): %s" % np.around(pca_digits.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(iris dataset): %s" % np.around(pca_iris.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(s-curve dataset): %s" % np.around(pca_s_curve.explained_variance_ratio_, 2))
print("Percentage of variance explained by each of the selected components(swiss-roll dataset): %s" % np.around(pca_s_curve.explained_variance_ratio_, 2))
361/92: mds_iris.stress_
361/93: mds_s_curve.stress_
361/94: mds_swiss_role.stress_
361/95: mds_digits.stress_
361/96:
# In MDS, we are trying to model the distances. Hence, the most obvious choice 
# for a goodness-of-fit statistic is one based on the differences between the actual distances and their
# predicted values. Such a measure is called stress.

print("Stress Value (digits dataset): %s" % np.around(mds_digits.stress_, 2))
print("Stress Value (iris dataset): %s" % np.around(mds_iris.stress_, 2))
print("Stress Value (s-curve dataset): %s" % np.around(mds_s_curve.stress_, 2))
print("Stress Value (swiss-roll dataset): %s" % np.around(mds_s_curve.stress_, 2))
361/97: mds_s_curve.stress_
361/98:
# In MDS, we are trying to model the distances. Hence, the most obvious choice 
# for a goodness-of-fit statistic is one based on the differences between the actual distances and their
# predicted values. Such a measure is called stress.
#**  MDS fits with stress values near zero are the best **

print("Stress Value (digits dataset): %s" % np.around(mds_digits.stress_, 2))
print("Stress Value (iris dataset): %s" % np.around(mds_iris.stress_, 2))
print("Stress Value (s-curve dataset): %s" % np.around(mds_s_curve.stress_, 2))
print("Stress Value (swiss-roll dataset): %s" % np.around(mds_s_curve.stress_, 2))
361/99: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
361/100:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
361/101:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

# Dataset4
X_swiss_role,y_swiss_role = make_swiss_roll(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
print("Dataset4: %s" % (X_swiss_role.shape, ))
361/102:
## Using Matplotlib
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
361/103:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/104:
## Using plotly
def plot_2d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter", "colspan": 2}, None],
           [{"type": "scatter", "colspan": 2}, None]],
    subplot_titles=("Dataset 3", "Dataset 4"))

    trace0 = go.Scatter(x=X_dataset1[:, 0], y=X_dataset1[:, 1], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter(x=X_dataset2[:, 0], y=X_dataset2[:, 1], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='2D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/105:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
361/106:
isomap_digits = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_digits = isomap_digits.fit_transform(X_digits) 

isomap_iris = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='kd_tree')
# Fit the data from X, and returns the embedded coordinates
X_iris = isomap_iris.fit_transform(X_iris) 

isomap_s_curve = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_s_curve = isomap_s_curve.fit_transform(X_s_curve) 

isomap_swiss_role = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_swiss_role = isomap_swiss_role.fit_transform(X_s_curve)
361/107: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
361/108: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
361/109:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
361/110: euclidean_distances(X_digits, X_digits)
361/111:
print(X_digits)
euclidean_distances(X_digits, X_digits)
361/112:
isomap_digits = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_digits_fit = isomap_digits.fit_transform(X_digits) 

isomap_iris = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='kd_tree')
# Fit the data from X, and returns the embedded coordinates
X_iris_fit = isomap_iris.fit_transform(X_iris) 

isomap_s_curve = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_s_curve_fit = isomap_s_curve.fit_transform(X_s_curve) 

isomap_swiss_role = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_swiss_role_fit = isomap_swiss_role.fit_transform(X_s_curve)
361/113:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
361/114:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

# Dataset4
X_swiss_role,y_swiss_role = make_swiss_roll(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
print("Dataset4: %s" % (X_swiss_role.shape, ))
361/115:
## Using Matplotlib
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
361/116:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/117:
## Using plotly
def plot_2d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter", "colspan": 2}, None],
           [{"type": "scatter", "colspan": 2}, None]],
    subplot_titles=("Dataset 3", "Dataset 4"))

    trace0 = go.Scatter(x=X_dataset1[:, 0], y=X_dataset1[:, 1], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter(x=X_dataset2[:, 0], y=X_dataset2[:, 1], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='2D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
361/118:
isomap_digits = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_digits_fit = isomap_digits.fit_transform(X_digits) 

isomap_iris = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='kd_tree')
# Fit the data from X, and returns the embedded coordinates
X_iris_fit = isomap_iris.fit_transform(X_iris) 

isomap_s_curve = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_s_curve_fit = isomap_s_curve.fit_transform(X_s_curve) 

isomap_swiss_role = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_swiss_role_fit = isomap_swiss_role.fit_transform(X_s_curve)
361/119: euclidean_distances(X_digits, X_digits)
361/120:
D_fit = euclidean_distances(X_digits_fit, X_digits_fit)
D = euclidean_distances(X_digits, X_digits)
361/121:
D_fit = euclidean_distances(X_digits_fit, X_digits_fit)
D = euclidean_distances(X_digits, X_digits)

D.shape
361/122:
D_fit = euclidean_distances(X_digits_fit, X_digits_fit)
D = euclidean_distances(X_digits, X_digits)

D_fit.shape
361/123:
# In MDS, we are trying to model the distances. Hence, the most obvious choice 
# for a goodness-of-fit statistic is one based on the differences between the actual distances and their
# predicted values. Such a measure is called stress.
#**  MDS fits with stress values near zero are the best **

print("Stress Value (digits dataset): %s" % np.around(mds_digits.stress_, 2))
print("Stress Value (iris dataset): %s" % np.around(mds_iris.stress_, 2))
print("Stress Value (s-curve dataset): %s" % np.around(mds_s_curve.stress_, 2))
print("Stress Value (swiss-roll dataset): %s" % np.around(mds_s_curve.stress_, 2))
361/124:
D_fit = euclidean_distances(X_digits_fit, X_digits_fit)
D = euclidean_distances(X_digits, X_digits)

isomap_digits.kernel_pca_.fit_transform(D_fit, D)
361/125: isomap_digits.reconstruction_error
361/126:
isomap_digits = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_digits = isomap_digits.fit_transform(X_digits) 

isomap_iris = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='kd_tree')
# Fit the data from X, and returns the embedded coordinates
X_iris = isomap_iris.fit_transform(X_iris) 

isomap_s_curve = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_s_curve = isomap_s_curve.fit_transform(X_s_curve) 

isomap_swiss_role = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_swiss_role = isomap_swiss_role.fit_transform(X_s_curve)
361/127: isomap_digits.reconstruction_error
361/128:
c = isomap_digits.reconstruction_error
print(c)
361/129: isomap_digits.reconstruction_error(X_digits)
361/130: isomap_digits.reconstruction_error()
361/131:
# Isomap Cost function 
```
The cost function of an isomap embedding is

E = frobenius_norm[K(D) - K(D_fit)] / n_samples

Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit, and K is the isomap kernel:
```
isomap_digits.reconstruction_error()
361/132:
# The cost function of an isomap embedding is
# E = frobenius_norm[K(D) - K(D_fit)] / n_samples
# Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit,
# and K is the isomap kernel
print("Error (digits dataset): %s" % np.around(isomap_digits.reconstruction_error(), 2))
print("Error (iris dataset): %s" % np.around(isomap_iris.reconstruction_error(), 2))
print("Error (s-curve dataset): %s" % np.around(isomap_s_curve.reconstruction_error(), 2))
print("Error (swiss-roll dataset): %s" % np.around(isomap_swiss_role.reconstruction_error(), 2))
361/133:
# In MDS, we are trying to model the distances. Hence, the most obvious choice 
# for a goodness-of-fit statistic is one based on the differences between the actual distances and their
# predicted values. Such a measure is called stress.
#**  MDS fits with stress values near zero are the best **

print("Stress Value (digits dataset): %s" % np.around(mds_digits.stress_, 2))
print("Stress Value (iris dataset): %s" % np.around(mds_iris.stress_, 2))
print("Stress Value (s-curve dataset): %s" % np.around(mds_s_curve.stress_, 2))
print("Stress Value (swiss-roll dataset): %s" % np.around(mds_s_curve.stress_, 2))
361/134: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
361/135: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
361/136:
# The cost function of an isomap embedding is
# E = frobenius_norm[K(D) - K(D_fit)] / n_samples
# Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit,
# and K is the isomap kernel
print("Error (digits dataset): %s" % np.around(isomap_digits.reconstruction_error(), 2))
print("Error (iris dataset): %s" % np.around(isomap_iris.reconstruction_error(), 2))
print("Error (s-curve dataset): %s" % np.around(isomap_s_curve.reconstruction_error(), 2))
print("Error (swiss-roll dataset): %s" % np.around(isomap_swiss_role.reconstruction_error(), 2))
   1:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
from sklearn.manifold import LocallyLinearEmbedding
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
   2:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
from sklearn.manifold import LocallyLinearEmbedding
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
   3:
lle_digits = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_digits = lle_digits.fit_transform(X_digits) 

lle_iris = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='kd_tree')
# Fit the data from X, and returns the embedded coordinates
X_iris = lle_iris.fit_transform(X_iris) 

lle_s_curve = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_s_curve = lle_s_curve.fit_transform(X_s_curve) 

lle_swiss_role = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_swiss_role = lle_swiss_role.fit_transform(X_s_curve)
   4:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
from sklearn.manifold import LocallyLinearEmbedding
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
   5:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

# Dataset4
X_swiss_role,y_swiss_role = make_swiss_roll(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
print("Dataset4: %s" % (X_swiss_role.shape, ))
   6:
lle_digits = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_digits = lle_digits.fit_transform(X_digits) 

lle_iris = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='kd_tree')
# Fit the data from X, and returns the embedded coordinates
X_iris = lle_iris.fit_transform(X_iris) 

lle_s_curve = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_s_curve = lle_s_curve.fit_transform(X_s_curve) 

lle_swiss_role = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_swiss_role = lle_swiss_role.fit_transform(X_s_curve)
   7: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
   8:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
   9:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
  10:
## Using plotly
def plot_2d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter", "colspan": 2}, None],
           [{"type": "scatter", "colspan": 2}, None]],
    subplot_titles=("Dataset 3", "Dataset 4"))

    trace0 = go.Scatter(x=X_dataset1[:, 0], y=X_dataset1[:, 1], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter(x=X_dataset2[:, 0], y=X_dataset2[:, 1], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='2D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
  11: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
  12: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
  13:
print("Error (digits dataset): %s" % np.around(lle_digits.reconstruction_error(), 2))
print("Error (iris dataset): %s" % np.around(lle_iris.reconstruction_error(), 2))
print("Error (s-curve dataset): %s" % np.around(lle_s_curve.reconstruction_error(), 2))
print("Error (swiss-roll dataset): %s" % np.around(lle_swiss_role.reconstruction_error(), 2))
  14: print("Error (digits dataset): %s" % np.around(lle_digits.reconstruction_error_, 2))
  15: print("Error (digits dataset): %s" % np.around(lle_digits.reconstruction_error_, 4))
  16: print("Error (digits dataset): %s" % np.around(lle_digits.reconstruction_error_, 4))
  17: print("Error (digits dataset): %s" % lle_digits.reconstruction_error_)
  18:
print("Error (digits dataset): %s" % lle_digits.reconstruction_error_)
print("Error (iris dataset): %s" % lle_iris.reconstruction_error_)
print("Error (s_curve dataset): %s" % lle_s_curve.reconstruction_error_)
print("Error (swiss role dataset): %s" % lle_swiss_role.reconstruction_error_)
  19: print("Error (digits dataset): %s" % np.around(lle_digits.reconstruction_error_, 7))
  20: print("Error (digits dataset): %s" % np.around(lle_digits.reconstruction_error_, 6))
  21:
print("Error (digits dataset): %s" % np.around(lle_digits.reconstruction_error_, 6))
print("Error (iris dataset): %s" % np.around(lle_iris.reconstruction_error_, 6))
print("Error (s-curve dataset): %s" % np.around(lle_s_curve.reconstruction_error_, 6))
print("Error (swiss-role dataset): %s" % np.around(lle_swiss_role.reconstruction_error_, 6))
  22:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
from sklearn.manifold import LocallyLinearEmbedding
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
  23:
# Dataset1
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# Dataset2
iris = load_iris()
X_iris = iris.data
y_iris = iris.target

# Dataset3
X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)

# Dataset4
X_swiss_role,y_swiss_role = make_swiss_roll(n_samples=1000, noise=0.0, random_state=0)

print("Dataset1: %s" % (X_digits.shape, ))
print("Dataset2: %s" % (X_iris.shape, ))
print("Dataset3: %s" % (X_s_curve.shape, ))
print("Dataset4: %s" % (X_swiss_role.shape, ))
  24:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
  25:
## Using plotly
def plot_2d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter", "colspan": 2}, None],
           [{"type": "scatter", "colspan": 2}, None]],
    subplot_titles=("Dataset 3", "Dataset 4"))

    trace0 = go.Scatter(x=X_dataset1[:, 0], y=X_dataset1[:, 1], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter(x=X_dataset2[:, 0], y=X_dataset2[:, 1], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='2D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
  26:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
from sklearn.manifold import LocallyLinearEmbedding
from sklearn.manifold import SpectralEmbedding
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
  27:
spectral_digits = SpectralEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_digits = spectral_digits.fit_transform(X_digits) 

spectral_iris = SpectralEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='kd_tree')
# Fit the data from X, and returns the embedded coordinates
X_iris = spectral_iris.fit_transform(X_iris) 

spectral_s_curve = SpectralEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_s_curve = spectral_s_curve.fit_transform(X_s_curve) 

spectral_swiss_role = SpectralEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')
# Fit the data from X, and returns the embedded coordinates
X_swiss_role = spectral_swiss_role.fit_transform(X_s_curve)
  28:
spectral_digits = SpectralEmbedding(n_components = 3,affinity='rbf', n_neighbors=10)
# Fit the data from X, and returns the embedded coordinates
X_digits = spectral_digits.fit_transform(X_digits) 

spectral_iris = SpectralEmbedding(n_components = 3,affinity='rbf', n_neighbors=10)
# Fit the data from X, and returns the embedded coordinates
X_iris = spectral_iris.fit_transform(X_iris) 

spectral_s_curve = SpectralEmbedding(n_components = 3,affinity='rbf', n_neighbors=10)
# Fit the data from X, and returns the embedded coordinates
X_s_curve = spectral_s_curve.fit_transform(X_s_curve) 

spectral_swiss_role = SpectralEmbedding(n_components = 3,affinity='rbf', n_neighbors=10)
# Fit the data from X, and returns the embedded coordinates
X_swiss_role = spectral_swiss_role.fit_transform(X_s_curve)
  29: plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)
  30: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
  31: spectral_digits.n_neighbors
  32: spectral_digits.gamma_
  33: plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)
  34: spectral_digits.affinity_matrix_
  35: spectral_digits.affinity
  36: spectral_digits.embedding_
  37:
## Using plotly
def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):
    # Initialize figure with subplots
    fig = make_subplots(
    rows=2, cols=2,
    specs=[[{"type": "scatter3d", "colspan": 2}, None],
           [{"type": "scatter3d", "colspan": 2}, None]],
    subplot_titles=("Dataset 1", "Dataset 2"))
    

    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)
    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)
    
    fig.append_trace(trace0, 1, 1)
    fig.append_trace(trace1, 2, 1)
    
    fig.update_layout(
        title_text='3D subplots with different colorscales',
        height=1000,
        width=900,
        margin=dict(l=0, r=0, b=0, t=0),
        showlegend=False
    )
    
    fig.show()
  38: _ih[-5:]
  39: %history -g
363/1:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
from sklearn.manifold import LocallyLinearEmbedding
from sklearn.manifold import SpectralEmbedding
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
  40:
def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):
    fig = plt.figure(figsize=(15, 8))
    #Clear the current figure
    plt.clf()
    ax = fig.add_subplot(1, 3, 1, projection='3d')
    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 2, projection='3d')
    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')
    
    ax = fig.add_subplot(1, 3, 3, projection='3d')
    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')
    plt.show()
  41:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
from sklearn.manifold import LocallyLinearEmbedding
from sklearn.manifold import SpectralEmbedding
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
  42:
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import decomposition
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_s_curve
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import MDS
from sklearn.manifold import Isomap
from sklearn.manifold import LocallyLinearEmbedding
from sklearn.manifold import SpectralEmbedding
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from sklearn.metrics.pairwise import euclidean_distances
  43: _ih[-15:]
  44: _ih[-45:]
  45: _ih[-65:]
  46: %history -g -f  DimensionalityReduction.py
