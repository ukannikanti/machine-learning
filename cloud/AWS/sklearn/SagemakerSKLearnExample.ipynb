{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasource_helper import read_from_s3\n",
    "from datasource_helper import read_from_redshift\n",
    "from datasource_helper import read_from_vertica\n",
    "from datasource_helper import write_df_to_s3_csv\n",
    "import getpass\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from S3, Vertica or Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Vertica\n",
    "SQL_QUERY = \"select * from MDA.on_training1 limit 10\"\n",
    "username = getpass.getpass()\n",
    "password = getpass.getpass()\n",
    "df = read_from_vertica(username=username, password=password, sql_query=SQL_QUERY)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Redshift\n",
    "SQL_QUERY = \"select * from MDA.on_training1 limit 10\"\n",
    "username = getpass.getpass()\n",
    "password = getpass.getpass()\n",
    "df = read_from_redshift(username=username, password=password, sql_query=SQL_QUERY)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from S3 \n",
    "s3Obj = read_from_s3(\"choice-mlflow-input\", \"demo/datasets/housing.csv\")\n",
    "df = pd.read_csv(s3Obj['Body'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Stage\n",
    "    1. Missing Values\n",
    "    2. Categorical Variables\n",
    "    3. Prepare Datasets for train/test/cross-validation\n",
    "    4. Feature Scaling on train dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running on Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output locations\n",
    "train_features_output_path = os.path.join('/home/ec2-user/demo_datasets/train', 'train_features.csv')\n",
    "train_labels_output_path = os.path.join('/home/ec2-user/demo_datasets/train', 'train_labels.csv')\n",
    "test_features_output_path = os.path.join('/home/ec2-user/demo_datasets/test', 'test_features.csv')\n",
    "test_labels_output_path = os.path.join('/home/ec2-user/demo_datasets/test', 'test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income_category'] = np.ceil(df['median_income']/1.5)\n",
    "df['income_category'].where(df['median_income'] < 5, 5.0, inplace=True)\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True, stratify=df['income_category'])\n",
    "\n",
    "# Now we should remove the income_category attribute, so the data is back to its original state:\n",
    "train_set = train_set.drop(\"income_category\", axis=1)\n",
    "test_set = test_set.drop(\"income_category\", axis=1)\n",
    "\n",
    "X_train = train_set.drop(columns=\"median_house_value\") \n",
    "X_test = test_set.drop(columns=\"median_house_value\") \n",
    "\n",
    "y_train = train_set[\"median_house_value\"].copy()\n",
    "y_test = test_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Handle Missing Values\n",
    "simple_imputer = SimpleImputer(missing_values=np.nan, strategy='mean', fill_value=None, \n",
    "                               verbose=0, copy=True)\n",
    "\n",
    "simple_imputer_categorical = SimpleImputer(missing_values=np.nan, strategy='most_frequent', fill_value=None, \n",
    "                               verbose=0, copy=True)\n",
    "\n",
    "# Handle Categorical Variables\n",
    "one_hot_encoder = OneHotEncoder(categories='auto', sparse=True, dtype=np.float64, handle_unknown='error')\n",
    "\n",
    "# Feature Scaling\n",
    "std_scalar = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "\n",
    "numeric_features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', \n",
    "                    'total_bedrooms', 'population', 'households', 'median_income']\n",
    "\n",
    "categorical_features = ['ocean_proximity']\n",
    "\n",
    "# A transformer to apply to apply on numerical features.\n",
    "numeric_transformer = Pipeline(steps=[('imputer', simple_imputer), ('scaler', std_scalar)])\n",
    "categorical_transformer = Pipeline(steps=[('imputer', simple_imputer_categorical), ('onehot', one_hot_encoder)])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "                    ('numerical', numeric_transformer, numeric_features),\n",
    "                    ('categorical', categorical_transformer, categorical_features)])\n",
    "\n",
    "train_features = preprocessor.fit_transform(X_train)\n",
    "test_features = preprocessor.fit_transform(X_test)\n",
    "\n",
    "print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "print('Saving training features to {}'.format(train_features_output_path))\n",
    "pd.DataFrame(train_features).to_csv(train_features_output_path, header=False, index=False)\n",
    "    \n",
    "print('Saving test features to {}'.format(test_features_output_path))\n",
    "pd.DataFrame(test_features).to_csv(test_features_output_path, header=False, index=False)\n",
    "    \n",
    "print('Saving training labels to {}'.format(train_labels_output_path))\n",
    "y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "    \n",
    "print('Saving test labels to {}'.format(test_labels_output_path))\n",
    "y_test.to_csv(test_labels_output_path, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon SageMaker Processing Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import os\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_features.csv')\n",
    "    train_labels_output_path = os.path.join('/opt/ml/processing/train', 'train_labels.csv')\n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_features.csv')\n",
    "    test_labels_output_path = os.path.join('/opt/ml/processing/test', 'test_labels.csv')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'housing.csv')   \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df['income_category'] = np.ceil(df['median_income']/1.5)\n",
    "    df['income_category'].where(df['median_income'] < 5, 5.0, inplace=True)\n",
    "    train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True, stratify=df['income_category'])\n",
    "\n",
    "    # Now we should remove the income_category attribute, so the data is back to its original state:\n",
    "    train_set = train_set.drop(\"income_category\", axis=1)\n",
    "    test_set = test_set.drop(\"income_category\", axis=1)\n",
    "\n",
    "    X_train = train_set.drop(columns=\"median_house_value\") \n",
    "    X_test = test_set.drop(columns=\"median_house_value\") \n",
    "\n",
    "    y_train = train_set[\"median_house_value\"].copy()\n",
    "    y_test = test_set[\"median_house_value\"].copy()\n",
    "\n",
    "    # Handle Missing Values\n",
    "    simple_imputer = SimpleImputer(missing_values=np.nan, strategy='mean', fill_value=None, \n",
    "                                   verbose=0, copy=True)\n",
    "\n",
    "    simple_imputer_categorical = SimpleImputer(missing_values=np.nan, strategy='most_frequent', fill_value=None, \n",
    "                                   verbose=0, copy=True)\n",
    "\n",
    "    # Handle Categorical Variables\n",
    "    one_hot_encoder = OneHotEncoder(categories='auto', sparse=True, dtype=np.float64, handle_unknown='error')\n",
    "\n",
    "    # Feature Scaling\n",
    "    std_scalar = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "\n",
    "    numeric_features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', \n",
    "                        'total_bedrooms', 'population', 'households', 'median_income']\n",
    "\n",
    "    categorical_features = ['ocean_proximity']\n",
    "\n",
    "    # A transformer to apply to apply on numerical features.\n",
    "    numeric_transformer = Pipeline(steps=[('imputer', simple_imputer), ('scaler', std_scalar)])\n",
    "    categorical_transformer = Pipeline(steps=[('imputer', simple_imputer_categorical), ('onehot', one_hot_encoder)])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "                        ('numerical', numeric_transformer, numeric_features),\n",
    "                        ('categorical', categorical_transformer, categorical_features)])\n",
    "\n",
    "    train_features = preprocessor.fit_transform(X_train)\n",
    "    test_features = preprocessor.fit_transform(X_test)\n",
    "\n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "\n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    pd.DataFrame(train_features).to_csv(train_features_output_path, header=False, index=False)\n",
    "\n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    pd.DataFrame(test_features).to_csv(test_features_output_path, header=False, index=False)\n",
    "\n",
    "    print('Saving training labels to {}'.format(train_labels_output_path))\n",
    "    y_train.to_csv(train_labels_output_path, header=False, index=False)\n",
    "\n",
    "    print('Saving test labels to {}'.format(test_labels_output_path))\n",
    "    y_test.to_csv(test_labels_output_path, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)\n",
    "\n",
    "sklearn_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source='s3://choice-mlflow-input/demo/datasets/housing.csv',\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train', \n",
    "                                                destination= 's3://choice-mlflow-input/demo/datasets/train/'),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test', \n",
    "                                                destination='s3://choice-mlflow-input/demo/datasets/test/')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2'])\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_s3Obj = read_from_s3(\"choice-mlflow-input\", \"demo/datasets/train/train_features.csv\")\n",
    "train_labels_s3Obj = read_from_s3(\"choice-mlflow-input\", \"demo/datasets/train/train_labels.csv\")\n",
    "train_features = pd.read_csv(train_features_s3Obj['Body'], header=None)\n",
    "train_labels = pd.read_csv(train_labels_s3Obj['Body'], header=None)\n",
    "\n",
    "\n",
    "test_features_s3Obj = read_from_s3(\"choice-mlflow-input\", \"demo/datasets/test/test_features.csv\")\n",
    "test_labels_s3Obj = read_from_s3(\"choice-mlflow-input\", \"demo/datasets/test/test_labels.csv\")\n",
    "test_features = pd.read_csv(test_features_s3Obj['Body'], header=None)\n",
    "test_labels = pd.read_csv(test_labels_s3Obj['Body'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=0.3, max_iter=1000)\n",
    "ridge.fit(train_features, train_labels)\n",
    "\n",
    "housing_predictions = ridge.predict(test_features)\n",
    "\n",
    "# Calculate Error\n",
    "mse = mean_squared_error(y_true=test_labels, y_pred=housing_predictions)\n",
    "mae = mean_absolute_error(y_true=test_labels, y_pred=housing_predictions)\n",
    "\n",
    "print(\"MSE : %0.3f \" % mse)\n",
    "print(\"MAE : %0.3f \" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model On Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import tree\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def define_hyperparameters(parser):\n",
    "    # Hyperparameters are defined here. In this simple example we are just including one hyperparameter.\n",
    "    parser.add_argument('--alpha', type=float, default=0.5)\n",
    "\n",
    "    \n",
    "def define_data_directories(parser):\n",
    "    #A string representing the path to the directory to write model artifacts to. \n",
    "    #Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    \n",
    "    #A string representing the filesystem path to write output artifacts to. \n",
    "    #Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts.\n",
    "    #These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    \n",
    "    #A string representing the path to the directory containing data in the 'train' channel\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "    #A string representing the path to the directory containing data in the 'test' channel\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "    \n",
    "    #A string representing the path to the directory containing data in the 'validation' channel\n",
    "    #parser.add_argument('--validation', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "\n",
    "    \n",
    "def train(args):\n",
    "    train_features_data = os.path.join(args.train, 'train_features.csv')\n",
    "    train_labels_data = os.path.join(args.train, 'train_labels.csv')\n",
    "    test_features_data = os.path.join(args.test, 'test_features.csv')\n",
    "    test_labels_data = os.path.join(args.test, 'test_labels.csv')\n",
    "   \n",
    "    print('Reading input data from {}'.format(args.train))\n",
    "    X_train = pd.read_csv(train_features_data, header=None)\n",
    "    y_train = pd.read_csv(train_labels_data, header=None)\n",
    "    X_test = pd.read_csv(test_features_data, header=None)\n",
    "    y_test = pd.read_csv(test_labels_data, header=None)\n",
    "\n",
    "    print('Fitting the model to data')\n",
    "    ridge = Ridge(alpha=args.alpha, max_iter=100, solver='sag')\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, ridge.predict(X_test))\n",
    "    print(\"MSE: %.4f\" % mse) \n",
    "    \n",
    "    # Dump the model to S3\n",
    "    joblib.dump(ridge, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "    \n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    print('Writing model artifacts to {}'.format(model_dir))\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    define_hyperparameters(parser=parser)\n",
    "    define_data_directories(parser=parser)\n",
    "    args = parser.parse_args()\n",
    "    train(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "script_path = 'train.py'\n",
    "    \n",
    "sklearn = SKLearn(entry_point=script_path, \n",
    "                  train_instance_type=\"ml.m4.4xlarge\",\n",
    "                  source_dir='/home/ec2-user/SageMaker/umasrivenkat_kannikanti/Demo',\n",
    "                  output_path= 's3://choice-mlflow-input/demo/output',\n",
    "                  role=role,\n",
    "                  sagemaker_session=sagemaker_session, \n",
    "                  hyperparameters={'alpha': 0.7})\n",
    "\n",
    "sklearn.fit({'train': 's3://choice-mlflow-input/demo/datasets/train/', 'test': 's3://choice-mlflow-input/demo/datasets/test/'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator = SKLearn(entry_point=script_path, \n",
    "                            train_instance_type=\"ml.m4.4xlarge\",\n",
    "                            source_dir='/home/ec2-user/SageMaker/umasrivenkat_kannikanti/Demo',\n",
    "                            output_path= 's3://choice-mlflow-input/demo/output',\n",
    "                            role=role,\n",
    "                            sagemaker_session=sagemaker_session, \n",
    "                            hyperparameters={'alpha': 0.7})\n",
    "\n",
    "hyperparameter_ranges = {'alpha': ContinuousParameter(0.5, 1.2)}\n",
    "\n",
    "## Stupid way of doing this.. but no other option.. Use it or move away from AWShit lol. \n",
    "objective_metric_name = 'MSE'\n",
    "metric_definitions = [{'Name': 'MSE',\n",
    "                       'Regex': 'MSE: ([0-9\\\\.]+)'}]\n",
    "\n",
    "tuner = HyperparameterTuner(sklearn_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=9,\n",
    "                            max_parallel_jobs=3)\n",
    "\n",
    "tuner.fit({'train': 's3://choice-mlflow-input/demo/datasets/train/', 'test': 's3://choice-mlflow-input/demo/datasets/test/'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Model & Inference Using Endpoint\n",
    "*Deploying the model to SageMaker hosting just requires a deploy call on the fitted model. This call takes an instance count and instance type*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = np.array([[0.591651, -0.691139, 1.384765, -0.486401, -0.621162, -0.731684, -0.567076, 0.655892, 1.0, 0.0, 0.0, 0.0, 0.0]])\n",
    "print(predictor.predict(test_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Endpoint cleanup\n",
    "sklearn.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Transform\n",
    "*We can also use the trained model for asynchronous batch inference on S3 data using SageMaker Batch Transform.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = sklearn.transformer(instance_count=1, instance_type='ml.t2.medium')\n",
    "write_df_to_s3_csv(df = pd.DataFrame(data=X_test), project_name=project_name, channel=\"test\")\n",
    "transformer.transform(\"s3://pricing_demo/test/\", content_type='text/csv')\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
