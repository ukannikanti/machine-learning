{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist to start a new project\n",
    "\n",
    "    1. Understand the business domain\n",
    "    2. Business Objective\n",
    "    3. How does the company expect to use and benefit from this model.\n",
    "    4. What the current solution looks like.\n",
    " \n",
    " \n",
    "These are very important points to know before we start work on any model as it will determine how we frame the problem, what algorithms will select, what performance measure you will use to evaluate your model, and how much effort you should spend tweaking it. \n",
    "\n",
    "ML Work FLow\n",
    "\n",
    "1. **Fetch data from various datasources.**\n",
    "    \n",
    "2. **Data Cleaning** <br>\n",
    "    *There are 2 types of problems we face in data preparation stage.*\n",
    "        a) Missing Values\n",
    "        b) Categorical Variables\n",
    "        \n",
    "     *Handle Missing Values:*\n",
    "          There are different approaches to handle missing values.\n",
    "              a) Get rid of the corresponding row.\n",
    "              b) Get rid of the entire column. \n",
    "              c) Set the missing values to some value. Depend on situtaion \n",
    "                  Mean or Median \n",
    "                  Expectation Maximization Algorithm \n",
    "                  Nearest Neighbour\n",
    "                  \n",
    "      *Handle Categorical Variables:*\n",
    "              As we know most machine algorithms prefer to work with numbers, so lets convert these categorical     \n",
    "              variables into numbers.\n",
    "              \n",
    "              There are 2 types: \n",
    "                  a) Nominal  => Named Categories (eg. Red, green, blue etc)\n",
    "                      Use below techinques to convert nominal values to numerical values\n",
    "                          1. One Hot Encoding (one column for each value to compare vs. all other values.)\n",
    "                          2. One Hot Encoding With Many Categorical Variables.\n",
    "                     \n",
    "                  b) Ordinal  => Categories with an implied order\n",
    "                          1. Binary Encoding     \n",
    "                          2. Label Encoding\n",
    "\n",
    "3. **Prepare Data For Test/Train/Validation** <br>\n",
    "       Key points to remember when preparing datasets for train, test and validation.\n",
    "           a) Do not generate new test set every time.\n",
    "           b) Ensure that the test set is representative of the whole dataset to avoid sampling bias.\n",
    "           \n",
    "4. **Discover and Visualize the data to Gain Insights** <br>\n",
    "       Make sure you have put the test set a side and you are only exploring the training set. \n",
    "       Understant the problem and make sure training dataset represnts visual perspective of what we want to \n",
    "       acheive. \n",
    "       \n",
    "5. **Feature Scaling** <br>\n",
    "       It is one of the most important transformations we need to apply to our data. With few exceptions, Machine \n",
    "       Learning algorithms don't perform well when the input numerical attributes have very different scales.\n",
    "       There are 2 different ways to get all attributes to have the same scale.\n",
    "           a) min-max scaling (Normalization)\n",
    "               Values are shifted and rescaled so that they end up ranging from 0 to 1. \n",
    "               $X_{initial}$ = X - $X_{mean}$/ ($X_{max}$ -  $X_{min}$)\n",
    "               \n",
    "           b) Standardization Scaling\n",
    "               It is quite different. First it subtracts the mean value and then it divides by the standard \n",
    "               deviation so that the resulting distribution has unit variance. \n",
    "               It doesn't bound values to specific range, which may be a problem for some algorithms (eg., neural \n",
    "               networks often expect input value ranging from 0 to 1). However, standardization is much less \n",
    "               effected by outliers. \n",
    "               \n",
    "6. **Choose Algorithm & Train Model** <br>\n",
    "        Train and then evaluate test or cross validation scores. \n",
    "        \n",
    "7. **Tuning Hyperparameters**<br>\n",
    "        In machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. \n",
    "        By contrast, the values of other parameters are derived via training.\n",
    "        \n",
    "        Hyperparameters can be classified as model hyperparameters, that cannot be inferred while fitting the \n",
    "        machine to the training set because they refer to the model selection task, or algorithm hyperparameters, \n",
    "        that in principle have no influence on the performance of the model but affect the speed and quality of the \n",
    "        learning process.\n",
    "        \n",
    "        There are different ways to tune the hyperparameters.\n",
    "        1. Grid Search (grid of hyperparameter values and for each combination, trains a model and scores on \n",
    "            the testing data)\n",
    "        2. Randomized Search (sets up a grid of hyperparameter values and selects random combinations to train \n",
    "            the model and score)\n",
    "        3. Bayesian Optimization (It uses Guassian process to find best parameters for model)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/ukannika/work/personal/machine-learning/datasets/housing.csv\", sep=\",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Missing Data\n",
    "\n",
    "   **Univariate Imputation [SimpleImputer]** <br>\n",
    "        One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using \n",
    "        only non-missing values in that feature dimension.\n",
    "        Supported Strategies => mean, median, most_frequent, costant\n",
    "\n",
    "   **Multivariate Imputation [IterativeImputer]** <br>  [currently it's in experimental]\n",
    "        In multivariate imputation algorithms use the entire set of available feature dimensions to estimate the \n",
    "        missing values. \n",
    "        \n",
    "   **KNNImputer**\n",
    "        Imputation for completing missing values using k-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_imputer = SimpleImputer(missing_values=np.nan, strategy='mean', fill_value=None, \n",
    "                               verbose=0, copy=True, add_indicator=False)\n",
    "\n",
    "simple_imputer_categorical = SimpleImputer(missing_values=np.nan, strategy='most_frequent', fill_value=None, \n",
    "                               verbose=0, copy=True, add_indicator=False)\n",
    "\n",
    "iterative_imputer = IterativeImputer(estimator= BayesianRidge(), missing_values=np.nan, \n",
    "                                     sample_posterior=False, max_iter=10, tol=0.001, n_nearest_features=None, \n",
    "                                     initial_strategy='mean', imputation_order='ascending', skip_complete=False, \n",
    "                                     min_value=None, max_value=None, verbose=0, random_state=None,\n",
    "                                     add_indicator=False)\n",
    "\n",
    "knn_imputer = KNNImputer(missing_values=np.nan, n_neighbors=10, weights='distance', metric='nan_euclidean', copy=True, add_indicator=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nominal\n",
    "one_hot_encoder = OneHotEncoder(categories='auto', drop=None, sparse=True, dtype=np.float64, handle_unknown='error')\n",
    "\n",
    "# Ordinal\n",
    "ordinal_encoder = OrdinalEncoder(categories='auto', dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover and Visualize the data to Gain Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(df,\n",
    "    dimensions=[\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"],\n",
    "    color=\"median_house_value\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Housing Data set',\n",
    "    dragmode='select',\n",
    "    width=800,\n",
    "    height=800,\n",
    "    hovermode='closest',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data For [Train, Test, Validation]\n",
    "\n",
    "**arrays**  \n",
    "    Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "\n",
    "**test_size**\n",
    "    If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "    If int, represents the absolute number of test samples. If None, the value is set to the complement of the train \n",
    "    size. If train_size is also None, it will be set to 0.25.\n",
    "\n",
    "**train_size**\n",
    "    If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. \n",
    "    If int, represents the absolute number of train samples. If None, the value is automatically set to the complement \n",
    "    of the test size.\n",
    "\n",
    "**random_state**\n",
    "    If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the \n",
    "    random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "**shuffle**\n",
    "    Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n",
    "\n",
    "**stratify**\n",
    "    If not None, data is split in a stratified fashion, using this as the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find attributes that most represent the whole dataset\n",
    "\n",
    "As we can see in our data median income is a very important attribute to predict median housing prices.\n",
    "We need to find important attributes to predict median housing prices. \n",
    "By finding these important attributes will help us to ensure that the test set is representative of the various \n",
    "categories of incomes in the whole dataset. \n",
    "\n",
    "For our testing purpose we are only considering one attribute which is median_income. \n",
    "\n",
    "Since median_income is a continous numerical attribute, we first need to create an income_category attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(x=df['median_income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see median income values are clustered around 2 to 5 (20000$ to 50000$), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum or else the estimate of the stratum's importance may be biased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['income_category'] = np.ceil(df['median_income']/1.5)\n",
    "df['income_category'].where(df['median_income'] < 5, 5.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "px.histogram(x=df['income_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Shape (16512, 10)\n",
      "Test Dataset Shape (4128, 10)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, shuffle=True, stratify=df['income_category'])\n",
    "\n",
    "# Now we should remove the income_category attribute, so the data is back to its original state:\n",
    "train_set = train_set.drop(\"income_category\", axis=1)\n",
    "test_set = test_set.drop(\"income_category\", axis=1)\n",
    "\n",
    "print(\"Train Dataset Shape %s\" % (train_set.shape,))\n",
    "print(\"Test Dataset Shape %s\" % (test_set.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_features = train_set.drop(columns=\"median_house_value\") \n",
    "housing_labels = train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(df,\n",
    "    dimensions=[\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"],\n",
    "    color=\"median_house_value\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Housing Data set',\n",
    "    dragmode='select',\n",
    "    width=800,\n",
    "    height=800,\n",
    "    hovermode='closest',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers\n",
    "fig = make_subplots(rows=3, cols=3)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=housing_features[\"total_rooms\"]),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=housing_features[\"median_income\"]),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=housing_features[\"housing_median_age\"]),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=housing_features[\"total_bedrooms\"]),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=housing_features[\"population\"]),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=housing_features[\"households\"]),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, width=800, title_text=\"Before Feature Scaling\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scalar = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "min_max_scalar = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "\n",
    "# This one only used for visualization & understanding after feature scaling.\n",
    "# We don't need to explicitly call this fit_transform here.. Best practice is to use SKlean pipeline preprocessing. \n",
    "after_feature_scaling_std_scalar_visualization = std_scalar.fit_transform(train_set.iloc[:, :9])\n",
    "after_feature_scaling_min_max_scalar_visualization = min_max_scalar.fit_transform(train_set.iloc[:, :9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After feature scaling\n",
    "fig = make_subplots(rows=3, cols=3)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_std_scalar_visualization[:, 4]),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_std_scalar_visualization[:, 8]),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_std_scalar_visualization[:, 3]),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_std_scalar_visualization[:, 5]),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_std_scalar_visualization[:, 6]),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_std_scalar_visualization[:, 7]),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, width=800, title_text=\"After Feature Scaling\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=3)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_min_max_scalar_visualization[:, 4]),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_min_max_scalar_visualization[:, 8]),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_min_max_scalar_visualization[:, 3]),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_min_max_scalar_visualization[:, 5]),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_min_max_scalar_visualization[:, 6]),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Box(y=after_feature_scaling_min_max_scalar_visualization[:, 7]),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, width=800, title_text=\"After Feature Scaling\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit Learn Design\n",
    "\n",
    "**Pipeline:** <br>\n",
    "      *Scikit-learn's pipeline class is a useful tool for encapsulating multiple different transformers alongside an\n",
    "      estimator into one object, so that we only have to call important methods once ( fit() , predict() , etc).*\n",
    "\n",
    "**Transformer** <br>\n",
    "       *A transformer, which can transform one dataset into another dataset perfomed by transform() method.*\n",
    "       \n",
    " **Estimator** <br>\n",
    "       *An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning \n",
    "       algorithm is an Estimator which trains on a DataFrame and produces a model. It is performed by fit() method.*\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', \n",
    "                    'total_bedrooms', 'population', 'households', 'median_income']\n",
    "\n",
    "categorical_features = ['ocean_proximity']\n",
    "\n",
    "# A transformer to apply to apply on numerical features.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', simple_imputer),\n",
    "    ('scaler', std_scalar)])\n",
    "\n",
    "numeric_transformer_iterative_imputer = Pipeline(steps=[\n",
    "    ('imputer', iterative_imputer),\n",
    "    ('scaler', std_scalar)])\n",
    "\n",
    "numeric_transformer_knn_imputer = Pipeline(steps=[\n",
    "    ('imputer', knn_imputer),\n",
    "    ('scaler', std_scalar)])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', simple_imputer_categorical),\n",
    "    ('onehot', one_hot_encoder)])\n",
    "\n",
    "categorical_transformer_knn_imputer = Pipeline(steps=[\n",
    "    ('imputer', knn_imputer),\n",
    "    ('onehot', one_hot_encoder)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_simple_imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "preprocessor_iterative_imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_iterative_imputer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "preprocessor_knn_imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_knn_imputer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_simple_imputer_train_set = preprocessor_simple_imputer.fit_transform(housing_features)\n",
    "preprocessed_iterative_imputer_train_set = preprocessor_iterative_imputer.fit_transform(housing_features)\n",
    "preprocessed_knn_imputer_train_set = preprocessor_knn_imputer.fit_transform(housing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['ocean_proximity'].unique())\n",
    "print(preprocessed_simple_imputer_train_set.shape)\n",
    "print(preprocessed_iterative_imputer_train_set.shape)\n",
    "print(preprocessed_knn_imputer_train_set.shape)\n",
    "preprocessed_knn_imputer_train_set[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check if there are any nan values in training sets: \")\n",
    "print(np.isnan(preprocessed_simple_imputer_train_set).any())\n",
    "print(np.isnan(preprocessed_simple_imputer_train_set).any())\n",
    "print(np.isnan(preprocessed_simple_imputer_train_set).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally we have the datasets for train, test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(X=preprocessed_simple_imputer_train_set, \n",
    "           fname=\"/Users/ukannika/work/personal/machine-learning/datasets/housing_features_simple_imputer.csv\", \n",
    "           fmt=\"%1.3f\", delimiter=\",\")\n",
    "\n",
    "np.savetxt(X=preprocessed_simple_imputer_train_set, \n",
    "           fname=\"/Users/ukannika/work/personal/machine-learning/datasets/housing_features_iterative_imputer.csv\", \n",
    "           fmt=\"%1.3f\", delimiter=\",\")\n",
    "\n",
    "np.savetxt(X=preprocessed_simple_imputer_train_set, \n",
    "           fname=\"/Users/ukannika/work/personal/machine-learning/datasets/housing_features_knn_imputer.csv\", \n",
    "           fmt=\"%1.3f\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write target values\n",
    "housing_labels.to_csv(\"/Users/ukannika/work/personal/machine-learning/datasets/housing_labels.csv\", sep=\",\"\n",
    "                      ,header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write test set\n",
    "X_test = test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = preprocessor_simple_imputer.fit_transform(X_test)\n",
    "np.savetxt(X=X_test_prepared, \n",
    "           fname=\"/Users/ukannika/work/personal/machine-learning/datasets/test_housing_features.csv\", \n",
    "           fmt=\"%1.3f\", delimiter=\",\")\n",
    "y_test.to_csv(\"/Users/ukannika/work/personal/machine-learning/datasets/test_housing_labels.csv\", sep=\",\", \n",
    "              header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
