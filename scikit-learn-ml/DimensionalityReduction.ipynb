{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map high dimensional data to low dimensional data using below algorithms\n",
    "\n",
    "```\n",
    "    Linear:\n",
    "        PCA\n",
    "        Dual PCA\n",
    "    \n",
    "    Manifold Learning: \n",
    "        MDS(Multi Dimensional Scaling)\n",
    "        Isomap\n",
    "        LLE (Locally Linear Embedding)\n",
    "        Spectral Embedding (Laplacian Eigen Map)\n",
    "        t-SNE\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import decomposition\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import make_s_curve\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset1\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# Dataset2\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Dataset3\n",
    "X_s_curve, color = make_s_curve(n_samples=1000, noise=0.0, random_state=0)\n",
    "\n",
    "# Dataset4\n",
    "X_swiss_role,y_swiss_role = make_swiss_roll(n_samples=1000, noise=0.0, random_state=0)\n",
    "\n",
    "print(\"Dataset1: %s\" % (X_digits.shape, ))\n",
    "print(\"Dataset2: %s\" % (X_iris.shape, ))\n",
    "print(\"Dataset3: %s\" % (X_s_curve.shape, ))\n",
    "print(\"Dataset4: %s\" % (X_swiss_role.shape, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(X_dataset1, y_dataset1, X_dataset2, y_dataset2, X_dataset3, y_dataset3, title):\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    #Clear the current figure\n",
    "    plt.clf()\n",
    "    ax = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "    scatter = ax.scatter(X_dataset1[:, 0], X_dataset1[:, 1], X_dataset1[:, 2], c=y_dataset1, cmap=plt.cm.nipy_spectral, edgecolor='k')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 2, projection='3d')\n",
    "    scatter = ax.scatter(X_dataset2[:, 0], X_dataset2[:, 1], X_dataset2[:, 2], c=y_dataset2, cmap=plt.cm.nipy_spectral, edgecolor='k')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 3, projection='3d')\n",
    "    scatter = ax.scatter(X_dataset3[:, 0], X_dataset3[:, 1], X_dataset3[:, 2], c=y_dataset3, cmap=plt.cm.nipy_spectral, edgecolor='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using plotly\n",
    "def plot_3d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):\n",
    "    # Initialize figure with subplots\n",
    "    fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    specs=[[{\"type\": \"scatter3d\", \"colspan\": 2}, None],\n",
    "           [{\"type\": \"scatter3d\", \"colspan\": 2}, None]],\n",
    "    subplot_titles=(\"Dataset 1\", \"Dataset 2\"))\n",
    "    \n",
    "\n",
    "    trace0 = go.Scatter3d(x=X_dataset1[:, 0], y=X_dataset1[:, 1], z=X_dataset1[:, 2], mode='markers', marker_color=y_dataset1)\n",
    "    trace1 = go.Scatter3d(x=X_dataset2[:, 0], y=X_dataset2[:, 1], z=X_dataset2[:, 2], mode='markers', marker_color=y_dataset2)\n",
    "    \n",
    "    fig.append_trace(trace0, 1, 1)\n",
    "    fig.append_trace(trace1, 2, 1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text='3D subplots with different colorscales',\n",
    "        height=1000,\n",
    "        width=900,\n",
    "        margin=dict(l=0, r=0, b=0, t=0),\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using plotly\n",
    "def plot_2d_interactive(X_dataset1, y_dataset1, X_dataset2, y_dataset2):\n",
    "    # Initialize figure with subplots\n",
    "    fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    specs=[[{\"type\": \"scatter\", \"colspan\": 2}, None],\n",
    "           [{\"type\": \"scatter\", \"colspan\": 2}, None]],\n",
    "    subplot_titles=(\"Dataset 3\", \"Dataset 4\"))\n",
    "\n",
    "    trace0 = go.Scatter(x=X_dataset1[:, 0], y=X_dataset1[:, 1], mode='markers', marker_color=y_dataset1)\n",
    "    trace1 = go.Scatter(x=X_dataset2[:, 0], y=X_dataset2[:, 1], mode='markers', marker_color=y_dataset2)\n",
    "    \n",
    "    fig.append_trace(trace0, 1, 1)\n",
    "    fig.append_trace(trace1, 2, 1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text='2D subplots with different colorscales',\n",
    "        height=1000,\n",
    "        width=900,\n",
    "        margin=dict(l=0, r=0, b=0, t=0),\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_digits = decomposition.PCA(n_components=3)\n",
    "# Fit the model with X and apply the dimensionality reduction on X.\n",
    "X_digits = pca_digits.fit_transform(X_digits) \n",
    "\n",
    "pca_iris = decomposition.PCA(n_components=3)\n",
    "# Fit the model with X and apply the dimensionality reduction on X.\n",
    "X_iris = pca_iris.fit_transform(X_iris) \n",
    "\n",
    "pca_s_curve = decomposition.PCA(n_components=2)\n",
    "# Fit the model with X and apply the dimensionality reduction on X.\n",
    "X_s_curve = pca_s_curve.fit_transform(X_s_curve) \n",
    "\n",
    "pca_swiss_roll = decomposition.PCA(n_components=2)\n",
    "# Fit the model with X and apply the dimensionality reduction on X.\n",
    "X_swiss_role = pca_swiss_roll.fit_transform(X_swiss_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of variance explained by each of the selected components.\n",
    "print(\"Percentage of variance explained by each of the selected components(digits dataset): %s\" % np.around(pca_digits.explained_variance_ratio_, 2))\n",
    "print(\"Percentage of variance explained by each of the selected components(iris dataset): %s\" % np.around(pca_iris.explained_variance_ratio_, 2))\n",
    "print(\"Percentage of variance explained by each of the selected components(s-curve dataset): %s\" % np.around(pca_s_curve.explained_variance_ratio_, 2))\n",
    "print(\"Percentage of variance explained by each of the selected components(swiss-roll dataset): %s\" % np.around(pca_s_curve.explained_variance_ratio_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manifold Learning\n",
    "\n",
    "A manifold is a topological space that locally resembles euclidean space near each point. More precisely, each point of an n-dimensional manifold has a neighborhood that is homeomorphic to the euclidean space of dimensions n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDA (Multi Dimensional Scaling)\n",
    "\n",
    "```\n",
    " PCA = MDS with euclidean distance \n",
    " Isomap = MDS with geodesic distance\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds_digits = MDS(n_components=3)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_digits = mds_digits.fit_transform(X_digits) \n",
    "\n",
    "mds_iris = MDS(n_components=3)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_iris = mds_iris.fit_transform(X_iris) \n",
    "\n",
    "mds_s_curve = MDS(n_components=2)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_s_curve = mds_s_curve.fit_transform(X_s_curve) \n",
    "\n",
    "mds_swiss_role = MDS(n_components=2)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_swiss_role = mds_swiss_role.fit_transform(X_s_curve) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In MDS, we are trying to model the distances. Hence, the most obvious choice \n",
    "# for a goodness-of-fit statistic is one based on the differences between the actual distances and their\n",
    "# predicted values. Such a measure is called stress.\n",
    "#**  MDS fits with stress values near zero are the best **\n",
    "\n",
    "print(\"Stress Value (digits dataset): %s\" % np.around(mds_digits.stress_, 2))\n",
    "print(\"Stress Value (iris dataset): %s\" % np.around(mds_iris.stress_, 2))\n",
    "print(\"Stress Value (s-curve dataset): %s\" % np.around(mds_s_curve.stress_, 2))\n",
    "print(\"Stress Value (swiss-roll dataset): %s\" % np.around(mds_s_curve.stress_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isomap\n",
    "\n",
    "The main idea of Isomap is to perform MDS not in the input space but in the geodesic space of the non-linear data manifold.\n",
    "\n",
    "```\n",
    " Important Parameters:\n",
    "     1. n_neighbors => number of neighbors to consider for each point.\n",
    "     2. n_components => number of coordinates for the manifold.\n",
    "     3. neighbors_algorithm [‘auto’|’brute’|’kd_tree’|’ball_tree’] => Algorithm to use for nearest neighbors search.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap_digits = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_digits = isomap_digits.fit_transform(X_digits) \n",
    "\n",
    "isomap_iris = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='kd_tree')\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_iris = isomap_iris.fit_transform(X_iris) \n",
    "\n",
    "isomap_s_curve = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_s_curve = isomap_s_curve.fit_transform(X_s_curve) \n",
    "\n",
    "isomap_swiss_role = Isomap(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_swiss_role = isomap_swiss_role.fit_transform(X_s_curve) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cost function of an isomap embedding is\n",
    "# E = frobenius_norm[K(D) - K(D_fit)] / n_samples\n",
    "# Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit,\n",
    "# and K is the isomap kernel\n",
    "print(\"Error (digits dataset): %s\" % np.around(isomap_digits.reconstruction_error(), 2))\n",
    "print(\"Error (iris dataset): %s\" % np.around(isomap_iris.reconstruction_error(), 2))\n",
    "print(\"Error (s-curve dataset): %s\" % np.around(isomap_s_curve.reconstruction_error(), 2))\n",
    "print(\"Error (swiss-roll dataset): %s\" % np.around(isomap_swiss_role.reconstruction_error(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locally Linear Embedding(LLE)\n",
    "\n",
    "1. Compute the neighbours of each data point.\n",
    "2. Compute the weights $W_{ij}$ that best reconstruct each data point $X_{i}$ from its neighbours, minimizing the below cost function.\n",
    "    \n",
    "     E(w) =  $\\sum_{i=1}^t || x_i - \\sum_{j=1}^k w_{ij}x_j||^2$\n",
    "\n",
    "3. Compute the vectors $Y_i$ best reconstructed by the weights $W_{ij}$, minimizing below equation\n",
    "\n",
    "     f(y) =  $\\sum_{i=1}^t || y_i - \\sum_{j=1}^t w_{ij}y_j||^2$\n",
    "\n",
    "```\n",
    " Important Parameters:\n",
    "     1. n_neighbors => number of neighbors to consider for each point.\n",
    "     2. n_components => number of coordinates for the manifold.\n",
    "     3. neighbors_algorithm [‘auto’|’brute’|’kd_tree’|’ball_tree’] => Algorithm to use for nearest neighbors search.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle_digits = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_digits = lle_digits.fit_transform(X_digits) \n",
    "\n",
    "lle_iris = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='kd_tree')\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_iris = lle_iris.fit_transform(X_iris) \n",
    "\n",
    "lle_s_curve = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_s_curve = lle_s_curve.fit_transform(X_s_curve) \n",
    "\n",
    "lle_swiss_role = LocallyLinearEmbedding(n_neighbors=10, n_components = 3, neighbors_algorithm='ball_tree')\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_swiss_role = lle_swiss_role.fit_transform(X_s_curve) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error (digits dataset): %s\" % np.around(lle_digits.reconstruction_error_, 7))\n",
    "print(\"Error (iris dataset): %s\" % np.around(lle_iris.reconstruction_error_, 7))\n",
    "print(\"Error (s-curve dataset): %s\" % np.around(lle_s_curve.reconstruction_error_, 7))\n",
    "print(\"Error (swiss-role dataset): %s\" % np.around(lle_swiss_role.reconstruction_error_, 7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian Eigen Map\n",
    "1. Transform the raw input data into graph representation using affinity (adjacency) matrix representation.\n",
    "\n",
    "2. Calculate the Laplacian Matrix L = D - W where D -> Diagonal Matrix and W -> Weight Matrix.\n",
    "\n",
    "3. Eigenvalue decomposition is done on graph Laplacian\n",
    "\n",
    "```\n",
    " Important Parameters:\n",
    "\n",
    "        n_components => The dimension of the projected subspace.\n",
    "\n",
    "        affinity => \n",
    "        How to construct the affinity matrix.\n",
    "        ‘nearest_neighbors’ : construct the affinity matrix by computing a graph of nearest neighbors.\n",
    "\n",
    "        ‘rbf’ : construct the affinity matrix by computing a radial basis function (RBF) kernel.\n",
    "\n",
    "        ‘precomputed’ : interpret X as a precomputed affinity matrix.\n",
    "\n",
    "        ‘precomputed_nearest_neighbors’ : interpret X as a sparse graph of precomputed nearest neighbors, and constructs the affinity matrix by selecting the n_neighbors nearest neighbors.\n",
    "\n",
    "        callable : use passed in function as affinity the function takes in data matrix (n_samples, n_features) and return affinity matrix (n_samples, n_samples).\n",
    "\n",
    "        gamma => Kernel coefficient for rbf kernel.\n",
    "\n",
    "        n_neighbors => Number of nearest neighbors for nearest_neighbors graph building.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_digits = SpectralEmbedding(n_components = 3,affinity='rbf', n_neighbors=10)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_digits = spectral_digits.fit_transform(X_digits) \n",
    "\n",
    "spectral_iris = SpectralEmbedding(n_components = 3,affinity='rbf', n_neighbors=10)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_iris = spectral_iris.fit_transform(X_iris) \n",
    "\n",
    "spectral_s_curve = SpectralEmbedding(n_components = 3,affinity='rbf', n_neighbors=10)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_s_curve = spectral_s_curve.fit_transform(X_s_curve) \n",
    "\n",
    "spectral_swiss_role = SpectralEmbedding(n_components = 3,affinity='rbf', n_neighbors=10)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_swiss_role = spectral_swiss_role.fit_transform(X_s_curve) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_interactive(X_digits, y_digits, X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_interactive(X_s_curve, color, X_swiss_role, y_swiss_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSNE\n",
    "\n",
    "t-distributed Stochastic Neighbor Embedding.\n",
    "\n",
    "```\n",
    "    Important Parameters:\n",
    "        n_components => Dimension of the embedded space.\n",
    "        perplexity => The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significanlty different results.\n",
    "        learning_rate => The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_digits = TSNE(n_components = 2,perplexity=10)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_digits = tsne_digits.fit_transform(X_digits) \n",
    "\n",
    "tsne_iris = TSNE(n_components = 2,perplexity=10)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_iris = tsne_digits.fit_transform(X_iris) \n",
    "\n",
    "tsne_s_curve = TSNE(n_components = 2,perplexity=10)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_s_curve = tsne_digits.fit_transform(X_s_curve) \n",
    "\n",
    "tsne_swiss_role = TSNE(n_components = 2,perplexity=10)\n",
    "# Fit the data from X, and returns the embedded coordinates\n",
    "X_swiss_role = tsne_digits.fit_transform(X_s_curve) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_interactive(X_digits, y_digits, X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
