{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.preprocessing import text\n",
    "\n",
    "import pandas as pd\n",
    "import itertools  \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of WordEmbedding Algorithms:\n",
    "        \n",
    "        1. CBOW\n",
    "        2. Skip-Gram with Negative Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert text to UTF-8 [Encode]\n",
    "2. Remove Special Characters, Numbers, Punctuations, Stop Words, html urls etc. [This can be done using regex or NLTK python library]\n",
    "3. Lemmatization & Stemming\n",
    "        It is the process of converting a word to its base form, e.g., “caring” to “care”\n",
    "4. Convert to lower case\n",
    "5. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/ukannika/work/personal/machine-learning/datasets/tweets.csv'\n",
    "raw_df = pd.read_csv(file_path, encoding = \"ISO-8859-1\")\n",
    "\n",
    "tweets = raw_df['Tweet Content']\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \", tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer\n",
    "Text tokenization utility class.\n",
    "\n",
    "num_words: *the maximum number of words to keep, based\n",
    "            on word frequency. Only the most common `num_words-1` words will\n",
    "            be kept.*\n",
    "            \n",
    "filters: *a string where each element is a character that will be\n",
    "            filtered from the texts. The default is all punctuation, plus\n",
    "            tabs and line breaks, minus the `'` character.*\n",
    "            \n",
    "lower: *boolean. Whether to convert the texts to lowercase.*\n",
    "\n",
    "split: *str. Separator for word splitting.*\n",
    "\n",
    "char_level: *if True, every character will be treated as a token.*\n",
    "\n",
    "oov_token: *if given, it will be added to word_index and used to\n",
    "            replace out-of-vocabulary words during text_to_sequence calls*\n",
    "        \n",
    "**Methods :**\n",
    "1. fit_on_texts => Updates internal vocabulary based on a list of texts. Should be used before texts_to_sequences\n",
    "\n",
    "2. texts_to_sequences => Transforms each text in texts to a sequence of integers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1 => Create a vocabulary(Each word assigned with unique number). This vocabulary get's created when \n",
    "tokenizer.fit_on_texts method called.\n",
    "\n",
    "Step2 => Convert text to sequences. Each sentence vector contains numbers associated with that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oov_token = '<UNK>'\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "                        num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n>^https?:\\/\\/.*[\\r\\n]*[^\\x00-\\x7F]+', lower=True,\n",
    "                        split=' ', char_level=False, oov_token=oov_token, document_count=0)\n",
    "\n",
    "# Tokenize our training data \n",
    "tokenizer.fit_on_texts(tweets) \n",
    "\n",
    "# Generate Sequeneces.\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "print(tweets[0])\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mappings between word to index and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in list(word_index)[10:15]:\n",
    "    print (\"{}:{}\".format(x,  word_index[x]))\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "for x in list(index_word)[10:15]:\n",
    "    print (\"{}:{}\".format(x,  index_word[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding\n",
    "Padding is a special form of masking where the masked steps are at the start or at the beginning of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index['PAD'] = 0 \n",
    "index_word[0] = 'PAD'\n",
    "\n",
    "vocab_size = len(word_index)\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word_index.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_pairs(sequence, window_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(0, len(sequence) - window_size, 1):\n",
    "        x.append(sequence[i:i+window_size])\n",
    "        y.append(sequence[i + window_size])\n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify generate_cbow_pairs method. \n",
    "for sequence in sequences[0:1]:\n",
    "    print(sequence, \"\\n\")\n",
    "    x, y = generate_cbow_pairs(sequence, 4)\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters for CBOW model.\n",
    "window_size = 4\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    x, y = generate_cbow_pairs(sequence, window_size)\n",
    "    train_features.extend(x)\n",
    "    train_labels.extend(y)\n",
    "\n",
    "# Convert the list to numpy array.\n",
    "x_train = np.asarray(train_features)\n",
    "y_train = np.asarray(train_labels)\n",
    "\n",
    "print(\"Features Shape: \", x_train.shape)\n",
    "print(\"Labels Shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_labels = np_utils.to_categorical(y_train, vocab_size)\n",
    "print(Y_labels.shape)\n",
    "Y_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model & Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding class Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "This layer can only be used as the first layer in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First understand Embedding layer input shape and output shape. \n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embed_size, input_length=window_size))\n",
    "\n",
    "# Assume input will have 32 rows and each row will have 10 dimensional vector. \n",
    "# Below line of code generates 32 rows with 1o dimensional vector and values between 0 and 1000.  \n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "\n",
    "# Print output shape\n",
    "# (32, 10, 300) => (batchSize, input dimensional space(window_size), embedding size)\n",
    "print(output_array.shape)\n",
    "\n",
    "# Now convert the 3D to 2D, as we need to feed this into Dense layer for training. \n",
    "# Example for converting. \n",
    "input_shape = (1, 2, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "        keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size),\n",
    "        keras.layers.GlobalAveragePooling1D(),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(vocab_size, activation='softmax')])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, Y_labels, batch_size=2000, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape) \n",
    "print(weights[0])\n",
    "\n",
    "# Weights Shape => (vocab_size, embedding_size) = (1125, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write weights to disk to view it on Embedding Projector. In practice,\n",
    "# we can use Embedding Projector or project this data to 30 Dimensions and then Use t-Sne to visualize it.\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(word_index):\n",
    "    vec = weights[num - 1] # skip 0, it's padding.\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vecs_pretrained.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta_pretrained.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join('/Users/ukannika/Downloads/glove.6B/', 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs_vec = np.fromstring(coefs, 'f', sep=' ')\n",
    "        out_m.write(word + \"\\n\")\n",
    "        out_v.write('\\t'.join([str(x) for x in coefs_vec]) + \"\\n\")   \n",
    "\n",
    "# View these in Embedding Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skip Gram Models With Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function transforms a sequence of word indexes (list of integers) into tuples of words of the form:\n",
    "    \n",
    "    (word, word in the same window), with label 1 (positive samples).\n",
    "    (word, random word from the vocabulary), with label 0 (negative samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = tf.keras.preprocessing.sequence.skipgrams(\n",
    "                sequences[0], vocab_size, window_size=1, negative_samples=0.6, shuffle=True,\n",
    "                categorical=False, sampling_table=None, seed=None)\n",
    "\n",
    "print(x, \"\\n\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train data\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    x_skipgm, y_skipgm = tf.keras.preprocessing.sequence.skipgrams(\n",
    "                sequence, vocab_size, window_size=1, negative_samples=0.6, shuffle=True,\n",
    "                categorical=False, sampling_table=None, seed=None)\n",
    "    x.extend(x_skipgm)\n",
    "    y.extend(y_skipgm)\n",
    "    \n",
    "x_train = np.asarray(x)\n",
    "y_train = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model = keras.Sequential([\n",
    "          keras.layers.Embedding(vocab_size, embed_size),\n",
    "          keras.layers.GlobalAveragePooling1D(),\n",
    "          keras.layers.Dense(16, activation='relu'),\n",
    "          keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x, y, batch_size=2000, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) \n",
    "\n",
    "# Weights Shape => (vocab_size, embedding_size) = (1125, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
