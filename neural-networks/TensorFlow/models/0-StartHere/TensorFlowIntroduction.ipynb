{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend \n",
    "from keras import activations\n",
    "from keras import Sequential\n",
    "import pydot\n",
    "import kerastuner as kt\n",
    "import IPython\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector, Matrix, Tensors\n",
    "\n",
    "A tf.Tensor has the following properties:\n",
    "\n",
    "    a single data type (float32, int32, or string, for example)\n",
    "    a shape\n",
    "    \n",
    "    \n",
    "A number of specialized tensors are available: see \n",
    "    \n",
    "    tf.Variable\n",
    "    tf.constant\n",
    "    tf.placeholder\n",
    "    tf.sparse.SparseTensor\n",
    "    tf.RaggedTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.Variable\n",
    "\n",
    "    A TensorFlow variable is the recommended way to represent shared, persistent state your program manipulates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_str = \"VariableType={variable_type} ; Rank={rank} ; Shape={shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar: Rank0\n",
    "value = tf.Variable(451, tf.int16)\n",
    "print(print_str.format(variable_type='Scalar', rank=tf.rank(value), shape=value.shape))\n",
    "\n",
    "\n",
    "# Vector: Rank1\n",
    "value = tf.Variable([1, 2], tf.int16)\n",
    "print(print_str.format(variable_type='Vector', rank=tf.rank(value), shape=value.shape))\n",
    "\n",
    "\n",
    "# Matrix: Rank2\n",
    "value = tf.Variable([[1, 2, 3], [3, 4, 5]], tf.int16)\n",
    "print(print_str.format(variable_type='Matrix', rank=tf.rank(value), shape=value.shape))\n",
    "\n",
    "\n",
    "#Tensor: Rank3\n",
    "value = tf.Variable([[[1,2,3], [4,5,6], [7,8,9]], [[1,2,3], [4,5,6], [7,8,9]]], tf.int16)\n",
    "print(print_str.format(variable_type='Tensor', rank=tf.rank(value), shape=value.shape))\n",
    "\n",
    "\n",
    "value = tf.Variable([[[1,2], [4,5], [7,8]], [[1,2], [4,5], [7,8]]], tf.int16)\n",
    "print(print_str.format(variable_type='Tensor', rank=tf.rank(value), shape=value.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.constant\n",
    "\n",
    "    Creates a constant tensor from a tensor-like object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.placeholder [Useful in v1]\n",
    "\n",
    "A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlowterminology, we then feed data into the graph through these placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.sparse.SparseTensor\n",
    "\n",
    "    TensorFlow represents a sparse tensor as three separate dense tensors: \n",
    "    indices\n",
    "    values\n",
    "    dense_shape\n",
    " \n",
    "In Python, the three tensors are collected into a SparseTensor class for ease of use. If you have separate indices, values, and dense_shape tensors, wrap them in a SparseTensor object before passing to the ops below.\n",
    "    \n",
    "\n",
    "Concretely, the sparse tensor SparseTensor(indices, values, dense_shape) comprises the following components, where N and ndims are the number of values and number of dimensions in the SparseTensor, respectively:\n",
    "\n",
    "\n",
    "**indices:** A 2-D int64 tensor of shape [N, ndims], which specifies the indices of the elements in the sparse tensor that contain nonzero values (elements are zero-indexed).\n",
    "\n",
    "**values:** A 1-D tensor of any type and shape [N], which supplies the values for each element in indices. \n",
    "\n",
    "**dense_shape:** A 1-D int64 tensor of shape [ndims], which specifies the dense_shape of the sparse tensor. Takes a list indicating the number of elements in each dimension. \n",
    "\n",
    "\n",
    "Example:  Represent the following using SparseTensor.\n",
    "\n",
    "    [[1, 0, 0, 0]\n",
    "     [0, 0, 2, 0]\n",
    "     [0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseTensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[11, 12], dense_shape=[3, 4])\n",
    "print(sparseTensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.RaggedTensor\n",
    "\n",
    "A RaggedTensor is a tensor with one or more ragged dimensions, which are dimensions whose slices may have different lengths.\n",
    "\n",
    "Dimensions whose slices all have the same length are called uniform dimensions.\n",
    "\n",
    "The total number of dimensions in a RaggedTensor is called its rank, and the number of ragged dimensions in a RaggedTensor is called its ragged-rank. A RaggedTensor's ragged-rank is fixed at graph creation time: it can't depend on the runtime values of Tensors, and can't vary dynamically for different session runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [3, 1, 4, 1, 5, 9, 2, 6]\n",
    "rt1 = tf.RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])\n",
    "print(rt1)\n",
    "\n",
    "rt2 = tf.RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])\n",
    "print(rt2)\n",
    "\n",
    "rt3 = tf.RaggedTensor.from_value_rowids(values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)\n",
    "print(rt3)\n",
    "\n",
    "# which specifies the start offset of each row.\n",
    "rt4 = tf.RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])\n",
    "print(rt4)\n",
    "\n",
    "# which specifies the stop offset of each row.\n",
    "rt5 = tf.RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])\n",
    "print(rt5)\n",
    "\n",
    "print(\"Ragged Rank: \",rt5.ragged_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Dataset objects\n",
    "\n",
    "    The tf.data.Dataset API supports writing descriptive and efficient input pipelines. Dataset usage follows a common pattern:\n",
    "    \n",
    "    1. Create a source dataset from your input data.\n",
    "    2. Apply dataset transformations to preprocess the data.\n",
    "    3. Iterate over the dataset and process the elements.\n",
    "    \n",
    "    \n",
    "There are two distinct ways to create a dataset:\n",
    "\n",
    "    A data source constructs a Dataset from data stored in memory or in one or more files.\n",
    "    A data transformation constructs a dataset from one or more tf.data.Dataset objects.\n",
    "    \n",
    "To construct a Dataset from data in memory:\n",
    "\n",
    "    Use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices()\n",
    "\n",
    "If input data is stored in a file in the recommended TFRecord format:\n",
    "\n",
    "    Use tf.data.TFRecordDataset(), CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))\n",
    "print(dataset.element_spec)\n",
    "\n",
    "for z in dataset:\n",
    "    print(z.numpy())\n",
    "    \n",
    "# Dataset containing a sparse tensor.\n",
    "sparse_dataset = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Numpy Arrays to Dataset:\n",
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "images, labels = train\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Architecture\n",
    "\n",
    "*Model* => Handles top level functionality. (compile, fit, evaluate, predict, save)\n",
    "\n",
    "*Layer* => Consists of business logic. \n",
    "\n",
    "Losses, Metrics\n",
    "\n",
    "Callbacks\n",
    "\n",
    "Optimizers\n",
    "\n",
    "Regularizers, Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers\n",
    "\n",
    "    Define a simple Dense Layer with 2 parameters => [W, b]\n",
    "    \n",
    "    Also we set the default value for units/neuron per layer to 32. \n",
    " \n",
    "*build() =>  This method can be used to create weights that depend on the shape(s) of the input(s)*\n",
    "\n",
    "*call()  =>  Performs the logic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(SimpleDense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    # Create the state of the layer (weights)\n",
    "    def build(self, input_shape):  \n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'),\n",
    "                             trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
    "                             trainable=True)\n",
    "\n",
    "    # Defines the computation from inputs to outputs\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates the layer.\n",
    "# Number of units/neurons in a layer set to 6\n",
    "linear_layer = SimpleDense(6)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "# Creates a tensor with all elements set to one (1). \n",
    "y = linear_layer(tf.ones((2, 4)))\n",
    "\n",
    "assert len(linear_layer.weights) == 2\n",
    "\n",
    "# These weights are trainable, so they're listed in `trainable_weights`:\n",
    "assert len(linear_layer.trainable_weights) == 2\n",
    "\n",
    "# Get Weights. Returns the list\n",
    "linear_layer.w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep track of variables\n",
    "Layers are recursively composable. If you assign a Layer instance as attribute of another Layer, the outer layer will start tracking the weights of the inner layer.\n",
    "We recommend creating such sublayers in the __init__() method (since the sublayers will typically have a build method, they will be built when the outer layer gets built).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have 2 different layers with different parameters. How do we keep track of variables. \n",
    "class MyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.my_var = tf.Variable(1.0)\n",
    "        self.my_var_list = [tf.Variable(x) for x in range(10)]\n",
    "\n",
    "class MyOtherLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyOtherLayer, self).__init__()\n",
    "        self.sublayer = MyLayer()\n",
    "        self.my_other_var = tf.Variable(10.0)\n",
    "\n",
    "m = MyOtherLayer()\n",
    "print(len(m.variables))  # 12 (11 from MyLayer, plus 1 from MyOtherLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_loss() &  add_metric() method\n",
    "\n",
    "Similarly to add_loss(), layers also have an add_metric() method for tracking the moving average of a quantity during training. method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that creates an activity regularization loss\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "    \n",
    "# Outer Layer\n",
    "class OuterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0  # No losses yet since the layer has never been called\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # We created one loss value\n",
    "\n",
    "# `layer.losses` gets reset at the start of each __call__\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # This is the loss created during the call above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient computation & Weights update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "dy_dx = g.gradient(y, x) # Will compute to 6.0\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    with tf.GradientTape() as gg:\n",
    "        gg.watch(x)\n",
    "        y = x * x\n",
    "        dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n",
    "d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n",
    "print(d2y_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient for below equations using chain rule. \n",
    "\n",
    "At x = 3\n",
    "\n",
    "y = $x^{2}$\n",
    "\n",
    "z = y + 3\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}$ = $\\frac{\\partial z}{\\partial y}$.$\\frac{\\partial y}{\\partial x}$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}$ = 1 . 2x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    y = x * x\n",
    "    z = y + 3\n",
    "    dz_dx = g.gradient(z, [y, x])\n",
    "    print(dz_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's define neural network architecture**\n",
    "\n",
    "    1. Input layer Shape (2, 2)\n",
    "    2. One Hidden Layer(Dense) with summation and RELU activation function\n",
    "    3. Output Layer(Dense) with Summation and Sigmoid acitivation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "        # Params => W, b\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.b = self.add_weight(shape=(self.units,), initializer='random_normal', trainable=True)\n",
    "        else:\n",
    "            self.b = None\n",
    "                \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output = backend.dot(inputs, self.w)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            output = backend.bias_add(output, self.b, data_format='channels_last')\n",
    "       \n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        \n",
    "        return output   \n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        assert input_shape[-1]\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "                regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(Dense, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "features = tf.Variable([[1, 1], [2, 1], [2, 2], [1,2]], dtype='float32')\n",
    "labels = tf.Variable([0, 1, 0, 1])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "hidden_layer = DenseLayer(6, activation='relu')\n",
    "output_layer = DenseLayer(1, activation='sigmoid')\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for i in range(1):\n",
    "    for x, y in zip(features.numpy(), labels.numpy()):\n",
    "        with tf.GradientTape() as hidden_tape:\n",
    "            hidden_ouput = hidden_layer(x.reshape(1, 2))\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                output = output_layer(hidden_ouput)\n",
    "                loss_value = loss_function(y, output)\n",
    "\n",
    "            dy_dx = tape.gradient(loss_value, output_layer.trainable_weights)  \n",
    "            optimizer.apply_gradients(zip(dy_dx, output_layer.trainable_weights))\n",
    "        dy1_dx = hidden_tape.gradient(dy_dx, hidden_layer.trainable_weights)   \n",
    "        optimizer.apply_gradients(zip(dy1_dx, hidden_layer.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "     Model => Handles top level functionality. (compile, fit, evaluate, predict, save)\n",
    "     Layer => Consists of business logic\n",
    "     \n",
    "In general, you will use the Layer class to define inner computation blocks, and will use the Model class to define the outer model -- the object you will train.\n",
    "\n",
    "The Model class has the same API as Layer, with the following differences:\n",
    "\n",
    "    It exposes built-in training, evaluation, and prediction loops (model.fit(), model.evaluate(), model.predict())\n",
    "    It exposes the list of its inner layers, via the model.layers property.\n",
    "    It exposes saving and serialization APIs (save(), save_weights()...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer1 = DenseLayer(6, activation='relu')\n",
    "        self.layer2 = DenseLayer(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        return self.layer2(x)\n",
    "\n",
    "myModel = MyModel()\n",
    "\n",
    "# By calling compile, Keras creates the DAG. \n",
    "myModel.compile(optimizer=tf.keras.optimizers.Adam(), loss = tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
    "\n",
    "# Fits the model to train on given input.\n",
    "myModel.fit(features.numpy(), labels.numpy(), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(myModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional Vs Sequential\n",
    "\n",
    "**Sequential:**\n",
    "\n",
    "    A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "    \n",
    "**Functional:**\n",
    "\n",
    "    The Keras functional API is a way to create models that is more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, models with shared layers, and models with multiple inputs or outputs.\n",
    "    \n",
    "    The main idea that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\n",
    "    \n",
    "    Will look into this on Machine Translation, Object Recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### callbacks \n",
    "\n",
    "    A callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference.\n",
    "\n",
    "    Examples include tf.keras.callbacks.TensorBoard to visualize training progress and results with TensorBoard, or tf.keras.callbacks.ModelCheckpoint to periodically save your model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.fit(\n",
    "    features.numpy(),\n",
    "    labels.numpy(),\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    verbose=0,\n",
    "    callbacks=[CustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "\n",
    "**Hyperparameters** are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\n",
    "\n",
    "    Model hyperparameters: which influence model selection such as the number and width of hidden layers\n",
    "    Algorithm hyperparameters: which influence the speed and quality of the learning algorithm such as the \n",
    "                               learning rate for Stochastic Gradient Descent (SGD) and the number of nearest\n",
    "                               neighbors for a k Nearest Neighbors (KNN) classifier\n",
    "   \n",
    "   \n",
    "The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called hyperparameter tuning or hypertuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset.\n",
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Choose an optimal value between 32-512\n",
    "    hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "    model.add(keras.layers.Dense(units = hp_units, activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(10))\n",
    "\n",
    "    # Tune the learning rate for the optimizer \n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
    "\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
    "                loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Instantiate the tuner and perform hypertuning\n",
    "\n",
    "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - \n",
    "\n",
    "        1. RandomSearch\n",
    "        2. Hyperband\n",
    "        3. BayesianOptimization\n",
    "        4. Sklearn. \n",
    "    \n",
    "In this tutorial, you use the Hyperband tuner.\n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_accuracy', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = '/Users/ukannika/work/artifact-store/htuner',\n",
    "                     project_name = 'intro_to_kt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the hyperparameter search, define a callback to clear the training outputs at the end of every training step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test), callbacks = [ClearTrainingOutput()])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Load ML Models\n",
    "\n",
    "Model progress can be saved during and after training. This means a model can resume where it left off and avoid long training times. Saving also means you can share your model and others can recreate your work. When publishing research models and techniques, most machine learning practitioners share:\n",
    "\n",
    "    code to create the model, and\n",
    "    the trained weights, or parameters, for the model\n",
    "    \n",
    "    \n",
    "Sharing this data helps others understand how the model works and try it themselves with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can save and load a model in the SavedModel format using the following APIs:**\n",
    "\n",
    "**Low-level** tf.saved_model API. This document describes how to use this API in detail.\n",
    "\n",
    "    Save: tf.saved_model.save(model, path_to_dir)\n",
    "    Load: model = tf.saved_model.load(path_to_dir)\n",
    "    \n",
    "    \n",
    "**High-level** tf.keras.Model API. Refer to the keras save and serialize guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation:\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple sequential model\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10)\n",
    "      ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = create_model()\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save checkpoints during training\n",
    "\n",
    "You can use a trained model without having to retrain it, or pick-up training where you left off in case the training process was interrupted. The tf.keras.callbacks.ModelCheckpoint callback allows you to continually save the model both during and at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where you want to store the checkpoints\n",
    "checkpoint_path = \"/tmp/training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights \n",
    "# Here the callback saves the model after each epoch. Look at docs for more variables. \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "model.fit(train_images, \n",
    "          train_labels,  \n",
    "          epochs=10,\n",
    "          validation_data=(test_images, test_labels),\n",
    "          callbacks=[cp_callback])  # Pass callback to training\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic model instance\n",
    "model1 = create_model()\n",
    "\n",
    "# Evaluate the model\n",
    "loss, acc = model1.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually save weights after training\n",
    "\n",
    "Manually saving weights with the Model.save_weights method. By default, tf.keras—and save_weights in particular—uses the TensorFlow checkpoint format with a .ckpt extension (saving in HDF5 with a .h5 extension is covered in the Save and serialize models guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.save_weights('/tmp/checkpoints/my_checkpoint')\n",
    "\n",
    "# Create a new model instance\n",
    "model = create_model()\n",
    "\n",
    "# Restore the weights\n",
    "model.load_weights('/tmp/checkpoints/my_checkpoint')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the entire model\n",
    "\n",
    "Call model.save to save a model's architecture, weights, and training configuration in a single file/folder. This allows you to export a model so it can be used without access to the original Python code*. Since the optimizer-state is recovered, you can resume training from exactly where you left off.\n",
    "\n",
    "An entire model can be saved in two different file formats (SavedModel and HDF5). The TensorFlow SavedModel format is the default file format in TF2.x. However, models can be saved in HDF5 format. More details on saving entire models in the two file formats is described below.\n",
    "\n",
    "Saving a fully-functional model is very useful—you can load them in TensorFlow.js (Saved Model, HDF5) and then train and run them in web browsers, or convert them to run on mobile devices using TensorFlow Lite (Saved Model, HDF5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SavedModel format\n",
    "\n",
    "The SavedModel format is another way to serialize models. Models saved in this format can be restored using tf.keras.models.load_model and are compatible with TensorFlow Serving. The SavedModel guide goes into detail about how to serve/inspect the SavedModel. The section below illustrates the steps to save and restore the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a new model instance.\n",
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "model.save('/tmp/saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload a fresh Keras model from the saved model:\n",
    "new_model = tf.keras.models.load_model('/tmp/saved_model/my_model')\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the restored model\n",
    "loss, acc = new_model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n",
    "\n",
    "print(new_model.predict(test_images).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types Of Applications:\n",
    "    1. Feed Forward Neural Networks [Classfication/Imbalanced Classification, Regression]\n",
    "    2. Word2Vec Semantic Analysis \n",
    "    3. Image Classifications [CNN]\n",
    "    4. RNN(LSTM/GRU) Sentiment Analysis, Machine Translation, Image Captioning\n",
    "    5. RNN (LSTM/GRU) Forecast Analysis\n",
    "    6. Transformer Machine Translation\n",
    "    7. Transfer Learning (BERT)\n",
    "    8. Generative Models\n",
    "    9. AutoEncoders/Variational AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
