{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend \n",
    "from keras import activations\n",
    "from keras import Sequential\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector, Matrix, Tensors\n",
    "\n",
    "A tf.Tensor has the following properties:\n",
    "\n",
    "    a single data type (float32, int32, or string, for example)\n",
    "    a shape\n",
    "    \n",
    "    \n",
    "A number of specialized tensors are available: see \n",
    "    \n",
    "    tf.Variable\n",
    "    tf.constant\n",
    "    tf.placeholder\n",
    "    tf.sparse.SparseTensor\n",
    "    tf.RaggedTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.Variable\n",
    "\n",
    "    A TensorFlow variable is the recommended way to represent shared, persistent state your program manipulates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_str = \"VariableType={variable_type} ; Rank={rank} ; Shape={shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar: Rank0\n",
    "value = tf.Variable(451, tf.int16)\n",
    "print(print_str.format(variable_type='Scalar', rank=tf.rank(value), shape=value.shape))\n",
    "\n",
    "\n",
    "# Vector: Rank1\n",
    "value = tf.Variable([1, 2], tf.int16)\n",
    "print(print_str.format(variable_type='Vector', rank=tf.rank(value), shape=value.shape))\n",
    "\n",
    "\n",
    "# Matrix: Rank2\n",
    "value = tf.Variable([[1, 2, 3], [3, 4, 5]], tf.int16)\n",
    "print(print_str.format(variable_type='Matrix', rank=tf.rank(value), shape=value.shape))\n",
    "\n",
    "\n",
    "#Tensor: Rank3\n",
    "value = tf.Variable([[[1,2,3], [4,5,6], [7,8,9]], [[1,2,3], [4,5,6], [7,8,9]]], tf.int16)\n",
    "print(print_str.format(variable_type='Tensor', rank=tf.rank(value), shape=value.shape))\n",
    "\n",
    "\n",
    "value = tf.Variable([[[1,2], [4,5], [7,8]], [[1,2], [4,5], [7,8]]], tf.int16)\n",
    "print(print_str.format(variable_type='Tensor', rank=tf.rank(value), shape=value.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.constant\n",
    "\n",
    "    Creates a constant tensor from a tensor-like object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.placeholder [Useful in v1]\n",
    "\n",
    "A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlowterminology, we then feed data into the graph through these placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.sparse.SparseTensor\n",
    "\n",
    "    TensorFlow represents a sparse tensor as three separate dense tensors: \n",
    "    indices\n",
    "    values\n",
    "    dense_shape\n",
    " \n",
    "In Python, the three tensors are collected into a SparseTensor class for ease of use. If you have separate indices, values, and dense_shape tensors, wrap them in a SparseTensor object before passing to the ops below.\n",
    "    \n",
    "\n",
    "Concretely, the sparse tensor SparseTensor(indices, values, dense_shape) comprises the following components, where N and ndims are the number of values and number of dimensions in the SparseTensor, respectively:\n",
    "\n",
    "\n",
    "**indices:** A 2-D int64 tensor of shape [N, ndims], which specifies the indices of the elements in the sparse tensor that contain nonzero values (elements are zero-indexed).\n",
    "\n",
    "**values:** A 1-D tensor of any type and shape [N], which supplies the values for each element in indices. \n",
    "\n",
    "**dense_shape:** A 1-D int64 tensor of shape [ndims], which specifies the dense_shape of the sparse tensor. Takes a list indicating the number of elements in each dimension. \n",
    "\n",
    "\n",
    "Example:  Represent the following using SparseTensor.\n",
    "\n",
    "    [[1, 0, 0, 0]\n",
    "     [0, 0, 2, 0]\n",
    "     [0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseTensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[11, 12], dense_shape=[3, 4])\n",
    "print(sparseTensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.RaggedTensor\n",
    "\n",
    "A RaggedTensor is a tensor with one or more ragged dimensions, which are dimensions whose slices may have different lengths.\n",
    "\n",
    "Dimensions whose slices all have the same length are called uniform dimensions.\n",
    "\n",
    "The total number of dimensions in a RaggedTensor is called its rank, and the number of ragged dimensions in a RaggedTensor is called its ragged-rank. A RaggedTensor's ragged-rank is fixed at graph creation time: it can't depend on the runtime values of Tensors, and can't vary dynamically for different session runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [3, 1, 4, 1, 5, 9, 2, 6]\n",
    "rt1 = tf.RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])\n",
    "print(rt1)\n",
    "\n",
    "rt2 = tf.RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])\n",
    "print(rt2)\n",
    "\n",
    "rt3 = tf.RaggedTensor.from_value_rowids(values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)\n",
    "print(rt3)\n",
    "\n",
    "# which specifies the start offset of each row.\n",
    "rt4 = tf.RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])\n",
    "print(rt4)\n",
    "\n",
    "# which specifies the stop offset of each row.\n",
    "rt5 = tf.RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])\n",
    "print(rt5)\n",
    "\n",
    "print(\"Ragged Rank: \",rt5.ragged_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Dataset objects\n",
    "\n",
    "    The tf.data.Dataset API supports writing descriptive and efficient input pipelines. Dataset usage follows a common pattern:\n",
    "    \n",
    "    1. Create a source dataset from your input data.\n",
    "    2. Apply dataset transformations to preprocess the data.\n",
    "    3. Iterate over the dataset and process the elements.\n",
    "    \n",
    "    \n",
    "There are two distinct ways to create a dataset:\n",
    "\n",
    "    A data source constructs a Dataset from data stored in memory or in one or more files.\n",
    "    A data transformation constructs a dataset from one or more tf.data.Dataset objects.\n",
    "    \n",
    "To construct a Dataset from data in memory:\n",
    "\n",
    "    Use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices()\n",
    "\n",
    "If input data is stored in a file in the recommended TFRecord format:\n",
    "\n",
    "    Use tf.data.TFRecordDataset(), CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))\n",
    "print(dataset.element_spec)\n",
    "\n",
    "for z in dataset:\n",
    "    print(z.numpy())\n",
    "    \n",
    "# Dataset containing a sparse tensor.\n",
    "sparse_dataset = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Numpy Arrays to Dataset:\n",
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "images, labels = train\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Architecture\n",
    "\n",
    "*Model* => Handles top level functionality. (compile, fit, evaluate, predict, save)\n",
    "\n",
    "*Layer* => Consists of business logic. \n",
    "\n",
    "Losses, Metrics\n",
    "\n",
    "Callbacks\n",
    "\n",
    "Optimizers\n",
    "\n",
    "Regularizers, Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers\n",
    "\n",
    "    Define a simple Dense Layer with 2 parameters => [W, b]\n",
    "    \n",
    "    Also we set the default value for units/neuron per layer to 32. \n",
    " \n",
    "*build() =>  This method can be used to create weights that depend on the shape(s) of the input(s)*\n",
    "\n",
    "*call()  =>  Performs the logic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(SimpleDense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    # Create the state of the layer (weights)\n",
    "    def build(self, input_shape):  \n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'),\n",
    "                             trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
    "                             trainable=True)\n",
    "\n",
    "    # Defines the computation from inputs to outputs\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates the layer.\n",
    "# Number of units/neurons in a layer set to 6\n",
    "linear_layer = SimpleDense(6)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "# Creates a tensor with all elements set to one (1). \n",
    "y = linear_layer(tf.ones((2, 4)))\n",
    "\n",
    "assert len(linear_layer.weights) == 2\n",
    "\n",
    "# These weights are trainable, so they're listed in `trainable_weights`:\n",
    "assert len(linear_layer.trainable_weights) == 2\n",
    "\n",
    "# Get Weights. Returns the list\n",
    "linear_layer.w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep track of variables\n",
    "Layers are recursively composable. If you assign a Layer instance as attribute of another Layer, the outer layer will start tracking the weights of the inner layer.\n",
    "We recommend creating such sublayers in the __init__() method (since the sublayers will typically have a build method, they will be built when the outer layer gets built).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have 2 different layers with different parameters. How do we keep track of variables. \n",
    "class MyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.my_var = tf.Variable(1.0)\n",
    "        self.my_var_list = [tf.Variable(x) for x in range(10)]\n",
    "\n",
    "class MyOtherLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyOtherLayer, self).__init__()\n",
    "        self.sublayer = MyLayer()\n",
    "        self.my_other_var = tf.Variable(10.0)\n",
    "\n",
    "m = MyOtherLayer()\n",
    "print(len(m.variables))  # 12 (11 from MyLayer, plus 1 from MyOtherLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_loss() &  add_metric() method\n",
    "\n",
    "Similarly to add_loss(), layers also have an add_metric() method for tracking the moving average of a quantity during training. method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that creates an activity regularization loss\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "    \n",
    "# Outer Layer\n",
    "class OuterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0  # No losses yet since the layer has never been called\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # We created one loss value\n",
    "\n",
    "# `layer.losses` gets reset at the start of each __call__\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # This is the loss created during the call above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient computation & Weights update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "dy_dx = g.gradient(y, x) # Will compute to 6.0\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    with tf.GradientTape() as gg:\n",
    "        gg.watch(x)\n",
    "        y = x * x\n",
    "        dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n",
    "d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n",
    "print(d2y_dx2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient for below equations using chain rule. \n",
    "\n",
    "At x = 3\n",
    "\n",
    "y = $x^{2}$\n",
    "\n",
    "z = y + 3\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}$ = $\\frac{\\partial z}{\\partial y}$.$\\frac{\\partial y}{\\partial x}$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial x}$ = 1 . 2x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    y = x * x\n",
    "    z = y + 3\n",
    "    dz_dx = g.gradient(z, [y, x])\n",
    "    print(dz_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's define neural network architecture**\n",
    "\n",
    "    1. Input layer Shape (2, 2)\n",
    "    2. One Hidden Layer(Dense) with summation and RELU activation function\n",
    "    3. Output Layer(Dense) with Summation and Sigmoid acitivation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "        # Params => W, b\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.b = self.add_weight(shape=(self.units,), initializer='random_normal', trainable=True)\n",
    "        else:\n",
    "            self.b = None\n",
    "                \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output = backend.dot(inputs, self.w)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            output = backend.bias_add(output, self.b, data_format='channels_last')\n",
    "       \n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        \n",
    "        return output   \n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) >= 2\n",
    "        assert input_shape[-1]\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] = self.units\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "                regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(Dense, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "features = tf.Variable([[1, 1], [2, 1], [2, 2], [1,2]], dtype='float32')\n",
    "labels = tf.Variable([0, 1, 0, 1])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "hidden_layer = DenseLayer(6, activation='relu')\n",
    "output_layer = DenseLayer(1, activation='sigmoid')\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for i in range(1):\n",
    "    for x, y in zip(features.numpy(), labels.numpy()):\n",
    "        with tf.GradientTape() as hidden_tape:\n",
    "            hidden_ouput = hidden_layer(x.reshape(1, 2))\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                output = output_layer(hidden_ouput)\n",
    "                loss_value = loss_function(y, output)\n",
    "\n",
    "            dy_dx = tape.gradient(loss_value, output_layer.trainable_weights)  \n",
    "            optimizer.apply_gradients(zip(dy_dx, output_layer.trainable_weights))\n",
    "        dy1_dx = hidden_tape.gradient(dy_dx, hidden_layer.trainable_weights)   \n",
    "        optimizer.apply_gradients(zip(dy1_dx, hidden_layer.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "     Model => Handles top level functionality. (compile, fit, evaluate, predict, save)\n",
    "     Layer => Consists of business logic\n",
    "     \n",
    "In general, you will use the Layer class to define inner computation blocks, and will use the Model class to define the outer model -- the object you will train.\n",
    "\n",
    "The Model class has the same API as Layer, with the following differences:\n",
    "\n",
    "    It exposes built-in training, evaluation, and prediction loops (model.fit(), model.evaluate(), model.predict())\n",
    "    It exposes the list of its inner layers, via the model.layers property.\n",
    "    It exposes saving and serialization APIs (save(), save_weights()...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer1 = DenseLayer(6, activation='relu')\n",
    "        self.layer2 = DenseLayer(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        return self.layer2(x)\n",
    "\n",
    "myModel = MyModel()\n",
    "\n",
    "# By calling compile, Keras creates the DAG. \n",
    "myModel.compile(optimizer=tf.keras.optimizers.Adam(), loss = tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
    "\n",
    "# Fits the model to train on given input.\n",
    "myModel.fit(features.numpy(), labels.numpy(), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(myModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional Vs Sequential\n",
    "\n",
    "**Sequential:**\n",
    "\n",
    "    A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "    \n",
    "**Functional:**\n",
    "\n",
    "    The Keras functional API is a way to create models that is more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, models with shared layers, and models with multiple inputs or outputs.\n",
    "    \n",
    "    The main idea that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\n",
    "    \n",
    "    Will look into this on Machine Translation, Object Recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### callbacks \n",
    "\n",
    "    A callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference.\n",
    "\n",
    "    Examples include tf.keras.callbacks.TensorBoard to visualize training progress and results with TensorBoard, or tf.keras.callbacks.ModelCheckpoint to periodically save your model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.fit(\n",
    "    features.numpy(),\n",
    "    labels.numpy(),\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    verbose=0,\n",
    "    callbacks=[CustomCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types Of Applications:\n",
    "    1. Feed Forward Neural Networks [Classfication/Imbalanced Classification, Regression]\n",
    "    2. Word2Vec Semantic Analysis \n",
    "    3. Image Classifications [CNN]\n",
    "    4. RNN(LSTM/GRU) Sentiment Analysis, Machine Translation, Image Captioning\n",
    "    5. RNN (LSTM/GRU) Forecast Analysis\n",
    "    6. Transformer Machine Translation\n",
    "    7. Transfer Learning (BERT)\n",
    "    8. Generative Models\n",
    "    9. AutoEncoders/Variational AutoEncoders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
