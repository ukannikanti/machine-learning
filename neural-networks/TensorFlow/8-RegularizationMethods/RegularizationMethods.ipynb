{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a method to prevent overfitting the model by adding a constraint to optimization function. \n",
    "\n",
    "Types:\n",
    "\n",
    "    1. Weight Decay (L1, L2)\n",
    "    2. Ensemble Methods\n",
    "    3. Dropout\n",
    "    4. Early stopping\n",
    "    5. Dataset Augmentation\n",
    "    6. Adding Noise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Decay (L1, L2)\n",
    "\n",
    "The L2 regularization penalty is computed as: loss = l2 * reduce_sum(square(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x has a shape of (2, 3) (two rows and three columns):\n",
    "x = tf.constant([[1, 1, 1], [1, 1, 1]])\n",
    "print(x.numpy())\n",
    "\n",
    "print(tf.math.reduce_sum(x))\n",
    "# sum all the elements\n",
    "# 1 + 1 + 1 + 1 + 1+ 1 = 6\n",
    "\n",
    "# loss = 2. * sum all the elements\n",
    "\n",
    "regularizer = tf.keras.regularizers.L2(2.)\n",
    "regularizer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(\n",
    "    5, input_dim=5,\n",
    "    kernel_initializer='ones',\n",
    "    kernel_regularizer=tf.keras.regularizers.L1(0.01),\n",
    "    activity_regularizer=tf.keras.regularizers.L2(0.01))\n",
    "\n",
    "tensor = tf.ones(shape=(5, 5)) * 2.0\n",
    "out = layer(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "Intuition: Can't rely on any one feature, so have to spread out weights. It means randomly removes connections in NN.\n",
    "\n",
    "The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "\n",
    "rate => Float between 0 and 1. Fraction of the input units to drop.\n",
    "\n",
    "In theoretical perspective L2 is same as dropout. In both approaches, weights shrinks. \n",
    "\n",
    "\n",
    "Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit, training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer.\n",
    "\n",
    "(This is in contrast to setting trainable=False for a Dropout layer. trainable does not affect the layer's behavior, as Dropout does not have any variables/weights that can be frozen during training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Python integer to use as random seed.\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n",
    "data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
    "print(data)\n",
    "\n",
    "outputs = layer(data, training=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping\n",
    "\n",
    "Stop training when a monitored metric has stopped improving.\n",
    "\n",
    "Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. \n",
    "\n",
    "A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates.\n",
    "\n",
    "The quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.compile().\n",
    "\n",
    "\n",
    "    min_delta\t=> Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change \n",
    "                   of less than min_delta, will count as no improvement.\n",
    "    \n",
    "    patience\t=> Number of epochs with no improvement after which training will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=0, verbose=0,\n",
    "    mode='auto', baseline=None, restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, just showing how to use it. DO NOT RUN THIS.\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,  \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
