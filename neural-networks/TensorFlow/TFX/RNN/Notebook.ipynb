{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, will do model training and evaluation in notebook and compare these results with Pipeline Local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = '/Users/ukannika/work/imdb/imdb.csv'\n",
    "batch_size = 32\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "imdb_csv_ds = tf.data.experimental.make_csv_dataset(\n",
    "    DATA_FILE,\n",
    "    batch_size= batch_size, \n",
    "    label_name='label',\n",
    "    num_epochs= num_epochs,\n",
    "    ignore_errors=True)\n",
    "\n",
    "# Split the dataset into train and val.\n",
    "val_size = int(0. * 200)\n",
    "\n",
    "train_data = imdb_csv_ds.skip(val_size)\n",
    "val_data = imdb_csv_ds.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences=[]\n",
    "training_labels=[]\n",
    "\n",
    "val_sentences=[]\n",
    "val_labels=[]\n",
    "\n",
    "for sentence,label in train_data:\n",
    "    print(sentence)\n",
    "    training_sentences.append(str(sentence['text'].numpy()))\n",
    "    training_labels.append(label.numpy())\n",
    "    \n",
    "for sentence,label in val_data:\n",
    "    val_sentences.append(str(sentence['text'].numpy()))\n",
    "    val_labels.append(label.numpy())\n",
    "    \n",
    "training_labels_final=np.array(training_labels)\n",
    "testing_labels_final=np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=8000\n",
    "embedding_dim = 64\n",
    "max_length = 400\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Encode sequences\n",
    "training_sequences=tokenizer.texts_to_sequences(training_sentences)\n",
    "val_sequences=tokenizer.texts_to_sequences(val_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "training_padded=tf.keras.preprocessing.sequence.pad_sequences(training_sequences, maxlen=max_length, padding='post')\n",
    "val_padded=tf.keras.preprocessing.sequence.pad_sequences(val_sequences, maxlen=max_length,  padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(8000, 64, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_padded, \n",
    "          training_labels_final, \n",
    "          steps_per_epoch=500, \n",
    "          validation_data=(val_padded, testing_labels_final),\n",
    "          validation_steps=200\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
