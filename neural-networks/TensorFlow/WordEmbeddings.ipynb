{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.preprocessing import text\n",
    "\n",
    "import pandas as pd\n",
    "import itertools  \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of WordEmbedding Algorithms:\n",
    "        \n",
    "        1. CBOW\n",
    "        2. Skip-Gram with Negative Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert text to UTF-8 [Encode]\n",
    "2. Remove Special Characters, Numbers, Punctuations, Stop Words, html urls etc. [This can be done using regex or NLTK python library]\n",
    "3. Lemmatization & Stemming\n",
    "        It is the process of converting a word to its base form, e.g., “caring” to “care”\n",
    "4. Convert to lower case\n",
    "5. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Pets change our lives &amp; become a part of o...\n",
       "1    Another spot of our #morethanmedicine bus in #...\n",
       "2    What a great team â¦@HealthSourceOHâ© â¦@Lo...\n",
       "3    What a great team â¦@HealthSourceOHâ© â¦@Lo...\n",
       "4    What a great team â¦@HealthSourceOHâ© â¦@Lo...\n",
       "Name: Tweet Content, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/Users/ukannika/work/personal/machine-learning/datasets/tweets.csv'\n",
    "raw_df = pd.read_csv(file_path, encoding = \"ISO-8859-1\")\n",
    "\n",
    "tweets = raw_df['Tweet Content']\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (386,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape: \", tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer\n",
    "Text tokenization utility class.\n",
    "\n",
    "num_words: *the maximum number of words to keep, based\n",
    "            on word frequency. Only the most common `num_words-1` words will\n",
    "            be kept.*\n",
    "            \n",
    "filters: *a string where each element is a character that will be\n",
    "            filtered from the texts. The default is all punctuation, plus\n",
    "            tabs and line breaks, minus the `'` character.*\n",
    "            \n",
    "lower: *boolean. Whether to convert the texts to lowercase.*\n",
    "\n",
    "split: *str. Separator for word splitting.*\n",
    "\n",
    "char_level: *if True, every character will be treated as a token.*\n",
    "\n",
    "oov_token: *if given, it will be added to word_index and used to\n",
    "            replace out-of-vocabulary words during text_to_sequence calls*\n",
    "        \n",
    "**Methods :**\n",
    "1. fit_on_texts => Updates internal vocabulary based on a list of texts. Should be used before texts_to_sequences\n",
    "\n",
    "2. texts_to_sequences => Transforms each text in texts to a sequence of integers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1 => Create a vocabulary(Each word assigned with unique number). This vocabulary get's created when \n",
    "tokenizer.fit_on_texts method called.\n",
    "\n",
    "Step2 => Convert text to sequences. Each sentence vector contains numbers associated with that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pets change our lives &amp; become a part of our families â¤ï¸\n",
      "That's why our members offer many solutions to help you to enjoy a long-lasting bond with your happy &amp; healthy pet ð±ð¶\n",
      "#MorethanMedicine #PetCare #PetsareFamily https://t.co/fZNIXge9a3\n",
      "[3, 34, 563, 9, 112, 31, 427, 2, 90, 18, 9, 959, 428, 2, 143, 17, 19, 9, 83, 367, 166, 51, 14, 4, 25, 30, 4, 104, 2, 429, 205, 22, 310, 21, 42, 2, 19, 31, 11, 19, 3, 960, 6, 8, 3, 69, 3, 494, 5, 961]\n"
     ]
    }
   ],
   "source": [
    "oov_token = '<UNK>'\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "                        num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n>^https?:\\/\\/.*[\\r\\n]*[^\\x00-\\x7F]+', lower=True,\n",
    "                        split=' ', char_level=False, oov_token=oov_token, document_count=0)\n",
    "\n",
    "# Tokenize our training data \n",
    "tokenizer.fit_on_texts(tweets) \n",
    "\n",
    "# Generate Sequeneces.\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "print(tweets[0])\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mappings between word to index and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eal:11\n",
      "and:12\n",
      "er:13\n",
      "ion:14\n",
      "we:15\n",
      "\n",
      "\n",
      "11:eal\n",
      "12:and\n",
      "13:er\n",
      "14:ion\n",
      "15:we\n"
     ]
    }
   ],
   "source": [
    "for x in list(word_index)[10:15]:\n",
    "    print (\"{}:{}\".format(x,  word_index[x]))\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "for x in list(index_word)[10:15]:\n",
    "    print (\"{}:{}\".format(x,  index_word[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding\n",
    "Padding is a special form of masking where the masked steps are at the start or at the beginning of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1125\n",
      "Vocabulary Sample: [('<UNK>', 1), ('a', 2), ('e', 3), ('o', 4), ('co', 5), ('more', 6), ('i', 7), ('anmedicine', 8), ('our', 9), ('animal', 10)]\n"
     ]
    }
   ],
   "source": [
    "word_index['PAD'] = 0 \n",
    "index_word[0] = 'PAD'\n",
    "\n",
    "vocab_size = len(word_index)\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word_index.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_pairs(sequence, window_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(0, len(sequence) - window_size, 1):\n",
    "        x.append(sequence[i:i+window_size])\n",
    "        y.append(sequence[i + window_size])\n",
    "    \n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 34, 563, 9, 112, 31, 427, 2, 90, 18, 9, 959, 428, 2, 143, 17, 19, 9, 83, 367, 166, 51, 14, 4, 25, 30, 4, 104, 2, 429, 205, 22, 310, 21, 42, 2, 19, 31, 11, 19, 3, 960, 6, 8, 3, 69, 3, 494, 5, 961] \n",
      "\n",
      "[[3, 34, 563, 9], [34, 563, 9, 112], [563, 9, 112, 31], [9, 112, 31, 427], [112, 31, 427, 2], [31, 427, 2, 90], [427, 2, 90, 18], [2, 90, 18, 9], [90, 18, 9, 959], [18, 9, 959, 428], [9, 959, 428, 2], [959, 428, 2, 143], [428, 2, 143, 17], [2, 143, 17, 19], [143, 17, 19, 9], [17, 19, 9, 83], [19, 9, 83, 367], [9, 83, 367, 166], [83, 367, 166, 51], [367, 166, 51, 14], [166, 51, 14, 4], [51, 14, 4, 25], [14, 4, 25, 30], [4, 25, 30, 4], [25, 30, 4, 104], [30, 4, 104, 2], [4, 104, 2, 429], [104, 2, 429, 205], [2, 429, 205, 22], [429, 205, 22, 310], [205, 22, 310, 21], [22, 310, 21, 42], [310, 21, 42, 2], [21, 42, 2, 19], [42, 2, 19, 31], [2, 19, 31, 11], [19, 31, 11, 19], [31, 11, 19, 3], [11, 19, 3, 960], [19, 3, 960, 6], [3, 960, 6, 8], [960, 6, 8, 3], [6, 8, 3, 69], [8, 3, 69, 3], [3, 69, 3, 494], [69, 3, 494, 5]] [112, 31, 427, 2, 90, 18, 9, 959, 428, 2, 143, 17, 19, 9, 83, 367, 166, 51, 14, 4, 25, 30, 4, 104, 2, 429, 205, 22, 310, 21, 42, 2, 19, 31, 11, 19, 3, 960, 6, 8, 3, 69, 3, 494, 5, 961]\n"
     ]
    }
   ],
   "source": [
    "# Verify generate_cbow_pairs method. \n",
    "for sequence in sequences[0:1]:\n",
    "    print(sequence, \"\\n\")\n",
    "    x, y = generate_cbow_pairs(sequence, 4)\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters for CBOW model.\n",
    "window_size = 4\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Shape:  (14494, 4)\n",
      "Labels Shape:  (14494,)\n"
     ]
    }
   ],
   "source": [
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    x, y = generate_cbow_pairs(sequence, window_size)\n",
    "    train_features.extend(x)\n",
    "    train_labels.extend(y)\n",
    "\n",
    "# Convert the list to numpy array.\n",
    "x_train = np.asarray(train_features)\n",
    "y_train = np.asarray(train_labels)\n",
    "\n",
    "print(\"Features Shape: \", x_train.shape)\n",
    "print(\"Labels Shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14494, 1125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_labels = np_utils.to_categorical(y_train, vocab_size)\n",
    "print(Y_labels.shape)\n",
    "Y_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model & Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding class Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "This layer can only be used as the first layer in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "        keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size),\n",
    "        keras.layers.GlobalAveragePooling1D(),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dense(vocab_size, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 4, 300)            337500    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1125)              289125    \n",
      "=================================================================\n",
      "Total params: 703,681\n",
      "Trainable params: 703,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14494 samples\n",
      "Epoch 1/10\n",
      "14494/14494 [==============================] - 0s 17us/sample - loss: 5.5715\n",
      "Epoch 2/10\n",
      "14494/14494 [==============================] - 0s 15us/sample - loss: 5.4480\n",
      "Epoch 3/10\n",
      "14494/14494 [==============================] - 0s 15us/sample - loss: 5.3968\n",
      "Epoch 4/10\n",
      "14494/14494 [==============================] - 0s 15us/sample - loss: 5.3210\n",
      "Epoch 5/10\n",
      "14494/14494 [==============================] - 0s 15us/sample - loss: 5.2497\n",
      "Epoch 6/10\n",
      "14494/14494 [==============================] - 0s 15us/sample - loss: 5.1712\n",
      "Epoch 7/10\n",
      "14494/14494 [==============================] - 0s 15us/sample - loss: 5.0891\n",
      "Epoch 8/10\n",
      "14494/14494 [==============================] - 0s 15us/sample - loss: 5.0067\n",
      "Epoch 9/10\n",
      "14494/14494 [==============================] - 0s 14us/sample - loss: 4.9217\n",
      "Epoch 10/10\n",
      "14494/14494 [==============================] - 0s 14us/sample - loss: 4.8282\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, Y_labels, batch_size=2000, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 300)\n",
      "[ 0.02007321  0.04350071  0.01544568 -0.00936638 -0.03054251 -0.03378773\n",
      "  0.0253515   0.01920177  0.04073009  0.04364152  0.0457342  -0.01785219\n",
      "  0.03220594  0.04336016  0.03884926 -0.02602983  0.04449982 -0.02690039\n",
      " -0.00022074 -0.03083779 -0.0433033  -0.04465207  0.04261256 -0.02998441\n",
      "  0.0375744   0.02662111 -0.0128968  -0.03417498  0.00962695  0.01021521\n",
      "  0.04195645 -0.0374647  -0.02804193 -0.01547996 -0.01075705 -0.0181417\n",
      "  0.03958631 -0.0018759   0.03126604 -0.01205534  0.03290215 -0.02777009\n",
      " -0.04677712 -0.04433953 -0.02434924  0.02098035 -0.03920626 -0.00438463\n",
      " -0.03070349 -0.03782425 -0.00810627  0.00963453  0.01537367  0.01646267\n",
      "  0.00408176 -0.02522275  0.02311251 -0.02515165  0.03230816  0.01896275\n",
      " -0.0452767   0.04619545  0.04516533  0.01172287  0.01555493  0.00772208\n",
      " -0.04287213  0.01814878  0.03943956 -0.01290433  0.04728908 -0.00499083\n",
      " -0.01116442  0.03490582 -0.04606031  0.03758509 -0.00830249 -0.0215108\n",
      " -0.03080089  0.00321914 -0.00918034 -0.04574318 -0.00302677  0.04316061\n",
      " -0.0397426   0.02669555  0.02481535 -0.03776284 -0.04370438 -0.0276878\n",
      "  0.02091458 -0.0377458  -0.03210142 -0.01601486  0.0470569  -0.04426688\n",
      " -0.0031289   0.04293675  0.00113983 -0.03306315  0.02464818 -0.01776968\n",
      "  0.00838584 -0.0328691  -0.00816578 -0.03837464 -0.03871733 -0.03505226\n",
      "  0.01084512 -0.02966254 -0.02950519  0.02522079 -0.02846763  0.02655938\n",
      "  0.04823362 -0.04718485  0.04499351  0.03433843 -0.0326575  -0.03836764\n",
      "  0.00677476 -0.02656789  0.02026788  0.04563699 -0.02299056  0.03335198\n",
      " -0.03324205 -0.04666468  0.01220142  0.04279217  0.02986619  0.00075458\n",
      " -0.00119627  0.01699117  0.00158615 -0.00622895 -0.0140811   0.00821433\n",
      "  0.00020234  0.00920513 -0.04443881 -0.0028379  -0.02754221  0.02296356\n",
      "  0.01602665 -0.00260984  0.0425739  -0.03047242 -0.02462214  0.02768121\n",
      "  0.02949376 -0.01757742 -0.04569633  0.02117557 -0.00733026 -0.00933461\n",
      " -0.01184427 -0.00015279 -0.02240279  0.02990992 -0.03234237 -0.00706626\n",
      " -0.02165985  0.00790928  0.04686794 -0.00462239  0.04687616  0.04505507\n",
      " -0.01728546  0.02288269  0.04452915 -0.04142096  0.04288146 -0.02423122\n",
      " -0.01660188 -0.04406881 -0.03071499 -0.02757679 -0.02467395 -0.02955719\n",
      "  0.0258312  -0.02871223 -0.00630951  0.01677269  0.0221932   0.02992663\n",
      " -0.02407929 -0.0288986   0.04979423  0.04555522  0.04250241  0.01054872\n",
      " -0.03982227 -0.00580919 -0.04910724  0.01758767  0.04262997 -0.0369341\n",
      " -0.00989649  0.00565741 -0.01308269  0.00959821  0.02928467  0.04095116\n",
      "  0.04552909  0.00294124 -0.04569521  0.0388849  -0.01671704  0.00484242\n",
      " -0.01952687 -0.01718006  0.00419313 -0.02198282 -0.00673031  0.00087267\n",
      " -0.04585658  0.00580624 -0.00855026  0.0001074  -0.0379182  -0.0121444\n",
      "  0.03684026 -0.00220614  0.04377011  0.0069978  -0.01669737 -0.04032958\n",
      "  0.02439178  0.01216143  0.00132436 -0.029616   -0.00814397 -0.02499521\n",
      "  0.03277544  0.00642387  0.02804762 -0.03206967 -0.04902884 -0.04152022\n",
      " -0.04971211 -0.0396981  -0.02349416  0.01326929 -0.02048283  0.04092965\n",
      " -0.00401283 -0.01486196  0.01809189 -0.0405516   0.02616841 -0.04506744\n",
      " -0.01288022  0.03472522  0.02728936 -0.02658695  0.01532731  0.03476853\n",
      " -0.02721347  0.034418   -0.02562222 -0.03149136 -0.04564342 -0.03406402\n",
      " -0.01380531  0.0387      0.03001862  0.04041852 -0.03628416 -0.03926324\n",
      "  0.00100253  0.0456734  -0.04756027  0.03332443  0.0277501  -0.03045673\n",
      " -0.01826822 -0.04612675 -0.00198858 -0.02783896 -0.04032501  0.00282121\n",
      " -0.03393894 -0.04355986 -0.02147114  0.02848066 -0.00707406  0.04020152\n",
      "  0.04269965 -0.03356532  0.00082338  0.02991909  0.00737469  0.02077332\n",
      "  0.01158447 -0.04143262 -0.03479704  0.02271605  0.04892038  0.04519134]\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape) \n",
    "print(weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write weights to disk to view it on Embedding Projector. In practice,\n",
    "# we can use Embedding Projector or project this data to 30 Dimensions and then Use t-Sne to visualize it.\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(word_index):\n",
    "    vec = weights[num - 1] # skip 0, it's padding.\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vecs_pretrained.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta_pretrained.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join('/Users/ukannika/Downloads/glove.6B/', 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs_vec = np.fromstring(coefs, 'f', sep=' ')\n",
    "        out_m.write(word + \"\\n\")\n",
    "        out_v.write('\\t'.join([str(x) for x in coefs_vec]) + \"\\n\")   \n",
    "\n",
    "# View these in Embedding Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skip Gram Models With Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function transforms a sequence of word indexes (list of integers) into tuples of words of the form:\n",
    "    \n",
    "    (word, word in the same window), with label 1 (positive samples).\n",
    "    (word, random word from the vocabulary), with label 0 (negative samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[960, 6], [9, 348], [19, 11], [19, 31], [2, 90], [310, 22], [367, 221], [143, 871], [8, 3], [563, 533], [90, 499], [166, 367], [22, 205], [21, 42], [5, 399], [30, 766], [143, 434], [367, 83], [367, 57], [9, 667], [6, 8], [21, 310], [31, 810], [8, 6], [494, 5], [90, 242], [31, 1055], [83, 9], [17, 11], [34, 3], [104, 2], [42, 233], [961, 5], [427, 2], [18, 597], [112, 31], [4, 999], [42, 1094], [3, 19], [3, 359], [2, 19], [205, 429], [494, 3], [563, 34], [9, 19], [494, 981], [11, 276], [3, 494], [42, 21], [367, 166], [83, 367], [18, 499], [25, 30], [5, 494], [5, 961], [428, 959], [19, 17], [2, 922], [51, 712], [30, 25], [25, 408], [3, 27], [4, 1013], [14, 4], [166, 1085], [3, 8], [143, 17], [3, 69], [2, 104], [51, 166], [104, 977], [4, 952], [19, 174], [69, 3], [959, 9], [21, 308], [563, 9], [2, 428], [19, 677], [494, 800], [429, 205], [51, 14], [112, 1024], [2, 429], [69, 330], [960, 3], [205, 22], [960, 343], [3, 69], [42, 2], [17, 516], [19, 1000], [19, 2], [19, 549], [143, 2], [18, 90], [90, 18], [3, 417], [18, 9], [9, 112], [104, 4], [310, 21], [17, 143], [34, 441], [310, 564], [166, 51], [112, 9], [14, 51], [31, 19], [6, 56], [6, 960], [2, 427], [25, 4], [428, 803], [3, 960], [959, 428], [3, 34], [30, 1077], [4, 104], [428, 2], [22, 310], [19, 9], [960, 549], [51, 142], [4, 30], [9, 928], [9, 18], [2, 1013], [19, 3], [69, 3], [9, 83], [31, 112], [166, 555], [34, 563], [2, 42], [83, 1035], [2, 143], [83, 668], [310, 200], [90, 2], [11, 31], [4, 25], [9, 959], [69, 257], [205, 552], [31, 427], [4, 14], [30, 4], [429, 2], [8, 668], [427, 31], [17, 19], [11, 19], [9, 563], [31, 11], [2, 276]] \n",
      "\n",
      "[1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "x, y = tf.keras.preprocessing.sequence.skipgrams(\n",
    "                sequences[0], vocab_size, window_size=1, negative_samples=0.6, shuffle=True,\n",
    "                categorical=False, sampling_table=None, seed=None)\n",
    "\n",
    "print(x, \"\\n\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train data\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for sequence in sequences:\n",
    "    x_skipgm, y_skipgm = tf.keras.preprocessing.sequence.skipgrams(\n",
    "                sequence, vocab_size, window_size=1, negative_samples=0.6, shuffle=True,\n",
    "                categorical=False, sampling_table=None, seed=None)\n",
    "    x.extend(x_skipgm)\n",
    "    y.extend(y_skipgm)\n",
    "    \n",
    "x_train = np.asarray(x)\n",
    "y_train = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49938 samples\n",
      "Epoch 1/5\n",
      "49938/49938 [==============================] - 0s 8us/sample - loss: 0.6503 - accuracy: 0.6331\n",
      "Epoch 2/5\n",
      "49938/49938 [==============================] - 0s 3us/sample - loss: 0.5719 - accuracy: 0.6723\n",
      "Epoch 3/5\n",
      "49938/49938 [==============================] - 0s 3us/sample - loss: 0.5249 - accuracy: 0.7414\n",
      "Epoch 4/5\n",
      "49938/49938 [==============================] - 0s 3us/sample - loss: 0.4953 - accuracy: 0.7742\n",
      "Epoch 5/5\n",
      "49938/49938 [==============================] - 0s 3us/sample - loss: 0.4809 - accuracy: 0.7789\n"
     ]
    }
   ],
   "source": [
    "# Build Model\n",
    "model = keras.Sequential([\n",
    "          keras.layers.Embedding(vocab_size, embed_size),\n",
    "          keras.layers.GlobalAveragePooling1D(),\n",
    "          keras.layers.Dense(16, activation='relu'),\n",
    "          keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x, y, batch_size=2000, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50006, 300)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) \n",
    "\n",
    "out_v = io.open('vecs_skipgram.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta_skipgram.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(word_index):\n",
    "    vec = weights[num - 1] # skip 0, it's padding.\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
