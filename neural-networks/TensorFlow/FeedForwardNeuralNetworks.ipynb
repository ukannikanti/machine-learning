{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import feature_column \n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\n",
    "dataframe = pd.read_csv(URL)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step in classification model, check imbalance ratio in dataset. There are many different ways to handle this. \n",
    "# Will look into more details later in this section. For now treat this dataset as perfect dataset for classification. \n",
    "neg, pos = np.bincount(dataframe['target'])\n",
    "total = neg + pos\n",
    "print('Total Number Of Samples: {}\\n Positive: {} ({:.2f}% of total)\\n Negative:{} ({:.2f}% of total)' \n",
    "      .format(total, pos, 100 * pos / total, neg, 100 * neg / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Train, Test, Val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create input pipeline using tf.data Module. \n",
    "    \n",
    "In practice, Spark to fetch data from datalake and use tf.data to read it from disk directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('target')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print('Features:', list(feature_batch.keys()))\n",
    "    print('Ages:', feature_batch['age'])\n",
    "    print('Targets:', label_batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data with Tensorflow Transform module. \n",
    "\n",
    "TensorFlow Transform is a library for preprocessing input data for TensorFlow, including creating features that require a full pass over the training dataset. \n",
    "\n",
    "For example, using TensorFlow Transform you could:\n",
    "\n",
    "    Normalize an input value by using the mean and standard deviation\n",
    "    Convert strings to integers by generating a vocabulary over all of the input values\n",
    "    Convert floats to integers by assigning them to buckets, based on the observed data distribution\n",
    "    \n",
    "    \n",
    "The output of tf.Transform is exported as a TensorFlow graph which you can use for both training and serving. Using the same graph for both training and serving can prevent skew, since the same transformations are applied in both stages.\n",
    "\n",
    "Tensorflow Transform module works with Apache Beam. [Different companies uses different approaches]\n",
    "\n",
    "Use Spark for transformations. \n",
    "\n",
    "Another approach to preprocess using tf.feature_column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf.feature_column module demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(feature_column):\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_column, dtype=\"float64\")\n",
    "    print(feature_layer(next(iter(train_ds))[0]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric_column => Represents real valued or numerical features. No changes applied to input. \n",
    "age = feature_column.numeric_column(\"age\")\n",
    "demo(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketized_column => Represents discretized dense input bucketed by boundaries.\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "demo(age_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_column_with_vocabulary_list => A CategoricalColumn with in-memory vocabulary.\n",
    "# categorical_column_with_hash_bucket => Represents sparse feature where ids are set by hashing.\n",
    "# categorical_column_with_identity => A CategoricalColumn that returns identity values.\n",
    "# categorical_column_with_vocabulary_file => A CategoricalColumn with a vocabulary file.\n",
    "# embedding_column => DenseColumn that converts from sparse, categorical input.   \n",
    "thal = feature_column.categorical_column_with_vocabulary_list('thal', ['fixed', 'normal', 'reversible'])\n",
    "thal_one_hot = feature_column.indicator_column(thal)\n",
    "demo(thal_one_hot)\n",
    "\n",
    "thal_embedding = feature_column.embedding_column(thal, 8)\n",
    "demo(thal_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossed_column => Returns a column for performing crosses of categorical features.\n",
    "# Combining features into a single feature\n",
    "\n",
    "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "demo(feature_column.indicator_column(crossed_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Apply above feature column transformations to current task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose columns to train a model\n",
    "feature_columns = []\n",
    "\n",
    "# add numeric cols\n",
    "for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\n",
    "      feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# add bucketized cols\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "# add categorical cols\n",
    "feature_columns.append(thal_embedding)\n",
    "\n",
    "# crossed cols\n",
    "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "feature_columns.append(crossed_feature)\n",
    "\n",
    "for column in feature_columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Layer\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create, compile, and train the model \n",
    "\n",
    "A Model groups layers into an object with training and inference features.\n",
    "\n",
    "Methods:\n",
    "    \n",
    "    compile => Configures the model for training.\n",
    "\n",
    "        optimizer\n",
    "        loss function\n",
    "        metrics\n",
    "        \n",
    "    fit => Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "        \n",
    "                      x: Input Data\n",
    "                      y: Target Data\n",
    "             batch_size: Number of samples per gradient update\n",
    "                 epochs: An epoch is an iteration over the entire x and y data provided\n",
    "              callbacks: List of callbacks to apply during training\n",
    "       validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data.\n",
    "        validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. \n",
    "          initial_epoch: Epoch at which to start training (useful for resuming a previous training run).\n",
    "        steps_per_epoch: Total number of (batches of samples) before declaring one epoch finished and start next.\n",
    "      \n",
    "    evaluate => Returns the loss value & metrics values for the model in test mode.Computation is done in batches.\n",
    "    \n",
    "    predict  => Generates output predictions for the input samples.\n",
    "    \n",
    "    Save     => Saves the model to Tensorflow SavedModel or a single HDF5 file.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_layer, # Feature layer will be input to our model\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit Model and View Training Loss With Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 13\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a TensorFlow model\n",
    "\n",
    "The phrase \"Saving a TensorFlow model\" typically means one of two things:\n",
    "\n",
    "    Checkpoints\n",
    "    SavedModel\n",
    "\n",
    "Checkpoints capture the exact value of all parameters (tf.Variable objects) used by a model. Checkpoints do not contain any description of the computation defined by the model and thus are typically only useful when source code that will use the saved parameter values is available.\n",
    "\n",
    "The SavedModel format on the other hand includes a serialized description of the computation defined by the model in addition to the parameter values (checkpoint). Models in this format are independent of the source code that created the model. They are thus suitable for deployment via TensorFlow Serving, TensorFlow Lite, TensorFlow.js, or programs in other programming languages (the C, C++, Java, Go, Rust, C# etc. TensorFlow APIs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
